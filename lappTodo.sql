-- phpMyAdmin SQL Dump
-- version 4.7.7
-- https://www.phpmyadmin.net/
--
-- Servidor: 127.0.0.1
-- Tiempo de generación: 04-11-2019 a las 23:35:19
-- Versión del servidor: 10.1.30-MariaDB
-- Versión de PHP: 7.2.2

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
SET AUTOCOMMIT = 0;
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Base de datos: `lapp`
--

-- --------------------------------------------------------

--
-- Estructura de tabla para la tabla `failed_jobs`
--

CREATE TABLE `failed_jobs` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `connection` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `queue` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `payload` longtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `exception` longtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `failed_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- --------------------------------------------------------

--
-- Estructura de tabla para la tabla `follows`
--

CREATE TABLE `follows` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL,
  `seguidor` bigint(20) NOT NULL,
  `seguido` bigint(20) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- Volcado de datos para la tabla `follows`
--

INSERT INTO `follows` (`id`, `created_at`, `updated_at`, `seguidor`, `seguido`) VALUES
(1, NULL, NULL, 17, 19),
(2, NULL, NULL, 52, 42),
(3, NULL, NULL, 60, 38),
(4, NULL, NULL, 19, 53),
(5, NULL, NULL, 43, 38),
(6, NULL, NULL, 31, 28),
(7, NULL, NULL, 15, 24),
(8, NULL, NULL, 36, 13),
(9, NULL, NULL, 9, 18),
(10, NULL, NULL, 17, 49),
(11, NULL, NULL, 32, 19),
(12, NULL, NULL, 24, 10),
(13, NULL, NULL, 57, 32),
(14, NULL, NULL, 29, 53),
(15, NULL, NULL, 47, 18),
(16, NULL, NULL, 37, 6),
(17, NULL, NULL, 43, 26),
(18, NULL, NULL, 52, 25),
(19, NULL, NULL, 23, 12),
(20, NULL, NULL, 57, 45),
(21, NULL, NULL, 46, 32),
(22, NULL, NULL, 41, 5),
(23, NULL, NULL, 36, 16),
(24, NULL, NULL, 10, 48),
(25, NULL, NULL, 2, 44),
(26, NULL, NULL, 1, 15),
(27, NULL, NULL, 59, 6),
(28, NULL, NULL, 8, 29),
(29, NULL, NULL, 33, 9),
(30, NULL, NULL, 29, 18),
(31, NULL, NULL, 14, 24),
(32, NULL, NULL, 28, 49),
(33, NULL, NULL, 59, 42),
(34, NULL, NULL, 53, 25),
(35, NULL, NULL, 58, 17),
(36, NULL, NULL, 22, 56),
(37, NULL, NULL, 49, 34),
(38, NULL, NULL, 2, 14),
(39, NULL, NULL, 3, 45),
(40, NULL, NULL, 47, 8),
(41, NULL, NULL, 38, 49),
(42, NULL, NULL, 23, 24),
(43, NULL, NULL, 46, 1),
(44, NULL, NULL, 14, 33),
(45, NULL, NULL, 10, 47),
(46, NULL, NULL, 52, 18),
(47, NULL, NULL, 7, 3),
(48, NULL, NULL, 23, 33),
(49, NULL, NULL, 21, 19),
(50, NULL, NULL, 57, 25),
(51, NULL, NULL, 58, 3),
(52, NULL, NULL, 51, 52),
(53, NULL, NULL, 22, 33),
(54, NULL, NULL, 60, 13),
(55, NULL, NULL, 23, 51),
(56, NULL, NULL, 39, 13),
(57, NULL, NULL, 54, 42),
(58, NULL, NULL, 33, 22),
(59, NULL, NULL, 17, 5),
(60, NULL, NULL, 15, 60),
(61, NULL, NULL, 20, 49),
(62, NULL, NULL, 56, 48),
(63, NULL, NULL, 40, 50),
(64, NULL, NULL, 55, 54),
(65, NULL, NULL, 42, 22),
(66, NULL, NULL, 30, 46),
(67, NULL, NULL, 5, 44),
(68, NULL, NULL, 42, 48),
(69, NULL, NULL, 57, 55),
(70, NULL, NULL, 49, 36),
(71, NULL, NULL, 10, 16),
(72, NULL, NULL, 49, 1),
(73, NULL, NULL, 14, 1),
(74, NULL, NULL, 30, 43),
(75, NULL, NULL, 36, 59),
(76, NULL, NULL, 39, 50),
(77, NULL, NULL, 36, 34),
(78, NULL, NULL, 44, 34),
(79, NULL, NULL, 34, 50),
(80, NULL, NULL, 27, 41),
(81, NULL, NULL, 40, 9),
(82, NULL, NULL, 46, 54),
(83, NULL, NULL, 6, 33),
(84, NULL, NULL, 30, 35),
(85, NULL, NULL, 36, 21),
(86, NULL, NULL, 1, 51),
(87, NULL, NULL, 49, 17),
(88, NULL, NULL, 40, 34),
(89, NULL, NULL, 29, 15),
(90, NULL, NULL, 26, 44),
(91, NULL, NULL, 8, 25),
(92, NULL, NULL, 51, 59),
(93, NULL, NULL, 27, 35),
(94, NULL, NULL, 3, 22),
(95, NULL, NULL, 11, 7),
(96, NULL, NULL, 45, 56),
(97, NULL, NULL, 47, 12),
(98, NULL, NULL, 15, 29),
(99, NULL, NULL, 18, 1),
(100, NULL, NULL, 49, 16),
(101, NULL, NULL, 31, 17),
(102, NULL, NULL, 56, 49),
(103, NULL, NULL, 1, 40),
(104, NULL, NULL, 7, 9),
(105, NULL, NULL, 7, 6),
(106, NULL, NULL, 14, 40),
(107, NULL, NULL, 54, 24),
(108, NULL, NULL, 57, 2),
(109, NULL, NULL, 58, 5),
(110, NULL, NULL, 9, 59),
(111, NULL, NULL, 25, 12),
(112, NULL, NULL, 22, 7),
(113, NULL, NULL, 43, 47),
(114, NULL, NULL, 37, 31),
(115, NULL, NULL, 20, 34),
(116, NULL, NULL, 38, 44),
(117, NULL, NULL, 47, 7),
(118, NULL, NULL, 9, 54),
(119, NULL, NULL, 48, 47),
(120, NULL, NULL, 9, 17),
(121, NULL, NULL, 30, 60),
(122, NULL, NULL, 5, 53),
(123, NULL, NULL, 35, 10),
(124, NULL, NULL, 22, 26),
(125, NULL, NULL, 32, 16),
(126, NULL, NULL, 1, 52),
(127, NULL, NULL, 44, 30),
(128, NULL, NULL, 36, 35),
(129, NULL, NULL, 34, 10),
(130, NULL, NULL, 16, 38),
(131, NULL, NULL, 33, 21),
(132, NULL, NULL, 33, 50),
(133, NULL, NULL, 6, 29),
(134, NULL, NULL, 41, 52),
(135, NULL, NULL, 25, 60),
(136, NULL, NULL, 19, 32),
(137, NULL, NULL, 15, 52),
(138, NULL, NULL, 40, 19),
(139, NULL, NULL, 35, 32),
(140, NULL, NULL, 12, 54),
(141, NULL, NULL, 16, 3),
(142, NULL, NULL, 60, 49),
(143, NULL, NULL, 58, 37),
(144, NULL, NULL, 32, 20),
(145, NULL, NULL, 18, 58),
(146, NULL, NULL, 13, 34),
(147, NULL, NULL, 27, 50),
(148, NULL, NULL, 16, 30),
(149, NULL, NULL, 24, 50),
(150, NULL, NULL, 17, 12),
(151, NULL, NULL, 58, 6),
(152, NULL, NULL, 39, 31),
(153, NULL, NULL, 41, 22),
(154, NULL, NULL, 6, 38),
(155, NULL, NULL, 56, 50),
(156, NULL, NULL, 44, 32),
(157, NULL, NULL, 7, 40),
(158, NULL, NULL, 22, 44),
(159, NULL, NULL, 56, 60),
(160, NULL, NULL, 3, 42),
(161, NULL, NULL, 55, 4),
(162, NULL, NULL, 43, 27),
(163, NULL, NULL, 8, 38),
(164, NULL, NULL, 41, 2),
(165, NULL, NULL, 48, 36),
(166, NULL, NULL, 25, 8),
(167, NULL, NULL, 26, 15),
(168, NULL, NULL, 56, 27),
(169, NULL, NULL, 6, 35),
(170, NULL, NULL, 26, 1),
(171, NULL, NULL, 55, 31),
(172, NULL, NULL, 31, 43),
(173, NULL, NULL, 15, 28),
(174, NULL, NULL, 27, 29),
(175, NULL, NULL, 19, 33),
(176, NULL, NULL, 28, 27),
(177, NULL, NULL, 31, 54),
(178, NULL, NULL, 39, 40),
(179, NULL, NULL, 35, 8),
(180, NULL, NULL, 56, 45),
(181, NULL, NULL, 59, 53),
(182, NULL, NULL, 48, 4),
(183, NULL, NULL, 29, 60),
(184, NULL, NULL, 52, 43),
(185, NULL, NULL, 19, 58),
(186, NULL, NULL, 51, 16),
(187, NULL, NULL, 50, 45),
(188, NULL, NULL, 48, 40),
(189, NULL, NULL, 26, 38),
(190, NULL, NULL, 1, 49),
(191, NULL, NULL, 50, 53),
(192, NULL, NULL, 29, 1),
(193, NULL, NULL, 29, 2),
(194, NULL, NULL, 58, 13),
(195, NULL, NULL, 9, 10),
(196, NULL, NULL, 56, 42),
(197, NULL, NULL, 3, 31),
(198, NULL, NULL, 59, 45),
(199, NULL, NULL, 57, 43),
(200, NULL, NULL, 46, 9),
(201, NULL, NULL, 37, 20),
(202, NULL, NULL, 27, 60),
(203, NULL, NULL, 10, 14),
(204, NULL, NULL, 57, 18),
(205, NULL, NULL, 38, 18),
(206, NULL, NULL, 36, 25),
(207, NULL, NULL, 20, 43),
(208, NULL, NULL, 14, 53),
(209, NULL, NULL, 13, 14),
(210, NULL, NULL, 54, 49),
(211, NULL, NULL, 16, 10),
(212, NULL, NULL, 14, 42),
(213, NULL, NULL, 21, 52),
(214, NULL, NULL, 14, 58),
(215, NULL, NULL, 22, 53),
(216, NULL, NULL, 41, 11),
(217, NULL, NULL, 9, 13),
(218, NULL, NULL, 43, 17),
(219, NULL, NULL, 18, 21),
(220, NULL, NULL, 28, 30),
(221, NULL, NULL, 56, 32),
(222, NULL, NULL, 52, 38),
(223, NULL, NULL, 11, 26),
(224, NULL, NULL, 35, 15),
(225, NULL, NULL, 32, 25),
(226, NULL, NULL, 1, 32),
(227, NULL, NULL, 9, 56),
(228, NULL, NULL, 11, 8),
(229, NULL, NULL, 20, 10),
(230, NULL, NULL, 24, 44),
(231, NULL, NULL, 58, 52),
(232, NULL, NULL, 4, 6),
(233, NULL, NULL, 40, 48),
(234, NULL, NULL, 56, 30),
(235, NULL, NULL, 14, 26),
(236, NULL, NULL, 6, 51),
(237, NULL, NULL, 43, 14),
(238, NULL, NULL, 25, 1),
(239, NULL, NULL, 50, 31),
(240, NULL, NULL, 54, 1),
(241, NULL, NULL, 4, 2),
(242, NULL, NULL, 6, 57),
(243, NULL, NULL, 32, 6),
(244, NULL, NULL, 29, 56),
(245, NULL, NULL, 10, 41),
(246, NULL, NULL, 1, 60),
(247, NULL, NULL, 24, 26),
(248, NULL, NULL, 56, 21),
(249, NULL, NULL, 25, 7),
(250, NULL, NULL, 54, 44),
(251, NULL, NULL, 46, 40),
(252, NULL, NULL, 33, 51),
(253, NULL, NULL, 58, 4),
(254, NULL, NULL, 14, 55),
(255, NULL, NULL, 18, 48),
(256, NULL, NULL, 55, 7),
(257, NULL, NULL, 48, 45),
(258, NULL, NULL, 32, 60),
(259, NULL, NULL, 28, 41),
(260, NULL, NULL, 36, 53),
(261, NULL, NULL, 35, 41),
(262, NULL, NULL, 3, 53),
(263, NULL, NULL, 12, 37),
(264, NULL, NULL, 17, 39),
(265, NULL, NULL, 12, 6),
(266, NULL, NULL, 24, 4),
(267, NULL, NULL, 10, 59),
(268, NULL, NULL, 8, 9),
(269, NULL, NULL, 10, 7),
(270, NULL, NULL, 4, 28),
(271, NULL, NULL, 23, 4),
(272, NULL, NULL, 26, 27),
(273, NULL, NULL, 48, 12),
(274, NULL, NULL, 21, 24),
(275, NULL, NULL, 9, 39),
(276, NULL, NULL, 44, 18),
(277, NULL, NULL, 59, 2);

-- --------------------------------------------------------

--
-- Estructura de tabla para la tabla `migrations`
--

CREATE TABLE `migrations` (
  `id` int(10) UNSIGNED NOT NULL,
  `migration` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `batch` int(11) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- Volcado de datos para la tabla `migrations`
--

INSERT INTO `migrations` (`id`, `migration`, `batch`) VALUES
(1, '2014_10_12_000000_create_users_table', 1),
(2, '2014_10_12_100000_create_password_resets_table', 1),
(3, '2019_08_19_000000_create_failed_jobs_table', 1),
(4, '2019_10_06_153441_create_posts_table', 1),
(5, '2019_10_06_220225_agregar_mas_datos_to_posts', 2),
(6, '2019_10_26_204413_create_follows_table', 3),
(7, '2019_10_26_210412_agregar_datos_follows', 4),
(8, '2019_10_26_210919_create_perfiles_table', 5),
(9, '2019_10_26_222358_agregar_nick_user', 6),
(10, '2019_10_29_225922_add_image_to_profile', 7),
(11, '2019_10_29_230804_add_image_to_perfiles', 8);

-- --------------------------------------------------------

--
-- Estructura de tabla para la tabla `password_resets`
--

CREATE TABLE `password_resets` (
  `email` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `token` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- --------------------------------------------------------

--
-- Estructura de tabla para la tabla `perfiles`
--

CREATE TABLE `perfiles` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `uid` bigint(20) NOT NULL,
  `biografia` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `company` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `ubicacion` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `paginaWeb` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL,
  `imagen` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'noImagen.jpg'
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- Volcado de datos para la tabla `perfiles`
--

INSERT INTO `perfiles` (`id`, `uid`, `biografia`, `company`, `ubicacion`, `paginaWeb`, `created_at`, `updated_at`, `imagen`) VALUES
(1, 1, 'alguien con ganas de saber mas', 'desarrollador', 'Distrito de Moche', 'quora', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona8.jpeg'),
(2, 2, 'alguien con animos de saber mas cada dia', 'estudiante intermedio', 'Distrito de Moche', 'amazon', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona0.jpeg'),
(3, 3, 'alguien con animos de saber mas cada dia', 'profesor', 'Distrito de Salaverry', 'twitter', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona7.jpeg'),
(4, 4, 'alguien con ganas de conocer mas', 'desarrollador', 'Distrito de Huanchaco', 'faebook', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona3.jpeg'),
(5, 5, 'alguien con ganas de compartir sus ideas', 'estudiante intermedio', 'Distrito de Trujillo', 'google', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona7.jpeg'),
(6, 6, 'alguien con ganas de compartir sus ideas', 'estudiante aprendiz', 'Distrito de Moche', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona10.jpeg'),
(7, 7, 'alguien con ganas de conocer mas', 'estudiante aprendiz', 'Distrito de Simbal', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona2.jpeg'),
(8, 8, 'alguien con ganas de conocer mas', 'desarrollador', 'Distrito de Florencia de Mora', 'gitlab', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona7.jpeg'),
(9, 9, 'alguien con animos de saber mas cada dia', 'estudiante avanzado', 'Distrito de Víctor Larco Herrera', 'google', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona9.jpeg'),
(10, 10, 'alguien con ganas de conocer mas', 'estudiante avanzado', 'Distrito de Trujillo', 'twitter', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona1.jpeg'),
(11, 11, 'alguien con ganas de saber mas', 'profesor', 'Distrito de Víctor Larco Herrera', 'quora', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona0.jpeg'),
(12, 12, 'alguien con animos de saber mas cada dia', 'estudiante avanzado', 'Distrito de Florencia de Mora', 'gitlab', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona6.jpeg'),
(13, 13, 'alguien con ganas de saber mas', 'estudiante aprendiz', 'Distrito de Trujillo', 'amazon', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona0.jpeg'),
(14, 14, 'alguien con animos de saber mas cada dia', 'profesor', 'Distrito de Simbal', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona3.jpeg'),
(15, 15, 'alguien con ganas de compartir sus ideas', 'estudiante intermedio', 'Distrito de El Porvenir (Trujillo)', 'twitter', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona7.jpeg'),
(16, 16, 'alguien con ganas de compartir sus ideas', 'estudiante aprendiz', 'Distrito de Huanchaco', 'amazon', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona8.jpeg'),
(17, 17, 'alguien con ganas de compartir sus ideas', 'desarrollador', 'Distrito de Simbal', 'gitlab', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona0.jpeg'),
(18, 18, 'alguien con animos de saber mas cada dia', 'estudiante intermedio', 'Distrito de La Esperanza (Trujillo)', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona7.jpeg'),
(19, 19, 'alguien con animos de saber mas cada dia', 'estudiante intermedio', 'Distrito de Salaverry', 'gitlab', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona2.jpeg'),
(20, 20, 'alguien con ganas de compartir sus ideas', 'estudiante intermedio', 'Distrito de Poroto', 'amazon', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona9.jpeg'),
(21, 21, 'alguien con ganas de saber mas', 'desarrollador', 'Distrito de Moche', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona8.jpeg'),
(22, 22, 'alguien con ganas de conocer mas', 'desarrollador', 'Distrito de Moche', 'quora', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona1.jpeg'),
(23, 23, 'alguien con ganas de saber mas', 'estudiante aprendiz', 'Distrito de Salaverry', 'faebook', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona2.jpeg'),
(24, 24, 'alguien con ganas de conocer mas', 'estudiante aprendiz', 'Distrito de Simbal', 'faebook', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona2.jpeg'),
(25, 25, 'alguien con animos de saber mas cada dia', 'estudiante avanzado', 'Distrito de La Esperanza (Trujillo)', 'twitter', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona7.jpeg'),
(26, 26, 'alguien con animos de saber mas cada dia', 'desarrollador', 'Distrito de El Porvenir (Trujillo)', 'gitlab', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona2.jpeg'),
(27, 27, 'alguien con ganas de saber mas', 'estudiante intermedio', 'Distrito de Laredo', 'quora', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona9.jpeg'),
(28, 28, 'alguien con ganas de compartir sus ideas', 'estudiante aprendiz', 'Distrito de Salaverry', 'gitlab', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona3.jpeg'),
(29, 29, 'alguien con ganas de conocer mas', 'estudiante avanzado', 'Distrito de Florencia de Mora', 'twitter', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona8.jpeg'),
(30, 30, 'alguien con ganas de saber mas', 'desarrollador', 'Distrito de La Esperanza (Trujillo)', 'gitlab', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona6.jpeg'),
(31, 31, 'alguien con ganas de conocer mas', 'estudiante avanzado', 'Distrito de Moche', 'twitter', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona10.jpeg'),
(32, 32, 'alguien con ganas de saber mas', 'estudiante intermedio', 'Distrito de Florencia de Mora', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona4.jpeg'),
(33, 33, 'alguien con ganas de compartir sus ideas', 'estudiante avanzado', 'Distrito de Laredo', 'quora', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona0.jpeg'),
(34, 34, 'alguien con animos de saber mas cada dia', 'estudiante intermedio', 'Distrito de Simbal', 'amazon', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona3.jpeg'),
(35, 35, 'alguien con ganas de saber mas', 'estudiante avanzado', 'Distrito de Florencia de Mora', 'twitter', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona3.jpeg'),
(36, 36, 'alguien con ganas de conocer mas', 'estudiante intermedio', 'Distrito de La Esperanza (Trujillo)', 'amazon', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona10.jpeg'),
(37, 37, 'alguien con animos de saber mas cada dia', 'estudiante intermedio', 'Distrito de El Porvenir (Trujillo)', 'google', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona5.jpeg'),
(38, 38, 'alguien con ganas de saber mas', 'estudiante intermedio', 'Distrito de Trujillo', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona5.jpeg'),
(39, 39, 'alguien con animos de saber mas cada dia', 'profesor', 'Distrito de La Esperanza (Trujillo)', 'quora', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona7.jpeg'),
(40, 40, 'alguien con animos de saber mas cada dia', 'desarrollador', 'Distrito de Víctor Larco Herrera', 'gitlab', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona8.jpeg'),
(41, 41, 'alguien con ganas de saber mas', 'estudiante avanzado', 'Distrito de La Esperanza (Trujillo)', 'google', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona1.jpeg'),
(42, 42, 'alguien con animos de saber mas cada dia', 'estudiante intermedio', 'Distrito de Trujillo', 'google', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona7.jpeg'),
(43, 43, 'alguien con ganas de conocer mas', 'profesor', 'Distrito de Salaverry', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona10.jpeg'),
(44, 44, 'alguien con ganas de saber mas', 'estudiante avanzado', 'Distrito de Trujillo', 'amazon', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona0.jpeg'),
(45, 45, 'alguien con ganas de saber mas', 'profesor', 'Distrito de Víctor Larco Herrera', 'google', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona9.jpeg'),
(46, 46, 'alguien con ganas de compartir sus ideas', 'profesor', 'Distrito de Moche', 'quora', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona6.jpeg'),
(47, 47, 'alguien con ganas de saber mas', 'profesor', 'Distrito de Víctor Larco Herrera', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona2.jpeg'),
(48, 48, 'alguien con ganas de compartir sus ideas', 'estudiante aprendiz', 'Distrito de Florencia de Mora', 'amazon', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona6.jpeg'),
(49, 49, 'alguien con ganas de compartir sus ideas', 'estudiante aprendiz', 'Distrito de Florencia de Mora', 'faebook', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona5.jpeg'),
(50, 50, 'alguien con ganas de conocer mas', 'profesor', 'Distrito de Florencia de Mora', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona10.jpeg'),
(51, 51, 'alguien con ganas de conocer mas', 'desarrollador', 'Distrito de Simbal', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona10.jpeg'),
(52, 52, 'alguien con ganas de saber mas', 'estudiante intermedio', 'Distrito de Huanchaco', 'quora', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona2.jpeg'),
(53, 53, 'alguien con ganas de saber mas', 'estudiante avanzado', 'Distrito de El Porvenir (Trujillo)', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona10.jpeg'),
(54, 54, 'alguien con animos de saber mas cada dia', 'estudiante aprendiz', 'Distrito de Poroto', 'gitlab', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona10.jpeg'),
(55, 55, 'alguien con animos de saber mas cada dia', 'estudiante intermedio', 'Distrito de Florencia de Mora', 'twitter', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona4.jpeg'),
(56, 56, 'alguien con ganas de saber mas', 'desarrollador', 'Distrito de Poroto', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona1.jpeg'),
(57, 57, 'alguien con ganas de compartir sus ideas', 'profesor', 'Distrito de Poroto', 'github', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona2.jpeg'),
(58, 58, 'alguien con ganas de conocer mas', 'estudiante avanzado', 'Distrito de Salaverry', 'twitter', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona10.jpeg'),
(59, 59, 'alguien con ganas de saber mas', 'profesor', 'Distrito de Moche', 'twitter', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona8.jpeg'),
(60, 60, 'alguien con ganas de conocer mas', 'desarrollador', 'Distrito de Huanchaco', 'amazon', '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'persona6.jpeg');

-- --------------------------------------------------------

--
-- Estructura de tabla para la tabla `posts`
--

CREATE TABLE `posts` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `titulo` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `cuerpo` mediumtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL,
  `user_id` int(11) NOT NULL,
  `imagen` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `descripcion` mediumtext COLLATE utf8mb4_unicode_ci NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- Volcado de datos para la tabla `posts`
--

INSERT INTO `posts` (`id`, `titulo`, `cuerpo`, `created_at`, `updated_at`, `user_id`, `imagen`, `descripcion`) VALUES
(1, 'Así usa Youtube las redes neuronales para elegir qué contenidos te recomienda ver a continuación', '<div class=\"blob js-post-images-container\">\n<p><strong>Uno de los usos más comunes de la tecnología de aprendizaje automático son los sistemas de recomendación</strong> de las plataformas online: cada vez que Facebook privilegia una publicación sobre otra a la hora de mostrarla en tu \'newsfeed\' o Twitter destaca un tuit en la sección \"Por si te lo perdiste\" de tu \'timeline\', lo que vemos es el resultado de sistemas de recomendación basados en inteligencia artificial.</p>\n<!-- BREAK 1 -->\n<p>Investigadores de Google publicaron recientemente un artículo académico (\"<a href=\"https://dl.acm.org/citation.cfm?id=3346997\">Recommending what video to watch next: a multitask ranking system</a>\") en el que ofrecen algunos detalles relevantes sobre el funcionamiento del sistema de recomendación de vídeos de Youtube, uno de los más relevantes y avanzados de la industria, y que destaca por <strong>su efectividad a la hora de retener la atención del usuario</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Al tratarse de un plataforma en la que una gran cantidad de usuarios suben cientos de horas de vídeo cada segundo, <strong>el funcionamiento de su sistema de recomendación ha de ser forzosamente diferente de los de otras plataformas de streaming</strong> como Spotify o Netflix, que cuentan con un catálogo estable y centralizado: la evaluación de datos y la generación de recomendaciones en tiempo real cobra una importancia mucho mayor en el caso del portal de vídeos de Google.</p>\n<!-- BREAK 3 -->\n\n<h2>La clave: dos redes neuronales profundas</h2>\n<p>Hasta 2016, Youtube recurría a algoritmos que sencillamente recomendaban vídeos en base a un conjunto de varios criterios: duración del vídeo, número de suscriptores, número de veces que había sido compartido, etc. Sin embargo, hace 3 años Youtube empezó a adoptar <a href=\"https://www.xataka.com/robotica-e-ia/las-redes-neuronales-que-son-y-por-que-estan-volviendo\">las redes neuronales</a>.</p>\n<!-- BREAK 4 -->\n<p>Ahora, el sistema de recomendación de vídeos de Youtube <strong>funciona como un embudo estructurado en dos etapas</strong>, cada una de ellas responsabilidad de una red neuronal distinta:</p>\n<!-- BREAK 5 -->\n<p>1) <strong>Generación de ítems candidatos</strong>: En esta fase, las opciones se reducen de millones a miles. Recurre a datos extraídos del historial de los usuarios para ofrecer un listado de vídeos que tenga en cuenta el filtrado colaborativo (¿qué otros vídeos han atraído la atención del resto de personas que ven vídeos similares a los de este usuario?, por ejemplo).  </p>\n<!-- BREAK 6 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Youtube\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/bb4f61/youtube/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/bb4f61/youtube/450_1000.jpg 450w, https://i.blogs.es/bb4f61/youtube/650_1200.jpg 681w, https://i.blogs.es/bb4f61/youtube/1024_2000.jpg 1024w, https://i.blogs.es/bb4f61/youtube/1366_2000.jpg 1366w\"/><noscript><img alt=\"Youtube\" src=\"https://i.blogs.es/bb4f61/youtube/450_1000.jpg\"/></noscript> <span> Extraído de \'Recommending what video to watch next: a multitask ranking system\'. </span> </div></div></div>\n<p>2) <strong>Clasificación</strong>: En esta fase, las opciones se reducen de miles a decenas. Este proceso asigna una puntuación a cada vídeo, la cual determina la visibilidad que tendrá el mismo a la hora de mostrar las recomendaciones cuando estemos usando Youtube.</p>\n<!-- BREAK 7 -->\n<p>Aspectos como su similitud con contenidos que hayamos visualizado anteriormente <strong>aumentarán</strong> la probabilidad de que aparezcan entre los primeros puestos, mientras que se <strong>reducirán</strong> si el vídeo ya fue recomendado antes y el usuario \'pasó\' del mismo.</p>\n<!-- BREAK 8 -->\n<p>Otro factor que influye es <strong>la \'edad\' del vídeo</strong>: para evitar un sesgo en favor del contenido más antiguo (el que más visitas y \'likes\' acumula, al fin y al cabo), el sistema de recomendación favorece la presencia de contenido novedoso entre las recomendaciones.</p>\n<!-- BREAK 9 -->\n<h2>Engagement y sesgos</h2>\n<p>Sin embargo, aun conociendo todos los factores que tiene en cuenta Youtube a la hora de generar sus recomendaciones, resulta imposible predecir los mismos con exactitud, porque <strong>las redes neuronales profundas van aprendiendo sobre la marcha, alterando ligeramente sus resultados</strong> para cumplir con el objetivo básico con el que fueron creadas: en este caso, aumentar el \'engagement\' (es decir, la retención del usuario frente a la pantalla).</p>\n<!-- BREAK 10 -->\n<p>De hecho, Youtube ha tenido que realizar cambios en su IA en los últimos tiempos, por <strong>el incentivo perverso que estaba demostrando ser esa búsqueda del engagement a toda costa</strong>. <a href=\"https://www.xataka.com/inteligencia-artificial/que-ia-recomendaciones-youtube-se-volvio-conspiranoica-condujo-a-conspiranoia-a-muchos-usuarios\">Hace unos meses</a> abordamos cómo esta política había llevado a muchos usuarios a \'engancharse\' a contenidos pseudocientíficos y conspiranoicos.</p>\n<!-- BREAK 11 -->\n\n<p>Guillaume Chaslot, ex-trabajador de Google y asesor del Center for Humane Technology, cuenta la historia de un conocido, \"Brian\", que encontraba en esa situación:</p>\n<!-- BREAK 12 -->\n<blockquote>\n<p>\"Para sus padres, familiares y amigos, su historia es desgarradora. Pero desde el punto de vista de la IA de YouTube, es un gran éxito. Diseñamos la IA de YouTube para que aumentara el tiempo que las personas pasan online, porque eso conlleva más anuncios. La IA considera a Brian como un modelo que debe multiplicarse\".</p>\n<p>\"¿Cuántas personas como Brian son seducidas por esas \'madrigueras de conejo\' todos los días? Por diseño, la IA intentará captar a la mayor cantidad posible. [...] Por lo tanto, si \'la tierra es plana\' mantiene a los usuarios más tiempo on line que \'la tierra es redonda\', esa teoría se verá favorecida por el algoritmo de recomendación\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://towardsdatascience.com/using-deep-neural-networks-to-make-youtube-recommendations-dfc0a1a13d1e\">Towards Data Science</a> (y <a href=\"https://towardsdatascience.com/how-youtube-recommends-videos-b6e003a5ab2f\">II</a>) &amp; <a href=\"https://medium.com/vantageai/how-youtube-is-recommending-your-next-video-7e5f1a6bd6d9\">Vantage AI</a></p>\n<!-- BREAK 13 -->\n<p>Imagen | <a href=\"https://www.publicdomainpictures.net/en/view-image.php?image=240456&amp;picture=youtube\">Public Domain Pictures</a></p>\n<!-- BREAK 14 --> </div>', '2019-11-04 02:12:06', '2019-11-04 02:12:06', 45, 'portada0.jpg', 'Youtube adoptó las redes neuronales en 2016, y son un aspecto básico del proceso que lleva al portal de streaming a recomendar que veas a continuación uno u otro vídeo.'),
(2, 'La inteligencia artificial sale a la caza de los lobos de Wall Street: el NASDAQ la usa para detectar fraudes bursátiles', '<div class=\"blob js-post-images-container\">\n<p>El NASDAQ, la segunda mayor bolsa de valores estadounidense si atendemos a la capitalización de mercado de las sociedades cotizadas, ha venido funcionando hasta ahora  de manera automatizada, pero sin salvaguardias frente a los intentos de manipulación del mercado, una deficiencia que ahora busca subsanar <strong>implementando mecanismos basados en la inteligencia artificial</strong>.</p>\n<!-- BREAK 1 -->\n<p>Hoy en día, por ejemplo, un inversor puede llevar a cabo una cantidad desmesurada órdenes de venta de acciones de una compañía, hacer que el valor de éstas caiga, cancelar dichas órdenes y lanzarse a continuación a comprar más acciones de dichas compañías, cuyos precios se situarían en mínimos gracias a su acción.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p><strong>A este tipo de fraude se le conoce como \'spoofing\'</strong>, y es difícil de evitar porque los traders pueden recurrir a algoritmos para emitir órdenes y cancelarlas de manera casi instantánea, una y otra vez. Por eso los responsables del NASDAQ buscan atajar esta clase de amenazas recurriendo a la misma arma: la IA.</p>\n<!-- BREAK 3 -->\n\n<p>La herramienta que terminaron desarrollando, basada en <a href=\"https://www.xataka.com/robotica-e-ia/deep-learning-que-es-y-por-que-va-a-ser-una-tecnologia-clave-en-el-futuro-de-la-inteligencia-artificial\">la tecnología de aprendizaje profundo</a>, se puso en marcha hace unas semanas, un año después de iniciarse el proyecto: el objetivo es que sea capaz de analizar el movimiento de acciones del índice (<strong>miles de millones de acciones en un día normal</strong>) para detectar los patrones característicos de una operación de fraude.</p>\n<!-- BREAK 4 -->\n<p>En palabras de Michael O\'Rourke (responsable del equipo de machine intelligence del NASDAQ), lo bueno del aprendizaje profundo es que es \"útil para encontrar cosas que son muy difíciles de describir\". O\'Rourke construyó un dataset que basado en los datos históricos de negociación de Nasdaq . <strong>Dado el gran volumen de datos que maneja diariamente el índice</strong>, la calidad de estos datos de entrenamiento a la hora de permitir identificar patrones es muy alta.</p>\n<!-- BREAK 5 -->\n<p>A continuación el equipo construyó un modelo de IA con el que examinarían los datos comerciales e <strong>identificarían toda aquella actividad que se desviara de las tendencias normales del mercado</strong>. Dicha actividad sería después analizada por humanos para determinar si es benigna o si, por el contrario, sería merecedora de una investigación más profunda.</p>\n<!-- BREAK 6 -->\n\n<h2>El futuro de los algoritmos en el ámbito financiero</h2>\n<p><strong>Uno de los temores respecto al uso de esta IA residía en que generase una avalancha de falsos positivos</strong>, lo que cargaría de trabajo al personal encargado de revisar las operaciones sospechosas. Sin embargo, O\'Rourke estima que por ahora la tasa de falsos positivos resulta \"aceptable\".</p>\n<!-- BREAK 7 -->\n<p><strong>La compañía había experimentado antes con el uso de la IA con fines de supervisión</strong>, en las bolsas que opera en el Norte de Europa tras su adquisición del operador báltico-escandinavo OMX; sin embargo, esta es la primera vez que aplica la inteligencia artificial en su mercado estadounidense, el NASDAQ propiamente dicho.</p>\n<!-- BREAK 8 -->\n<p>El plan es que, si el experimento resulta exitoso, <strong>pueda recurrir a lo que se conoce como \'transferencia de aprendizaje\'</strong>: aplicar lo aprendido por este modelo para supervisar otros mercados donde se cuenta con menos información histórica disponible. En última instancia, confían en poder implementar esta tecnología en el software que venden a otros operadores, como el de la Bolsa de Hong Kong.</p>\n<!-- BREAK 9 -->\n\n<p>Martina Rejsjo, responsable del ya citado equipo de vigilancia de mercados de Nadsaq, afirma que -independientemente del resultado de este experimento- l<strong>a IA ostentará en el futuro un papel clave en la supervisión de los mercados bursátiles</strong>, debido al gran volumen de información comercial que debe ser analizada diariamente. Para Rejsjo, el papel de los humanos en esa tarea quedará relegado a comprobar los avisos de los algoritmos.</p>\n<!-- BREAK 10 -->\n<p>Sin embargo, al tiempo que el NASDAQ empieza a apostar por la automatización, el principal mercado de valores de los EE.UU., l<strong>a Bolsa de Nueva York, está apostando por recorrer el camino contrario</strong>: dejar en manos de humanos tareas hasta ahora automatizadas.</p>\n<!-- BREAK 11 -->\n<p>Así, desde agosto, los fondos cotizados basados en bonos, commodities y divisas, que operaban en su propia plataforma (el NYSE Arca Exchange, basado en algoritmos automatizados), han pasado a hacerlo en el parqué del NYSE, donde los \'creadores designados de mercado\' (DMM, por sus siglas en inglés), pueden supervisar las operaciones relacionadas con estos fondos, con el fin de reducir su volatilidad.</p>\n<!-- BREAK 12 -->\n<p>Vía | <a href=\"https://fortune.com/2019/10/29/machine-learning-nasdaq-spoofing-stock-trading-fraud/\">Fortune</a> &amp; <a href=\"https://www.wsj.com/articles/nasdaq-deploys-ai-to-detect-stock-market-abuse-11564571534\">Wall Street Journal</a> &amp; <a href=\"https://www.eleconomista.es/mercados-cotizaciones/noticias/9766376/03/19/Las-ordenes-automatizadas-de-ETFs-se-humanizaran-en-la-Bolsa-de-Nueva-York.html\">El Economista</a></p>\n<!-- BREAK 13 -->\n<p>Imagen | <a href=\"https://www.flickr.com/photos/maguisso/217357080\">Luis Villa del Campo</a></p>\n</div>', '2019-11-03 02:00:26', '2019-11-03 02:00:26', 30, 'portada1.jpg', 'El NASDAQ recurre al aprendizaje profundo para detectar posibles intentos de manipulación del mercado, y confía en exportar el modelo a otras bolsas. Pero la Bolsa de Nueva York recorre el camino contrario: mayor papel para los humanos.'),
(3, 'Los robots sexuales suben un nuevo peldaño hacia el realismo: tendrán la capacidad de \'respirar\'', '<div class=\"blob js-post-images-container\">\n<p>¿No te importaría recurrir a un robot sexual, pero <strong>los modelos actualmente en el mercado no resultan lo bastante \'humanos\' como para motivarte</strong>? Tranquilo, hay un fabricante de este reducido sector que ya ha pensado en ti, y se ha puesto manos a la obra para construir un robot que \'respira\'.</p>\n<!-- BREAK 1 -->\n<p>Su fabricante, AI-Aitech, es una compañía china que ya causó furor en su país de origen hace unos meses presentando a Emma, la sexbot de 3.100 dólares tan realista que incluso <strong>su temperatura se aproximaba a los 37º</strong>. Pues bien, el siguiente paso es incluir en sus robots <strong>una cavidad en el pecho que les dote de la apariencia de respiración</strong>, con sus correspondientes movimientos de inspiración y espiración.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Sam White, CEO de Cloud Climax, distribuidor británico de AI-Aitech, ha confirmado no sólo este aspecto, sino también que trabajan para lograr robots capaces de <strong>mover de forma realista sus extremidades, lo que les dotaría de la capacidad de abrazar o caminar</strong>. Según White, dichos avances se incorporarían a las nuevas generaciones de Emma... y, presumiblemente, a <a href=\"https://www.xataka.com/robotica-e-ia/robots-sexuales-masculinos-hay-prototipo-que-sepamos-algunas-ideas-nadie-parece-estar-listo-para-lanzar-uno\">la versión masculina de dicho robot</a>, que Cloud Climax confirmó semioficialmente hace unos meses.</p>\n<!-- BREAK 3 -->\n\n<h2>El realismo es un logro, pero quizá nos terminemos pasando de frenada</h2>\n<p><strong>Cada vez más personas reconocen que estarían dispuestas a probar el sexo con robots</strong>, y el creciente realismo de los mismos (junto, claro está, a la desaparición de ciertos tabúes culturales) parece tener un papel fundamental en esa tendencia.</p>\n<!-- BREAK 4 -->\n<p>Hoy, los \'sexbots\' están lejos ya de ser las clásicas muñecas de silicona representadas por los primeros modelos, sino que <a href=\"https://www.xataka.com/robotica-e-ia/tenemos-nuevos-detalles-realdoll-x-robot-sexual-inteligencia-artificial-que-finalmente-estara-disponible-septiembre\">empiezan ya a incorporar inteligencia artificial</a> que <strong>les permite entender y responder a determinadas preguntas</strong> (y recordar datos que juzguen relevantes de las conversaciones), <strong>así como expresar emociones</strong>.</p>\n<!-- BREAK 5 -->\n<p>Brick Dollbanger, un \'probador experto\' de esta clase de robots, afirma que la introducción de la tecnología 5G ayudará a que los nuevos modelos cuenten con un flujo más constante de información desde el software, obtendrá una mejor sincronización y un movimiento más suave de su hardware.</p>\n<!-- BREAK 6 -->\n<blockquote>\n<p>\"Esa es la clave de la evolución sintética: no sólo contar con movimiento, sino que éste sea semejante al humano hasta el punto de ser indistinguibles de las personas\".</p>\n</blockquote>\n<div class=\"base-asset-media\"><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"200\" id=\"audio_30194522\" scrolling=\"no\" src=\"https://www.ivoox.com/player_ej_30194522_4_1.html?c1=ff6600\" style=\"border:1px solid #EEE; box-sizing:border-box; width:100%;\"></iframe></div>\n<p>De hecho, el creciente realismo de los nuevos modelos genera <a href=\"https://www.xataka.com/robotica-e-ia/robots-sexuales-estan-aqui-leyes-no-estan-al-dia-problemas-eticos-privacidad-que-conllevan\">reticencias de todo tipo</a> ante dicha evolución, y ha llevado a futurólogos como Ian Pearson a predecir que dentro de poco empezaremos a ver <strong>\'leyes visuales\' que busquen impedir un exceso de parecido con humanos reales</strong>. <a href=\"https://www.mirror.co.uk/tech/sex-robots-could-subject-visual-16494038\">Según Pearson</a>, dichas leyes podrían obligar a los fabricantes a dotar de características antinaturales a sus modelos (por ejemplo, de unos ojos verde brillante).</p>\n<!-- BREAK 7 -->\n<p>Vía | <a href=\"https://futurism.com/the-byte/invented-sex-robot-breathes\">Futurism.com</a></p>\n<p>Imagen | <a href=\"https://commons.wikimedia.org/wiki/File:Robots_Repaired_While_U_Wait_(Galaxy_September_1954).jpg\">Ed Emshwiller</a></p>\n<!-- BREAK 8 --> </div>', '2019-10-31 21:01:48', '2019-10-31 21:01:48', 42, 'portada2.jpg', 'Una cavidad que permita simular la respiración, unida a la capacidad de mover de forma realista las extremidades, son las características anunciadas para próximas generaciones del sexbot Emma, del fabricante chino AI-AItech.'),
(4, 'La inteligencia artificial AlphaStar se proclama \'gran maestro\' de Starcraft II en igualdad de condiciones frente a los humanos', '<div class=\"blob js-post-images-container\">\n<p>Hace dos años, AlphaGo, <strong>una inteligencia artificial desarrollada por DeepMind</strong>, <a href=\"https://www.xataka.com/robotica-e-ia/alphago-aplasta-al-mejor-jugador-del-mundo-de-go-la-inteligencia-artificial-es-imbatible\">derrotaba al mejor jugador de Go del mundo</a>. Pocos meses después, otra IA sucesora de aquella, AlphaZero, lograba <a href=\"https://www.xataka.com/robotica-e-ia/alphazero-de-deepmind-ya-es-la-mejor-del-mundo-en-varios-juegos-de-mesa-solo-se-esta-entrenando\">batir a humanos en varios juegos de mesa (shogi, go, ajedrez)</a> y además lo hacía tras <strong>aprender a jugar compitiendo contra sí misma</strong>.</p>\n<!-- BREAK 1 -->\n<p>En enero de este mismo año, era la IA <a href=\"https://www.xataka.com/robotica-e-ia/primero-fue-el-go-y-ahora-la-inteligencia-artificial-de-google-quiere-jugar-a-la-perfeccion-a-starcraft-ii\">AlphaStar la que se hacía con la victoria</a> tras una partida contra un equipo de jugadores profesionales de <a href=\"https://www.vidaextra.com/estrategia/mejor-juego-de-estrategia-del-2010-starcraft-ii\">StarCraft II, un videojuego de estrategia</a> en tiempo real, dotado de una gran complejidad. Pero <strong>ahora DeepMind ha actualizado AlphaStar</strong>, y ésta ha logrado superar un nuevo hito de la IA.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Nature acaba de publicar el estudio llevado a cabo por los investigadores de DeepMind, en el que demuestran que <strong>esta IA ha logrado colarse entre el 0,2% de los mejores jugadores del mundo del clásico de Blizzar</strong>d, los que ostentan el título de \'Grandmaster\'... y lo ha hecho, al contrario que en gestas similares, sometiéndose exactamente <strong>a las mismas reglas que los jugadores humanos</strong>.</p>\n<!-- BREAK 3 -->\n\n<p>Para ello, la IA veía el mapa de juego a través de una \'cámara\' (y no la totalidad del escenario) y <strong>se autolimitaba la frecuencia de las acciones para igualarlas con las de un ser humano</strong>: 22 acciones no duplicadas cada cinco segundos de juego, para alinearlo con el movimiento humano estándar. Además, lo ha hecho <strong>jugando a través del servidor oficial, Blattle.net</strong>, sin ningún tipo de adaptación o personalización.</p>\n<!-- BREAK 4 -->\n<h2>Las maravillas del aprendizaje por refuerzo</h2>\n<p>\"AlphaStar <strong>ha logrado el nivel de gran maestro sólo con una red neural y algoritmos de aprendizaje de propósito general</strong> que eran inimaginables hace solo diez años, cuando investigaba inteligencias artificiales para StarCraft basadas en sistemas de reglas\", afirma <a href=\"https://esports.xataka.com/industria/oriol-vinyals-espanol-detras-ultimo-exito-deepmind-ia-capaz-arrollar-a-dos-profesionales-starcraft-ii\">el español Oriol Vinyals</a>.</p>\n<!-- BREAK 5 -->\n<p>Vinyals, quien hace unos años fuera \"un jugador bastante serio\" de StarCraft, según sus propias palabras, es hoy en día el responsable del equipo de investigadores de DeepMind que han desarrollado AlphaStar, y que han logrado perfeccionar la técnica que inauguró AlphaGo Zero: el aprendizaje por refuerzo.</p>\n<!-- BREAK 6 -->\n\n<p>Éste permite que la máquina aprenda a alcanzar una desempeño excelente en juegos a fuerza de jugar millones de partidas contra sí misma, <strong>en lugar de aprender del análisis de partidas jugadas por humanos</strong>, como hiciera el primer AlphaGo. El objetivo de la IA es maximizar la recompensa esperada, y toma nota de las tácticas que mejor contribuyen a dicho objetivo, para replicarlas después. </p>\n<!-- BREAK 7 -->\n<p>Al entrenar dentro de simulaciones, <strong>AlphaStar ha podido pasar jugando el equivalente a cientos de años</strong> en unos pocos de nuestros meses, antes de <a href=\"https://www.xataka.com/inteligencia-artificial/cualquier-jugador-europeo-starcraft-ii-tendra-oportunidad-desafiar-a-alphastar-ia-deepmind\">\'salir\' a Internet a jugar contra humanos de forma anónima</a>.</p>\n<!-- BREAK 8 -->\n<p>\"El sistema muestra mucha habilidad a la hora de saber cuándo enfrentarse a un enemigo o no hacerlo [pero] no parece situado más allá del nivel que un humano podría alcanzar\", afirman los jugadores del Team Liquid, uno de los equipos profesionales de jugadores de este videojuego. \"<strong>Todavía es posible encontrar algunas debilidades en el sistema</strong>\", añaden, lo que explica que la IA aún perdiera algunas partidas contra humanos.</p>\n<!-- BREAK 9 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Jugadores de Starcraft\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/492e06/1280px-starcraft_gamescom_2017_-36851382835-/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/492e06/1280px-starcraft_gamescom_2017_-36851382835-/450_1000.jpg 450w, https://i.blogs.es/492e06/1280px-starcraft_gamescom_2017_-36851382835-/650_1200.jpg 681w, https://i.blogs.es/492e06/1280px-starcraft_gamescom_2017_-36851382835-/1024_2000.jpg 1024w, https://i.blogs.es/492e06/1280px-starcraft_gamescom_2017_-36851382835-/1366_2000.jpg 1366w\"/><noscript><img alt=\"Jugadores de Starcraft\" src=\"https://i.blogs.es/492e06/1280px-starcraft_gamescom_2017_-36851382835-/450_1000.jpg\"/></noscript> <span> Jugadores de Starcraft durante la Gamescom 2017. Autor: \'dronepicr\' en Flickr </span> </div></div></div>\n<p>Desde DeepMind, afirman que las técnicas desarrolladas para convertir a AlphaStar en uno de los mejores jugadores de StarCraft II del mundo <strong>pueden servir a partir de ahora para ofrecer aplicaciones reales</strong> mediante del uso de IAs capaces de trabajar \"en medios complejos, dinámicos y con múltiples actores\".</p>\n<!-- BREAK 10 -->\n<p>Vía | <a href=\"https://www.theverge.com/2019/10/30/20939147/deepmind-google-alphastar-starcraft-2-research-grandmaster-level\">The Verge</a></p>\n<!-- BREAK 11 -->\n<p>Imagen | <a href=\"https://www.flickr.com/photos/30478819@N08/20134232668\">Marco Verch</a></p>\n</div>', '2019-10-31 12:31:43', '2019-10-31 12:31:43', 16, 'portada3.jpg', 'Esta IA desarrollada por DeepMind se desenvuelve con \'StarCraft II\' mejor que el 99,8% de los jugadores humanos.'),
(5, 'El MIT afronta un gran reto de los coches autónomos: poder \'ver\' a la vuelta de la esquina usando la inteligencia artificial', '<div class=\"blob js-post-images-container\">\n<p>La mayoría de vehículos autónomos recurren a <a href=\"https://www.xataka.com/investigacion/el-futuro-de-los-coches-autonomos-podria-estar-en-este-diminuto-sensor\">la tecnología LiDAR</a> como método de detección de obstáculos, evitando así las colisiones tanto con mobiliario urbano como con peatones y otros vehículos. <strong>Pero los sensores LiDAR cuentan con una limitación</strong>: sólo pueden evitar chocar con elementos que se encuentren en, digamos, su \"campo de visión\". </p>\n<!-- BREAK 1 -->\n<p>A la hora de evitar una colisión, sin embargo, incluso <strong>unas milésimas de segundo pueden marcar la diferencia</strong>. De modo que un equipo de investigadores del MIT han decidido exprimir las capacidades de la inteligencia artificial para lograr <strong>prever la aparición de obstáculos incluso antes de que sean visibles</strong>, permitiendo adelantarse a la aparición de los peatones que salen de entre los coches o a las motos que no podemos ver hasta el último momento porque quedan ocultas por alguna columna de un aparcamiento subterráneo.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>¿Y cómo han conseguido esto? <strong>Fácil: fijándose en las sombras</strong>. Sí: la tecnología desarrollada por el CSAIL (el Laboratorio de Ciencias de la Computación e Inteligencia Artificial del MIT) es capaz de detectar pequeñas variaciones en las sombras del suelo haciendo uso de visión artificial para determinar si hay algo viniendo hacia nosotros a la vuelta de la esquina.</p>\n<!-- BREAK 3 -->\n\n<h2>ShadowCamera nos ve venir (en las condiciones adecuadas)</h2>\n<p>Por ahora, el sistema  <strong>sólo ha sido probado con éxito en entornos de interior</strong>, donde las velocidades tienden a ser más bajas, y la iluminación más consistente, lo que facilita la labor de análisis de las sombras. Confían en lograr aumentar su efectividad bajo otras condiciones a lo largo de los próximos meses.</p>\n<!-- BREAK 4 -->\n<p>En palabras de Daniela Rus, directora del CSAIL y coautora de la investigación,</p>\n<blockquote>\n<p>\"Para aquellas tareas en las que los robots se mueven a través de entornos con presencia de objetos o personas en movimiento, nuestro método puede proporcionar al robot una alerta temprana que advierta de que alguien se aproxima, permitiendo al vehículo reducir la velocidad, adaptar su trayectoria, y prepararse con antelación para evitar una colisión\".</p>\n</blockquote>\n<p>Para asegurarse de que el sistema sistema (<strong>bautizado con el descriptivo nombre de \'ShadowCam\'</strong> o \'cámara de sombras\') es capaz de detectar cambios prácticamente invisibles para el ojo humano, el MIT ha combinado la captura de imágenes y la odometría visual (una tecnología que podemos encontrar, por ejemplo en los Mars Rovers de la NASA).</p>\n<!-- BREAK 5 -->\n\n<p>Los investigadores han calculado que eso permite <strong>ganar más de medio segundo con respecto a los coches que se limitan al tradicional uso del LiDAR</strong>. Pero su utilidad futura va más allá de los coches autónomos: en el futuro, los robots que se desplacen transportando materiales por pasillos de hospitales o fábricas, por poner un ejemplo, podrán recurrir a ShadowCam o alguna tecnología similar para sortear obstáculos.</p>\n<!-- BREAK 6 -->\n<p>Vía | <a href=\"https://news.mit.edu/2019/helping-autonomous-vehicles-see-around-corners-1028\">MIT News\n</a></p>\n<!-- BREAK 7 -->\n<p>Imagen | Imagen de <a href=\"https://pixabay.com/es/users/jarmoluk-143740/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=427955\">Michal Jarmoluk</a> en <a href=\"https://pixabay.com/es/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=427955\">Pixabay</a></p>\n<!-- BREAK 8 --> </div>', '2019-10-29 12:01:16', '2019-10-29 12:01:16', 33, 'portada4.jpg', 'ShadowCam es capaz de \'vernos venir\' analizando las sombras que proyectamos en el suelo, incluso si nos oculta una columna.'),
(6, 'Dotar a las máquinas de los conocimientos de física de un bebé humano, un paso necesario para la conducción 100% autónoma', '<div class=\"blob js-post-images-container\">\n<p>Hay un motivo fundamental por el que el camino hacia la creación de robots y vehículos autónomos (y nos referimos a los <a href=\"https://www.xataka.com/entrevistas/nivel-5-conduccion-autonoma-no-sera-posible-a-nivel-tecnico-2024-2025-javier-gonzalez-presidente-bosch-espana-portugal\">completamente autónomos</a>) está dando tantos dolores de cabeza a los expertos: <strong>los robots vienen a ser \'idiot savants\' capaces de hacer muy bien una única cosa, pero sólo esa</strong>, mientras que su inteligencia artificial desaparece a la hora de hacer frente al resto de situaciones. E incluso a la hora de realizar su tarea habitual fuera de sus condiciones de trabajo ideales.</p>\n<!-- BREAK 1 -->\n<p>Por desgracia, la tarea de un vehículo autónomo, que <strong>se desplaza por el mundo real y se ve forzado a interactuar con otros vehículos, con personas, y animales</strong> (por no mencionar las condiciones meteorológicas, etc) no puede ser más polifacética y más carente de \"condiciones de trabajo ideales\". Es decir, no podrán mostrar verdadera autonomía hasta que no cuenten con una capacidad mínima de conocimiento de su entorno. <strong>Nada exagerado: basta con la que tendría un bebé de unos pocos meses de edad</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n\n<p>Un bebé humano empieza a desarrollar, en torno a los dos meses de vida, <strong>modelos físicos mentales que les permiten prever cómo funcionan las cosas a su alrededor</strong> (cómo colocar sobre el suelo, por ejemplo, un objeto que no habíamos visto antes, de tal forma que mantenga una posición vertical); pocos meses más tarde son capaces de anticipar cómo caerá al cubo un material dependiendo de si es un fluido o un sólido (arena o una piedra). </p>\n<!-- BREAK 3 -->\n<h2>\"Incluso un niño de cinco meses podría entenderlo. ¡Que me traigan un niño de cinco meses!\"</h2>\n<p>Hasta ahora hemos sido <strong>incapaces de replicar esos modelos físicos que poseemos de manera innata en las máquina</strong>s \'inteligentes\' que creamos, explica Avideh Zakhor, profesor de Berkeley experto en visión artificial. Y esa es la clase de problema a la que deberemos dar solución si queremos dar nuevos pasos adelante en el desarrollo de máquinas autónomas.</p>\n<!-- BREAK 4 -->\n<p>Lograrlo no sólo permitirá a estos vehículos navegar por el mundo real, sino que <strong>incluso podría ahorrar potencia de cálculo</strong>, según afirma Lochlainn Wilson, director general de SE4, una compañía japonesa que diseña robots con la intención de que un día puedan operar en Marte: dada la <a href=\"https://www.xataka.com/inteligencia-artificial/adios-a-houston-tenemos-problema-nasa-quiere-que-misiones-que-vayan-a-marte-pidan-ayuda-a-inteligencia-artificial\">latencia del envío de datos entre la Tierra y el planeta rojo</a>, la realización de tareas complejas de forma autónoma cobra más importancia que nunca.</p>\n<!-- BREAK 5 -->\n\n<p>La solución adoptada por SE4 y otras compañías del sector pasa por <a href=\"https://www.xataka.com/inteligencia-artificial/robots-no-necesitan-profesores-humanos-les-basta-realismo-simulaciones\">entrenar a sus IAs con simulaciones</a> recurriendo a lo más parecido que hemos sido capaces de crear hasta ahora a nuestros modelos innatos: los <a href=\"https://www.xataka.com/videojuegos/mundos-fotorrealistas-seres-humanos-virtuales-tres-videos-nuevas-e-impresionantes-capacidades-unity-unreal-engine\">motores gráficos de videojuegos</a>, o bien versiones más complejas de los mismos como Bullets Physics, una creación open source de un ingeniero de Google que pone especial <strong>empeño en reproducir aspectos de la física real, como la fricción</strong>.</p>\n<!-- BREAK 6 -->\n<p>Por supuesto, hay otro aspecto que debe evolucionar al mismo ritmo que la comprensión artificial de la física: la <a href=\"https://www.xataka.com/robotica-e-ia/el-video-que-muestra-como-luce-nuestro-mundo-ante-los-ojos-de-la-inteligencia-artificial\">percepción del mundo físico</a> a través de la <strong>visión por ordenador</strong>. Según Gary Marcus, co-fundador de la robótica empresa Robust.ai:</p>\n<!-- BREAK 7 -->\n<blockquote>\n<p>\"No tendremos robots domésticos, por ejemplo, hasta las máquinas no puedan interpretar bien las escenas. Tendremos Roombas, pero no a \'Rosie la robot\' [la chacha robótica de los Supersónicos]\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://spectrum.ieee.org/robotics/artificial-intelligence/lets-build-robots-that-are-as-smart-as-babies\">IEEE Spectrum</a> &amp; <a href=\"https://www.axios.com/newsletters/axios-future-a4e9b0f2-3e49-433a-91f1-8aad170700d9.html?chunk=1#story1\">Axios</a></p>\n<!-- BREAK 8 -->\n<p>Imagen | Maxpixel</p>\n</div>', '2019-10-27 20:32:42', '2019-10-27 20:32:42', 23, 'portada5.jpeg', 'Hasta ahora hemos sido incapaces de replicar en las máquinas \'inteligentes\' que creamos esos modelos físicos que nuestras mentes manejan antes de que cumplamos un año.'),
(7, 'El reconocimiento facial de resonancias magnéticas amenaza el anonimato de participantes en estudios médicos según la Clínica Mayo', '<div class=\"blob js-post-images-container\">\n<p>Miles de personas en todo el mundo se han sometido alguna vez a alguna prueba médica (desde resonancias magnéticas a test análisis genéticos, por ejemplo) en el marco de un estudio clínico. Los que participan en esta clase de investigaciones <strong>lo hacen porque cuentan con que su privacidad está a salvo</strong>, pues no se conservan datos que permitan identificar a los sujetos que participan en las mismas.</p>\n<!-- BREAK 1 -->\n<p>Por desgracia, <strong>el límite que establecía hasta ahora qué clase de datos pueden considerarse \'seguros\' en ese sentido puede haber quedado obsoleto</strong> por culpa de los avances en el campo del big data y de la inteligencia artificial.</p>\n<!-- BREAK 2 -->\n<!--more-->\n\n<p>Eso afirma al menos un equipo de investigadores de la Clínica Mayo, que acaban de publicar en la \'New England Journal of Medicine\' los resultados de un experimento con el que demuestran que <strong>es posible reconstruir el rostro de una persona gracias a los datos recogidos en una resonancia magnética de la cabeza</strong>, permitiendo después someter la reconstrucción a un software de reconocimiento facial basado en Azure. Con éxito.</p>\n<!-- BREAK 3 -->\n<p>Los investigadores reclutaron a 84 voluntarios, de edades comprendidas entre los 34 y los 89 años, que recientemente se hubieran sometido a alguna resonancia magnética durante un estudio clínico. Se fotografió a los voluntarios desde cinco ángulos distintos, y se procedió a intentar reconstruir sus rostros a partir de los datos disponibles: <strong>la resonancia magnética permite captar elementos como el contorno de la piel</strong>, la grasa intramuscular y la médula ósea del cráneo, pero no otros muy útiles, como el hueso o el cabello.</p>\n<!-- BREAK 4 -->\n<p><strong>En 70 de los 84 casos, el algoritmo usado permitió vincular directamente el rostro reconstruido a las fotografías de su propietario</strong>, y en otros 10 el software propuso el rostro correcto entre las 5 primeras opciones. En total, la tecnología sólo fue incapaz de identificar 4 de los 84 rostros.</p>\n<!-- BREAK 5 -->\n\n<h2>¿Dónde está el problema?</h2>\n<p>El gran problema de esta vía imprevista de \'desanonimización\' de datos médicos reside en que <strong>abre la puerta a que éstos sean usados con fines comerciales o, peor, delictivos (como usarlos con fines de chantaje)</strong>. Ahora mismo, la única protección de la privacidad de los sujetos de pruebas clínicas que requieran resonancias magnéticas de la cabeza, reside en los que los investigadores que accedan a los datos de los estudios se comprometan a no tratar de identificar a los participantes.</p>\n<!-- BREAK 6 -->\n<p>Según explicó al WSJ Eliot Siegel, profesor de radiología en la Escuela de Medicina de la Universidad de Maryland, \"el riesgo para el paciente medio es [ahora] muy pequeño... pero <strong>a medida que pase el tiempo, el riesgo aumentará y es muy importante tener esto en cuenta mientras seguimos creando datasets</strong> cada vez mayores destinados a impulsar el machine learning\".</p>\n<!-- BREAK 7 -->\n<p>Vía | <a href=\"https://www.wsj.com/articles/facial-recognition-software-was-able-to-identify-patients-from-mri-scans-11571864543\">Wall Street Journal</a></p>\n<!-- BREAK 8 -->\n<p>Imagen | Mayo Clinic / New England Journal of Medicine</p>\n</div>', '2019-10-26 01:15:55', '2019-10-26 01:15:55', 25, 'portada6.jpg', 'Los últimos estudios muestran que la inteligencia artificial permite \'desanonimizar\' datos médicos teóricamente anonimizados: una resonancia magnética de la cabeza permite reconstruir un rostro listo para aplicar el reconocimiento facial.'),
(8, 'Animar el Will Smith digital de \'Géminis\' ha costado millones, pero Hollywood ve cercano el uso masivo de los deepfakes en el cine', '<div class=\"blob js-post-images-container\">\n<p>En la innovadora película <a href=\"https://www.xataka.com/cine-y-tv/17-peliculas-de-ciencia-ficcion-para-visitar-si-te-gusto-la-llegada\">\'El Congreso\'</a>, Robin Wright se interpretaba a sí misma, o al menos a una versión de ella que aceptaba vender su imagen, su rostro y expresiones a una multinacional para que pudiera usarla libremente como intérprete (digital) de nuevas películas.</p>\n<!-- BREAK 1 -->\n<!--more-->\n<p>Sólo tenía que meterse en una gran jaula <strong>donde tomarían trillones de fotos simultáneas de cada pose y gesto</strong>: cuando saliera de la misma ya no sería actriz, sólo la inspiración de la nueva (y eternamente joven) Robin Wright.</p>\n<!-- BREAK 2 -->\n<p>Sólo 6 años después del estreno de \'El Congreso\', podemos encontrar en las carteleras una película bien distinta, <a href=\"https://www.xataka.com/cine-y-tv/geminis-analisis-will-smith-se-enfrenta-a-su-yo-20-anos-gracias-a-cgi-efectivo-divertido-toque-serie-b\">\'Géminis\'</a>, cuyo leitmotiv se base a enfrentar a Will Smith contra <strong>una copia digital de sí mismo... 30 años más joven</strong>.</p>\n<!-- BREAK 3 -->\n\n<p>Lejos de lo ocurrido en intentos anteriores de Hollywood para lograr reproducir digitalmente la imagen de sus intérpretes, esta película de acción dirigida por Ang Lee <strong>logra cruzar con éxito el \'valle inquietante\'</strong> (ese estadio en que los rostros artificiales son ya muy realistas pero <a href=\"https://www.xataka.com/videojuegos/los-colonos-en-el-valle-inquietante\">no lo bastante como para no resultar perturbadores</a>) en el que durante los últimos años han caída tantas otras películas, como \'La Momia II\'.</p>\n<!-- BREAK 4 -->\n<p>Un valle inquietante que, por cierto, ya <strong>cualquier experto en deepfakes puede ayudar a sortear</strong>:</p>\n<!-- BREAK 5 -->\n<p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">We fixed the VFX shot of the Scorpion King from the Mummy Returns. Top row is the original, bottom row is our take. How do you guys think we did? What do you think <a href=\"https://twitter.com/TheRock?ref_src=twsrc%5Etfw\">@TheRock</a> would think? <a href=\"https://t.co/XJnFiYkmrm\">pic.twitter.com/XJnFiYkmrm</a></p>— Corridor (@CorridorDigital) <a href=\"https://twitter.com/CorridorDigital/status/1149760459186118656?ref_src=twsrc%5Etfw\">July 12, 2019</a></blockquote> <script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script></p>\n<!-- BREAK 6 -->\n<p>Con \'Géminis\', sin embargo, casi todo (salvo una criticada escena al final de la película) <strong>ha sido perfecto</strong>: el joven Will Smith mostraba un sudor y un brillo de piel realistas, un movimiento de ojos y párpados sincronizado con su lenguaje corporal... Smith, al contrario que el personaje de Robin Wright, se muestra entusiasmado con su \'doble\':</p>\n<!-- BREAK 7 -->\n<blockquote>\n<p>\"Ahora hay un modelo totalmente digital de mi yo con 23 años. [...] Ya puedo ponerme gordo mientras uso a mi joven gemelo [en las películas]\".</p>\n</blockquote>\n<p><strong>Se abre ahora la puerta a una nueva industria del entretenimiento</strong>, que no tenga que renunciar a personajes cuando los actores que los encarnaron decidan cambiar de aires (imaginemos, no sé, a Hugh Jackman encarnando a Lobezno durante otros 20 años); o en el que no sea necesario contratar a varios actores para encarnar diferentes momentos de la vida de un mismo personaje.</p>\n<!-- BREAK 8 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Rejuvenecidos\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/1ea481/rejuvenecidos/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/1ea481/rejuvenecidos/450_1000.jpg 450w, https://i.blogs.es/1ea481/rejuvenecidos/650_1200.jpg 681w, https://i.blogs.es/1ea481/rejuvenecidos/1024_2000.jpg 1024w, https://i.blogs.es/1ea481/rejuvenecidos/1366_2000.jpg 1366w\"/><noscript><img alt=\"Rejuvenecidos\" src=\"https://i.blogs.es/1ea481/rejuvenecidos/450_1000.jpg\"/></noscript> <span> Will Smith no ha sido el único rejuvenecimiento espectacular que nos ha ofrecido Hollywood últimamente (Fotogramas de \'El irlandés\' y \'Ant-Man\', respectivamente) </span> </div></div></div>\n<h2>Los deepfakes como democratización de los efectos digitales</h2>\n<p>Pero si hay algo que evita que esa puerta esté abierta ya de par en par es el factor crematístico. <strong>El presupuesto, vaya</strong>. Darren Hendler, de VFX House Digital Domain, estimaba recientemente en Hollywood Reporter que la creación del \'activo\' (el modelo humano digitalizado) de \'Géminis\' podría haber costado entre 500.000 y un millón de dólares, a lo que se suman hasta 100.000 dólares más por cada escena.</p>\n<!-- BREAK 9 -->\n<p>El Financial Times, por su parte, hace un cálculo más ambicioso y menciona fuentes de la industria que <strong>estiman que el \'activo\' costó el doble del sueldo del verdadero Will Smith</strong>, lo que situaría la cifra en unos 25 millones de dólares.</p>\n<!-- BREAK 10 -->\n<p>Sin embargo, sólo unas semanas antes del estreno de \'Géminis\', un youtuber identificado únicamente como Sham00k recurrió al software \'open source\' DeepFaceLab para <strong>introducir a Will Smith en la que posiblemente sea la película más famosa que ha rechazado protagonizar: \'Matrix\'</strong>. El resultado está lejos aún del de \'Géminis\', pero no tanto como debería estarlo si atendemos a la diferencia de presupuestos (prácticamente cero en el caso del deepfake).</p>\n<!-- BREAK 11 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/1h-yy3h1u04\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Y la tendencia es clara: según Paul Franklin, cofundador del estudio de efectos visuales DNEG, \"el precio del realismo se ha reducido drásticamente en los últimos 20 años\", <strong>y ese descenso se ha acelerado desde que los deepfakes han entrado en escena</strong>. Peter Rojas, de Betaworks Ventures, los ve como \"la democratización de los efectos especiales\".</p>\n<!-- BREAK 12 -->\n<p>Hendler, por su parte, cree que lo visto en \'Géminis\' y en la inminente \'El irlandés\', \"serán los últimos modelos 100% digitales de actores humanos que no utilicen alguna clase de <a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-gans-redes-generativas-antagonicas\">GAN (red generativa antagónica)</a> como parte del proceso\". Y añade otro vaticinio: \"creo que comenzaremos a ver esta clase de cosas en producciones de menor presupuesto\".</p>\n<!-- BREAK 13 -->\n<p>Vía | <a href=\"https://www.ft.com/content/9df280dc-e9dd-11e9-a240-3b065ef5fc55\">FT</a> &amp; <a href=\"https://www.hollywoodreporter.com/behind-screen/rise-all-digital-actor-1229783\">Hollywood Reporter</a></p>\n<!-- BREAK 14 -->\n<p>Imagen | Paramount Pictures</p>\n</div>', '2019-10-24 17:01:28', '2019-10-24 17:01:28', 24, 'portada7.jpg', 'El Financial Times cifra el coste del joven Smith en el doble del sueldo del actor real. Pero la introducción de la tecnología deepfake permitirá pronto ver cosas similares con presupuestos mucho más reducidos.'),
(9, 'La inteligencia artificial nos ayuda a tomar decisiones difíciles, pero tendemos a permitir (y preferir) que decida por nosotros', '<div class=\"blob js-post-images-container\">\n<p>Una de las funciones que le estamos dando a la inteligencia artificial es la de <strong>asesorar a humanos responsables de tomar decisiones críticas</strong>, como \"¿cuál es la pena adecuada que debemos imponer a este reo?\" o \"¿qué medicamento hay que recetarle a este paciente?\". La idea es que, como la IA a veces es capaz de <a href=\"https://www.xataka.com/robotica-e-ia/conocer-sexo-alguien-foto-su-retina-parecia-imposible-ahora-ia-ha-logrado-no-sabemos-como\">ver cosas que nosotros no vemos</a>, sus sugerencias nos ayudarán a elegir más sabiamente.</p>\n<!-- BREAK 1 -->\n<p>Pero esa es únicamente la teoría: en la práctica, los responsables de tomar esas decisiones, bien sea por sufrir excesiva carga de trabajo, bien por tener una excesiva fe en las máquinas, <strong>tienden en demasiados casos a dar por buenos los consejos de los algoritmos</strong> y a actuar siguiendo la misma a pies juntillas.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p><strong>Esto es lo que suele denominarse \'sesgo de automatización\'</strong>: la falta de escepticismo ante la información que nos proporcionan los algoritmos. Paradójicamente, aquí somos nosotros, y no las máquinas, quienes pecamos de actuar de forma automática. Y como todos los sesgos, tendemos a negarlos.</p>\n<!-- BREAK 3 -->\n\n<h2>Si lo dice la IA, ¿será por algo?</h2>\n<p>Los primeros estudios sobre el sesgo de automatización se centraron en los sistemas de navegación de los aviones: <a href=\"https://www.xataka.com/inteligencia-artificial/que-lanzamiento-misiles-nucleares-ataque-enemigo-quede-manos-ia-propuesta-dos-expertos-ee-uu\">un estudio</a> publicado hace ya casi una década, recogía que los pilotos tendían a afirmar que <strong>no confiarían en un sistema automatizado</strong> que les informase de un incendio en el motor a menos que contaran con evidencias complementarias que lo corroboraran; sin embargo, una vez inmersos en las simulaciones, optaron por dar por bueno, de manera acrítica, el aviso de incendio.</p>\n<!-- BREAK 4 -->\n<p>Y esto es un problema. En primer lugar, porque sabemos que la información ofrecida por una inteligencia artificial puede fallar, bien por <a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-inteligencia-artificial-antagonica-como-puede-manipular-a-otras-ias\">haber sido \'hackeada\'</a>, bien por <a href=\"https://www.xataka.com/robotica-e-ia/hay-quien-critica-a-mucha-inteligencia-artificial-como-racista-etnocentrica-problema-esta-datos\">haber sido entrenada con datos erróneos o sesgados</a>. Y sin embargo, la desinformación de los usuarios y el excesivo marketing que se ha ido construyendo en torno a esta tecnología <strong>la dotan de un injustificado halo de precisión</strong>.</p>\n<!-- BREAK 5 -->\n<p>Ryan Kennedy, profesor de la Universidad de Houston especialista en automatización, explica que \"<strong>cuando la gente tiene que tomar decisiones en plazos relativamente cortos, con poca información</strong>... ahí es cuando tiende a confiar en cualquier consejo que les proporcione un algoritmo\".</p>\n<!-- BREAK 6 -->\n\n<p>Según Axios, un estudio de publicación inminente elaborado por Matthew Lungren, del Centro de IA Medicinal de la Univ. de Stanford, ha detectado que los médicos vinculados a su universidad siguen los consejos de una IA \"en algunos casos, aunque esté claro que son erróneos\". </p>\n<!-- BREAK 7 -->\n<p>Según Lungren, <strong>la solución recaería en proporcionar información sobre el nivel de fiabilidad de cada algoritmo</strong>, para que así los humanos responsables de la toma de decisiones pudieran poner en contexto los consejos que reciben.</p>\n<!-- BREAK 8 -->\n<p>Vía | <a href=\"https://www.axios.com/ai-automation-bias-trust-62ee0445-1fda-4143-b3d8-7d7ee8e328f6.html\">Axios</a></p>\n<!-- BREAK 9 -->\n<p>Imagen | <a href=\"https://www.flickr.com/photos/adammeek/34285917245\">Adam Meek</a></p>\n</div>', '2019-10-22 10:01:14', '2019-10-22 10:01:14', 39, 'portada8.jpg', 'La desinformación de los usuarios y el excesivo marketing que se ha ido construyendo en torno a la inteligencia artificial dotan a esta tecnología de un injustificado halo de precisión.');
INSERT INTO `posts` (`id`, `titulo`, `cuerpo`, `created_at`, `updated_at`, `user_id`, `imagen`, `descripcion`) VALUES
(10, 'La ONU, preocupada por un modelo de \'estado de bienestar digital\' que va dejando la política social en manos de algoritmos', '<div class=\"blob js-post-images-container\">\n<p>Philip Alston, relator especial para la pobreza de la ONU, presentó el pasado viernes ante la Asamblea General de la organización <a href=\"https://algorithmwatch.org/en/un-special-rapporteur-on-digital-technology-and-social-protection-denounces-a-human-rights-free-zone/\">un informe</a> en el que abordaba el concepto de \'Estado de bienestar digital\', en referencia a aquellos países que han optado por intentar <strong>mejorar la eficiencia de los sistemas de protección social recurriendo al uso del big data y la tecnología de inteligencia artificial</strong>.</p>\n<!-- BREAK 1 -->\n<p>Este es un fenómeno creciente: los servicios gubernamentales tienen que mantener sistemas de bienestar caros y los algoritmos prometen llegar al rescate facilitando y acelerando la toma de decisiones. Pero Alston denuncia en su informe que tales promesas no siempre se cumplen, y que este intento de mejorar los mecanismos públicos que intentan favorecer la equidad <strong>en muchos caos puede estar dañando la calidad de vida de los colectivos más vulnerables</strong>:</p>\n<!-- BREAK 2 -->\n<!--more-->\n<blockquote>\n<p>\"Este informe reconoce la atracción irresistible [a la que se exponen] que los gobiernos que se están moviendo hacia este modelo, pero advierte que existe un grave riesgo de que tropecemos como zombis con una distopía del bienestar digital\".</p>\n</blockquote>\n\n<h2>Los pobres, atrapados por algoritmos que ellos no comprenden (y parece que los legisladores tampoco)</h2>\n<p>Alston pone sitúa como ejemplo de esta distopía <a href=\"https://www.ohchr.org/EN/NewsEvents/Pages/DisplayNews.aspx?NewsID=25152&amp;LangID=E\">a programas como SyRI</a> (System Risk Indication), que permite a las autoridades nacionales y locales combinar múltiples categorías de datos en base a un \"modelo de riesgo\" no revelado que teóricamente permite identificar a algunas personas como más propensas a cometer fraude en la solicitud de ayudas.</p>\n<!-- BREAK 3 -->\n<p>Para Alston, si buscamos garantizar la justicia social, <strong>son necesarios \"cierto grado de transparencia y garantías de que determinados grupos no están siendo injustamente seleccionados\"</strong>, en referencia al hecho de que SyRI sólo se ha utilizado por ahora para analizar la población de barrios muy concretos.</p>\n<!-- BREAK 4 -->\n<blockquote>\n<p>\"Este problema se agrava cuando sabemos que, una vez que el programa fue dotado de base legal, prácticamente no hubo debate en el Parlamento holandés, los medios no dedicaron atención al asunto, y no hubo transparencia sobre el funcionamiento del sistema: el resultado es que los criterios [de SyRI] siguen siendo un misterio incluso para aquellos que lo han estudiado con detenimiento\".</p>\n</blockquote>\n<p>Teóricamente, el Reglamento Global de Protección de Datos de la UE <strong>garantiza un \"derecho a la explicación\"</strong> para promover la rendición de cuentas, pero los contornos precisos de esa protección en lo que respecta a sistemas automatizados <a href=\"https://www.law.ox.ac.uk/business-law-blog/blog/2018/05/rethinking-explainable-machines-next-chapter-gdprs-right-explanation\">no están del todo claros</a>.</p>\n<!-- BREAK 5 -->\n\n<p>Pocos días antes, The Guardian <a href=\"https://www.theguardian.com/technology/2019/oct/14/automating-poverty-algorithms-punish-poor\">se hacía eco</a> de los problemas que está causando en algunos países la introducción de este \"estado de bienestar digital\", desde el uso de la IA en Illinois para detectar excesos en la concesión de ayudas durante los últimos 30 años (lo que ha llevado a exigir ahora su reembolso), hasta la <strong>introducción -en países poco preparados para ello- de sistemas tecnológicos de los que depende la recepción de ayudas sociales</strong>.</p>\n<!-- BREAK 6 -->\n<p>Así, en la India, donde <strong>el acceso a las tiendas de racionamiento dependen del visto bueno del sistema biométrico Aadhar</strong>, los activistas han denunciado que errores en el mismo han provocado hasta 13 muertes de hambre, y que algunos pueblos han perdido durante dos meses el acceso a estas tiendas por carecer de conexión a Internet.</p>\n<!-- BREAK 7 -->\n<p>Pero no hace falta irse a casos tan lejanos y extremos: The Guardian también denuncia que, incluso en Occidente, millones de personas <strong>están viendo sus ayudas recortadas por sistemas informáticos que operan de un modo que pocos son capaces de controlar o entender</strong>, y en los que los errores de criterio se han convertido en endémicos, sin ninguna opción obvia para que las víctimas de los mismos recurran la decisión.</p>\n<!-- BREAK 8 -->\n<p>Vía | <a href=\"https://www.exponentialview.co/p/-algorithmic-decision-making-social\">ExponentialView</a></p>\n<!-- BREAK 9 -->\n<p>Imagen | PXhere</p>\n</div>', '2019-10-20 17:12:13', '2019-10-20 17:12:13', 21, 'portada9.jpg', 'Existe un grave riesgo de que \"tropecemos como zombis con una distopía del bienestar digital\", según Philip Alston, relator especial para la pobreza de la ONU.'),
(11, 'El proyecto que quiere que los drones no sólo graben vídeo de forma autónoma, sino que valoren la calidad de las tomas', '<div class=\"blob js-post-images-container\">\n<p>Actualmente, <strong>uno de los muchos usos que le damos a los drones es la grabación de vídeos en modo autónomo</strong>, una opción muy usada, por ejemplo, entre practicantes de deportes como el surf o el motorismo, que quieren dejar registradas sus hazañas pero no tienen ni el tiempo ni la capacidad de estar operando sus drones mientras las llevan a cabo.</p>\n<!-- BREAK 1 -->\n<p>Sin embargo, algo de lo que carecen esta clase de grabaciones es de sensibilidad artística: son funcionales, porque logran seguir con su cámara la acción, pero <strong>desconocen las preferencias visuales de los humanos</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Pero un equipo de investigadores del Instituto de Robótica de la Univ. Carnegie Mellon se ha propuesto solucionar este problema y acaban de publicar los <a href=\"https://arxiv.org/abs/1904.02579\">resultados de su investigación</a>. El título de la misma es \"<strong>¿Puede un robot de convertirse en director de cine?</strong> Aprendiendo los principios artísticos de la cinematografía aérea\".</p>\n<!-- BREAK 3 -->\n\n<blockquote>\n<p>\"El dron toma posición con el objetivo de registrar los aspectos más importantes de una escena, entendiendo de forma autónoma el contexto de la misma: dónde están los obstáculos, dónde están los actores... y es capaz de razonar sobre qué perspectivas harán visualmente más interesante al escena, así sobre cuáles le permitirán permanecer a salvo y no chocar\".</p>\n</blockquote>\n<h2>Cuantificar el arte y convertirlo en algoritmos</h2>\n<p>Claro está, <strong>cuantificar matemáticamente algo tan subjetivo como el interés artístico de una toma resulta problemático</strong>, de modo que el equipo optó por recurrir a <a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-aprendizaje-refuerzo\">una técnica llamada aprendizaje por refuerzo</a> profundo.</p>\n<!-- BREAK 4 -->\n<p>Pidieron a varias personas que puntuaran el atractivo visual de varias tomas alternativas para una misma acción, en la que variaban aspectos como la perspectiva, la distancia y la posición del actor en la pantalla (o la frecuencia con que cambiaban esos elementos a lo largo de la toma), y eso <strong>permitió que la IA aprendiese los patrones que, en principio, tenían en común las escenas mejor valoradas</strong>.</p>\n<!-- BREAK 5 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/ookhHnqmlaU\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Rogerio Bonatti, responsable del equipo de investigadores, afirma que <strong>uno de los aspectos en los que destaca su IA es a la hora de evitar las oclusiones</strong> (esto es, las ocasiones en las que los actores quedan involuntariamente tapados por obstáculos).</p>\n<!-- BREAK 6 -->\n<p>Hay que recordar que, al contrario que los sistemas hoy existentes, el sistema desarrollado por Bonatti y los suyos <strong>no requiere del uso de localizadores GPS, sino que sigue a los actores de forma inteligente</strong>, anticipando su trayectoria mediante algoritmos y cartografiando el entorno mediante LiDAR.</p>\n<!-- BREAK 7 -->\n\n<p>Bonatti, pese al título de su investigación, <strong>procura rebajar las expectativas: nada de drones directores de cine</strong>, \"el objetivo de esta investigación no es el de reemplazar a los humanos; no va a existir un mercado [de estos drones] para profesionales de cine. La idea es democratizar la cinematografía con drones\".</p>\n<!-- BREAK 8 -->\n<p>Como objetivo a futuro, Sebastian Scherer, otro integrante del equipo investigador, apunta a que una función que podrían incorporar esta clase de aparatos sería la de <strong>personalizar las preferencias artísticas en base a un determinado género cinematográfico o director</strong>.</p>\n<!-- BREAK 9 -->\n<p>Vía | <a href=\"https://www.inverse.com/article/60221-drones-filming-movies-how-directors\">Inverse</a></p>\n<!-- BREAK 10 -->\n<p>Imagen | <a href=\"https://www.pexels.com/photo/aerial-air-air-force-aircraft-392024/\">Pexels</a></p>\n</div>', '2019-10-19 01:05:04', '2019-10-19 01:05:04', 48, 'portada10.jpg', 'El drone que emocionó a Steven Spielberg. O el que le quitará el trabajo, aún no se sabe.'),
(12, 'Esta mano robótica de OpenAI ha aprendido sola a resolver un cubo de Rubik (y la meta es que aprenda a manejar cualquier objeto)', '<div class=\"blob js-post-images-container\">\n<p>Las manos humanas pueden realizar una gran cantidad de tareas, y más complejas de lo que imaginamos: son el resultado de millones de años de evolución, y por eso no cabe sorprenderse por <strong>lo poco que ha avanzado la robótica</strong>, en los 60 años que han transcurrido desde que la disciplina dio sus primeros pasos, <strong>a la hora de intentar imitar la destreza de nuestras extremidades</strong>.</p>\n<!-- BREAK 1 -->\n<p>Recordemos que si hay algo que caracteriza a la IA es su capacidad para lograr hacer cosas que a nosotros nos parecen imposibles... y de <strong>fallar en tareas sencillas y cotidianas para nosotros</strong> (lo que conocemos como la <a href=\"https://www.xataka.com/inteligencia-artificial/paradoja-moravec-que-inteligencia-artificial-hace-facil-dificil-viceversa\">Paradoja de Moravec</a>).</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Pero solventar ese reto que supone la réplica del movimiento manual humano constituye, para OpenAI, un paso ineludible en su búsqueda para desarrollar un robot de autoaprendizaje de propósito general. Es decir, de una máquina que sea capaz de llevar a cabo <strong>una amplia gama de tareas en el mundo real, sin necesidad de ser diseñada específicamente para cada una de ellas</strong>.</p>\n<!-- BREAK 3 -->\n\n<h2>Lento, pero seguro (y versátil)</h2>\n<p>Y por ello se han pasado este último año intentando (con éxito) que Dactyl, la mano robótica humanoide presentada por la propia OpenAI en 2018, <strong>aprendiera a resolver un cubo de Rubik con una sola mano</strong>, valga la redundancia.En palabras de Peter Welinder, investigador de robótica en OpenAI,</p>\n<!-- BREAK 4 -->\n<blockquote>\n<p>\"Hay un montón de robots capaces de resolver cubos de Rubik muy rápidamente. La gran diferencia entre lo logrado por ellos y lo que estamos haciendo aquí es que son robots con un diseño muy concreto: no hay manera de utilizarlos para llevar a cabo otras tareas diferentes\".</p>\n</blockquote>\n<p>Welinder hace referencia ahí a robots como el construido en 2016 por Infineon, diseñado específicamente para <strong>superar la velocidad humana a la hora de resolver un cubo de Rubik</strong> (y efectivamente logró superar el récord humano del momento).</p>\n<!-- BREAK 5 -->\n<p>Dos años más tarde, <a href=\"https://www.xatakaciencia.com/robotica/nuevo-record-robot-resuelve-un-cubo-de-rubik-en-0-38-segundos\">otros dos robots</a> (uno desarrollado por el MIT, y otro por el creador de un canal de Youtube japonés) lograron superar todas las marcas previas.</p>\n<!-- BREAK 6 -->\n\n<p>Estos logros se deben en parte al uso del <a href=\"https://www.xataka.com/inteligencia-artificial/resolver-cubos-rubik-tarea-que-inteligencia-artificial-no-mejor-opcion-supera-a-humana\">algoritmo de Kociemba</a>, que permite resolver los cubos en una veintena de movimientos, pese a ser artefactos que cuentan con <strong>43 millones de combinaciones</strong>.</p>\n<!-- BREAK 7 -->\n<p>El software desarrollado por OpenAI había logrado ya en 2017 resolver un cubo de Rubik dentro de una simulación, pero hacerlo en el mundo real, con todos esos movimientos de dedo y muñeca que exigía la manipulación del cubo, eran otro cantar. Llevaban más de un año trabajando en ello... <strong>hasta esta semana</strong>.</p>\n<!-- BREAK 8 -->\n<p>El vídeo publicado por OpenAI que muestra la hazaña también revela que l<strong>os movimientos de Dactyl son lentos e inseguros cuando los compramos con los de un ser humano</strong>. Tras muchos minutos, Dactyl finalmente es capaz de resolver el puzzle:</p>\n<!-- BREAK 9 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/kVmp0uGtShk\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>El gran avance de OpenAI se encuentra en la IA con que ha equipado a Dactyl: un sistema formado por <strong>dos redes neuronales</strong> que se coordinan no sólo para resolver el problema planteado, sino también para <strong>hacer frente a imprevistos</strong> (elementos externos que golpean el cubo, imposibilidad de mover ciertos dedos, llevar guantes, etc). </p>\n<!-- BREAK 10 -->\n<p>¿Y cómo lo logra? Exactamente igual que lo haría una persona: <strong>improvisando... y aprendiendo desde cero</strong>. El <a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-aprendizaje-refuerzo\">aprendizaje por refuerzo</a> permite que una mano robótica vaya aprendiendo poco a poco las mejores estrategias para resolver un problema, incluso si al comienzo del proceso era sencillamente incapaz de mover los dedos sin tirar el cubo.</p>\n<!-- BREAK 11 -->\n<p>Vía | <a href=\"https://www.theverge.com/2019/10/15/20914575/openai-dactyl-robotic-hand-rubiks-cube-one-handed-solve-dexterity-ai\">VentureBeat</a></p>\n<!-- BREAK 12 -->\n<p>Imagen | OpenAI</p>\n</div>', '2019-10-17 23:01:39', '2019-10-17 23:01:39', 37, 'portada11.jpg', 'OpenAI busca crear un robot que reproduzca la versatilidad de la mano humana a la hora de manejar objetos. Y el cubo de Rubik es un entrenamiento tan bueno como cualquier otro.'),
(13, 'Tencent Video modificará películas y series en China para introducir en ellas publicidad encubierta con efecto retroactivo', '<div class=\"blob js-post-images-container\">\n<p>Tencent Video es, con 97 millones de suscriptores, <strong>la mayor plataforma de vídeo en streaming de China</strong>; y su empresa matriz (la multinacional china <a href=\"https://www.xataka.com/videojuegos/tencent-asi-desconocido-gigante-chino-que-domina-industria-videojuego-dueno-fortnite-lol\">Tencent, propietaria también de Fortnite</a>) acaba de asociarse con la compañía de inteligencia artificial MirriAd para explorar <strong>una novedosa vía de monetización de su contenido</strong>, basada en la publicidad por emplazamiento (o \'product placement\').</p>\n<!-- BREAK 1 -->\n<p>Un momento, pero ¿el product placement no se basa en incluir productos comerciales reales (o anuncios de los mismos) en series y películas para promocionarlos? ¿Qué tiene eso de novedoso?  Muy sencillo: <strong>la idea es insertar promociones que no existían en el material original</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<h2>Ha nacido un nuevo formato publicitario</h2>\n<p>La integración del material promocional dentro de determinadas escenas preseleccionadas de las películas de Tencent Video <strong>se haría sobre la marcha y de forma totalmente automatizada</strong> gracias a la tecnología de visión por computador de <a href=\"https://www.xataka.com/otros/asi-va-a-ser-el-product-placement-retroactivo\">MirriAd</a>, y permitirá a los anunciantes contratar <strong>espacios personalizados</strong> dirigidos a un público concreto.</p>\n<!-- BREAK 3 -->\n\n<p>Así, dos personas que estén viendo al mismo tiempo el mismo capítulo podrán ver anuncios distintos, o no ver ninguno, según el perfil de cada usuario. <strong>Adiós a las pausas publicitarias</strong>: esta nueva tecnología hará posible ver los anuncios sin interrumpir la experiencia del espectador.</p>\n<!-- BREAK 4 -->\n<p>Si aún no te has hecho a la idea de cómo funcionaría, <strong>en el siguiente vídeo puedes ver un ejemplo</strong>:</p>\n<!-- BREAK 5 -->\n<p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">Wow! Worth watching this. China\'s largest video platform <a href=\"https://twitter.com/hashtag/Tencentvideo?src=hash&amp;ref_src=twsrc%5Etfw\">#Tencentvideo</a> (97M paying China subscribers) will begin inserting extra ads into movies/series that didn\'t exist in the original. <a href=\"https://twitter.com/hashtag/computervision?src=hash&amp;ref_src=twsrc%5Etfw\">#computervision</a> <a href=\"https://t.co/qltsQz9jdF\">pic.twitter.com/qltsQz9jdF</a></p>— Matthew Brennan (@mbrennanchina) <a href=\"https://twitter.com/mbrennanchina/status/1184114082804158464?ref_src=twsrc%5Etfw\">October 15, 2019</a></blockquote> <script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script></p>\n<!-- BREAK 6 -->\n<p>Esperemos que esta tecnología no termine usándose para <strong>obligarnos a ver obras clásicas del cine y TV saturadas de publicidad contextual</strong>, y que por el contrario sean capaces de usarla de forma inteligente: hace ya 8 años vimos cómo un capítulo repuesto de \'Cómo conocí a vuestra madre\' <a href=\"https://www.hypable.com/how-i-met-your-mother-is-inserting-new-ads-into-old-episodes/\">aprovechaba la ocasión</a> para actualizar uno de estos anuncios sutiles, cambiando una película de estreno (que ya había dejado de estarlo) por otra. Con suerte, la tecnología de MirriAd nos permitirá ver cosas así.</p>\n<!-- BREAK 7 -->\n<p>Stephan Beringer, CEO de MirriAd, afirma que su compañía está feliz de poder ofrecer este nuevo formato publicitario a los anunciantes chinos, pero avisa que <strong>su objetivo es asociarse con las mayores cadenas y plataformas digitales del mundo</strong>. De hecho, MirriAd ya trabaja también con compañías europeas y estadounidenses.</p>\n<!-- BREAK 8 -->\n<p>Vía | <a href=\"https://www.prnewswire.com/news-releases/mirriad-partners-with-tencent-one-of-the-worlds-largest-video-platforms-to-reach-huge-entertainment-audiences-with-branded-content-solution-300935061.html\">PR NewsWire</a>\nImagen | Material promocional de \'Blade Runner 2049\' (Sony Pictures Releasing, 2017)</p>\n<!-- BREAK 9 --> </div>', '2019-10-15 23:36:58', '2019-10-15 23:36:58', 4, 'portada12.jpg', 'Imagina estar viendo tu película ochentera favorita en una plataforma de streaming y encontrarte en una escena con un panel publicitario anunciando el iPhone 11. '),
(14, '\"Ya sé kung-fu\": La inteligencia artificial también puede aprender conectándose a una realidad simulada', '<div class=\"blob js-post-images-container\">\n<p><strong>El ser humano tiene, hoy en día, la opción de \"vivir en realidades sintéticas\"</strong>, en palabras de Justin Hendrix, director del NYC Media Lab: las redes sociales ya establecidas actúan como burbujas ideológicas que <a href=\"https://www.xataka.com/inteligencia-artificial/que-ia-recomendaciones-youtube-se-volvio-conspiranoica-condujo-a-conspiranoia-a-muchos-usuarios\">sesgan y radicalizan sus puntos de vista</a>, las nuevas redes sociales nos presentan el mundo a través de filtros y las que serán <a href=\"https://magnet.xataka.com/preguntas-no-tan-frecuentes/fortnite-nuevo-snapchat-que-su-dia-fue-nuevo-facebook\">las redes sociales del futuro</a> ya son, en sí mismas, mundos virtuales.</p>\n<!-- BREAK 1 -->\n<p>A lograr esto han contribuido, curiosamente, <strong>algoritmos de inteligencia artificial que son capaces de aprender del mundo real</strong>, extrayendo patrones de los objetos y seres que lo habitan. Se podría decir que la inteligencia artificial es un motor que funciona <a href=\"https://www.xataka.com/robotica-e-ia/machine-learning-y-deep-learning-como-entender-las-claves-del-presente-y-futuro-de-la-inteligencia-artificial\">alimentándola con cantidades ingentes de un combustible llamado \'datos\'</a>, datos que los humanos extraemos del mundo real.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Así, alimentando a una <a href=\"https://www.xataka.com/robotica-e-ia/las-redes-neuronales-que-son-y-por-que-estan-volviendo\">red neuronal</a> con miles o millones de fotografías de gatos -por ejemplo- podemos lograr que, en adelante, sea capaz de reconocer a otros gatos en otras fotografías que no conozca previamente, gracias a que <strong>detecta los patrones que todas ellas tienen en común</strong>.</p>\n<!-- BREAK 3 -->\n\n<p><strong>Los fabricantes de vehículos autónomos están invirtiendo grandes cantidades</strong> en lograr que éstos sean capaces de detectar peatones que pasean, que corren, que están parados en los semáforos o cruzando imprudentemente la carretera. No faltan, precisamente, fotos con miles de ejemplos de cada uno de esos casos (ya sea usando viandantes anónimos o actores).</p>\n<!-- BREAK 4 -->\n<p>Pero <a href=\"https://www.xataka.com/inteligencia-artificial/scale-ai-etiqueta-datos-que-grandes-silicon-valley-entrenan-a-sus-ias-vale-1000-millones-dolares\">recopilar y etiquetar</a> (a mano) todos esos ejemplos <strong>no es barato ni sencillo ni se puede hacer en poco tiempo</strong>. Y eso por no mencionar la multitud de situaciones que una IA debería poder identificar pero para las que apenas podemos proporcionarle ejemplos reales (grandes accidentes automovilísticos, por ejemplo).</p>\n<!-- BREAK 5 -->\n<p>También hay casos en los que el coche se cruza con algo que ya conoce, pero <strong>en una situación inédita, y no es capaz de reconocerlo</strong>... con consecuencias dramáticas (¿cómo hacer que reconozca como personas a un montón de niños hábilmente disfrazados para Halloween o Carnaval?).</p>\n<!-- BREAK 6 -->\n<h2>Aprendiendo con \'libros de texto\' simulados</h2>\n<div class=\"base-asset-media\"><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"200\" id=\"audio_32043162\" scrolling=\"no\" src=\"https://www.ivoox.com/player_ej_32043162_4_1.html?c1=ff6600\" style=\"border:1px solid #EEE; box-sizing:border-box; width:100%;\"></iframe></div>\n<p>Ya sea por coste o por imposibilidad para obtener datos reales, <strong>las empresas de inteligencia artificial han empezado a optar por \'engañar\' a sus IAs proporcionándoles fotos y vídeos falsos</strong>, a veces manipulados a la vieja usanza, en otros casos producto del uso de GANs o <a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-gans-redes-generativas-antagonicas\">redes generativas antagónicas</a> (deepfakes, en resumen). \"Si esta realidad no basta para entrenar a la inteligencia artificial\", piensan, \"la crearemos nosotros\".</p>\n<!-- BREAK 7 -->\n<p>De modo que muchas IAs ya viven, como muchos humanos en <strong>\"realidades sintéticas\", en las que supuestamente aprenden todo lo que luego tendrán que usar para relacionarse con el mundo real</strong>. Startups como Landing.ai , AI.Reverie o ANYVERSE ya generan millones de escenas que un coche real podría no experimentar, ni siquiera tras un millón de horas recorriendo calles y carreteras.</p>\n<!-- BREAK 8 -->\n<p>La generación de datos falsos-pero-realistas puede darse también con otro objetivo: el de <strong>anonimizar datos delicados</strong>, como los <a href=\"https://siliconangle.com/2019/10/10/diveplane-launches-twin-datasets-ai-training-anonymize-personally-identifiable-information/\">historiales médicos</a> que las IAs del sector sanitario necesita para afinar su capacidad de diagnóstico.</p>\n<!-- BREAK 9 -->\n<h2>Aprendiendo en \'aulas\' simuladas</h2>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/nilcJY5Kdt8\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Compañías como Waymo, por otro lado, utilizan estos mundos virtuales no para generar datos de entrenamiento para las IAs, sino para <strong>construir simulaciones que ponen a prueba su entrenamiento</strong>, haciendo que el software de un coche autónomo crea estar en un carretera real y obligándole a interactuar con falsos vehículos y peatones. Y, al no sufrir las limitaciones del mundo físico, <strong>pueden recorrer millones de kilómetros, una y otra vez, en pocas horas, aprendiendo de sus errores</strong>.</p>\n<!-- BREAK 10 -->\n<p><strong>Otras, como Nvidia, utilizan un sistema similar para entrenar a robots</strong> y que <a href=\"https://www.xataka.com/inteligencia-artificial/robots-no-necesitan-profesores-humanos-les-basta-realismo-simulaciones\">aprendan por sí mismos</a> la mejor forma de coger y transportar un objeto en base a unos determinados datos de peso y forma.</p>\n<!-- BREAK 11 -->\n<p>Vía | <a href=\"https://www.axios.com/newsletters/axios-future-3dec58db-32cc-4f97-a40d-435f41ed5347.html\">Axios</a></p>\n<!-- BREAK 12 -->\n<p>Imagen | \'Matrix\' (Warner Bros, 1999)</p>\n</div>', '2019-10-13 16:09:46', '2019-10-13 16:09:46', 4, 'portada13.jpg', 'A veces no tenemos suficientes datos extraídos de la realidad para entrenar a las IAs. A veces, sencillamente, es más caro y lento.'),
(15, 'Toyota presenta su prototipo de coche autónomo dotado de una IA para mejorar la atención de los pasajeros y comunicarse con ellos', '<div class=\"blob js-post-images-container\">\n<p>Toyota ha actualizado su prototipo de vehículo eléctrico y autónomo <a href=\"https://www.motorpasion.com/prototipos/inteligencia-artificial-a-lo-disney-asi-es-el-toyota-concept-i\">Concept-i 2017</a>, lo ha equipado con un <strong>diseño futurista y tecnología a juego</strong> gracias a su rama de I+D+i (el Toyota Research Institute) y lo ha bautizado como LQ.</p>\n<!-- BREAK 1 -->\n<p>Y ahora, un par de semanas <strong>antes de su presentación oficial en sociedad en el marco del Salón del Automóvil de Tokio</strong>, la marca ofrece las primeras imágenes y datos del modelo. 4,5 metros de largo, 1,8 de ancho y 1,5 de alto son las dimensiones de nuestro vehículo. Sabemos también que la autonomía de su batería le permite recorrer en torno a 300 km sin recargar.</p>\n<!-- BREAK 2 -->\n<!--more-->\n\n<h2>Yui, el asistente de voz del LQ (y otras innovaciones)</h2>\n<p>Pero lo que más llama la atención de este modelo es \'Yui\', <strong>el asistente de inteligencia artificial que integrará el LQ</strong> y que nos permitirá controlar múltiples aspectos del mismo a través de comandos de voz.</p>\n<!-- BREAK 3 -->\n<p>Toyota afirma que la clave del automóvil del futuro radicará en <strong>crear una verdadera \'conexión\' entre éste y sus pasajeros</strong>, y Yui parece responder a este planteamiento: no sólo ilumina zonas concretas del suelo y el techo para que cada pasajero sepa cuándo se está comunicando con él en concreto, sino que lo hace con distintos colores para indicar si está conectado el modo autónomo.</p>\n<!-- BREAK 4 -->\n<p>Además, Yu tiene la misión de <strong>optimizar el estado de ánimo de los ocupantes del coche</strong> gestionando de manera inteligente la iluminación, el aire acondicionado ¡y hasta el ambientador!</p>\n<!-- BREAK 5 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <img alt=\"Lq 2\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/3a8d2c/lq-2/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/3a8d2c/lq-2/450_1000.jpg 450w, https://i.blogs.es/3a8d2c/lq-2/650_1200.jpg 681w, https://i.blogs.es/3a8d2c/lq-2/1024_2000.jpg 1024w, https://i.blogs.es/3a8d2c/lq-2/1366_2000.jpg 1366w\"/><noscript><img alt=\"Lq 2\" src=\"https://i.blogs.es/3a8d2c/lq-2/450_1000.jpg\"/></noscript> </div></div>\n<p>El LQ se basa en <a href=\"https://www.xataka.com/automovil/de-0-a-5-cuales-son-los-diferentes-niveles-de-conduccion-autonoma\">un nivel 4 de conducción autónoma</a>. ¿<strong>Qué significa esto</strong>? Que el vehículo es capaz de reaccionar ante objetos y eventualidades, y que no requiere de un usuario preparado para intervenir si el sistema lo solicita o se produce un fallo. La automatización, sin embargo, sigue sin ser total (ése sería el nivel 5).</p>\n<!-- BREAK 6 -->\n<p>Por ello, el LQ ofrece <strong>un particular diseño de asiento de conductor</strong>, que permite rellenar de aire frío unos conductos situados en la superficie del asiento, con un doble fin: situarle en una posición más erguida para facilitarle la conducción y evitar, con la temperatura, que pueda caer en la somnolencia.</p>\n<!-- BREAK 7 -->\n\n<p>Además, Toyota ha unido fuerzas con Panasonic para dotar al LQ de <strong>un aparcacoches automatizado</strong>, que permite que el coche aparque sólo, después incluso de que se hayan bajado todos los pasajeros. También es fruto de la colaboración con Panasonic la <strong>pantalla de visualización de realidad aumentada</strong>, destinada a ayudar a los coches a mantener la vista fija en la carretera.</p>\n<!-- BREAK 8 -->\n<p>Otro de los detalles futuristas del LQ es su DMD (dispositivo digital de microespejos), una función basada en el uso de <strong>un millón de pequeños espejos que permiten proyectar gráficos frente al coche</strong> con el fin de comunicar información tanto a los ocupantes del vehículo como a la gente que lo rodea,como se ve a continuación:</p>\n<!-- BREAK 9 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <img alt=\"Lq 3\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/7ad00f/lq-3/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/7ad00f/lq-3/450_1000.jpg 450w, https://i.blogs.es/7ad00f/lq-3/650_1200.jpg 681w, https://i.blogs.es/7ad00f/lq-3/1024_2000.jpg 1024w, https://i.blogs.es/7ad00f/lq-3/1366_2000.jpg 1366w\"/><noscript><img alt=\"Lq 3\" src=\"https://i.blogs.es/7ad00f/lq-3/450_1000.jpg\"/></noscript> </div></div>\n<p>Si estás interesado en conocer de cerca este modelo, la compañía ofrecerá la opción de participar en el llamado <strong>\'Toyota Yui Project Tour\'</strong>, un evento que te permitirá, si eres seleccionado, probar esta tecnología entre junio y septiembre de 2020.</p>\n<!-- BREAK 10 -->\n<p>Vía | <a href=\"https://techcrunch.com/2019/10/11/toyotas-lq-concept-car-will-make-friends-with-you-via-its-onboard-ai/\">TechCrunch</a></p>\n<!-- BREAK 11 -->\n<p>Imagen | Toyota</p>\n</div>', '2019-10-12 06:17:06', '2019-10-12 06:17:06', 17, 'portada14.jpg', 'Yui: ese es el nombre del asistencia digital que Toyota ha creado para permitirnos controlar su coche mediante la voz.'),
(16, 'Adiós a \"Houston, tenemos un problema\": la NASA quiere que las misiones que vayan a Marte pidan ayuda a la inteligencia artificial', '<div class=\"blob js-post-images-container\">\n<p><strong>\"Houston, tenemos un problema\". Todos tenemos en mente esta frase pronunciada por Tom Hanks</strong> en la famosa escena de la película \"Apolo XIII\". Poco importa que la frase real del astronauta Jim Lovell <a href=\"https://es.wikipedia.org/wiki/Houston,_tenemos_un_problema\">no fuera exactamente así</a>: de cualquier modo, la frase ha pasado a la cultura popular como símbolo de la aparición de problemas imprevistos. Lo curioso es que la NASA no quiere volver a oírla una vez logre mandar misiones más allá de la Luna.</p>\n<!-- BREAK 1 -->\n<p>Eso es lo que revela un documento de la NASA hecho público esta semana, en el que se plantea la necesidad de que la nueva generación de astronautas <strong>dependa menos del control de la misión en tierra y más de los sistemas de inteligencia artificial</strong> con los que irá equipadas sus naves.</p>\n<!-- BREAK 2 -->\n<!--more-->\n\n<p>En el mismo, Jeremy Frank, de la División de Sistemas Inteligentes del Centro de Investigación Ames de la NASA en Mountain View (California), afirma que</p>\n<!-- BREAK 3 -->\n<blockquote>\n<p>\"las futuras misiones a Marte obligarán a que la tripulación opere al margen del control de misión: el objetivo es recurrir a software de IA para ayudar a los astronautas a llevar a cabo sus misiones de manera más autónoma\".</p>\n</blockquote>\n<p>Esperamos enviar a los primeros humanos hacia Marte dentro de aproximadamente 15 años. Nunca jamás un Homo Sapiens se habrá alejado tanto de la superficie de nuestro planeta natal.</p>\n<!-- BREAK 4 -->\n<p>Pero, al contrario que los héroes de los programas Sputnik y Apolo, los futuros astronautas no podrán confiar en la rápida respuesta de sus controles de misión en tierra: Marte está demasiado lejos para ello. <strong>Si algo saliera mal, tardarían casi tres cuartos de hora en recibir una respuesta</strong> de la Tierra.</p>\n<!-- BREAK 5 -->\n<p>\"Lo de \'Houston, tenemos un problema\' dejará de ser buena opción, porque la respuesta será demasiado lenta\", explicaba hace unos meses Ellen Stofan , ex científica jefe de la NASA. \"<strong>Sigo diciendo que lo que necesitamos es un HAL bueno</strong>\".</p>\n<!-- BREAK 6 -->\n\n<p>Quizá éste sea un buen momento para recordar que HAL (el ordenador inteligente de \'2001, una odisea de espacio\") <strong>era \"bueno\" hasta que le \'cortocircuitamos\' los criterios éticos</strong> generando en conflicto entre las diferentes instrucciones recibidas para cumplir con su misión.</p>\n<!-- BREAK 7 -->\n<p>\"El inmenso papel de la IA será permitir que los humanos se mantengan fuera de las trincheras\", dice Steve Chien , líder del grupo de inteligencia artificial en el Laboratorio de Propulsión a Chorro de la NASA. \"<strong>Será una forma mucho más efectiva de hacer ciencia</strong>: no queremos que el astronauta pase todo su tiempo asegurándose de que el sistema de soporte vital funciona\".</p>\n<!-- BREAK 8 -->\n<div class=\"article-asset-summary article-asset-normal\"><div class=\"asset-content\"><div class=\"sumario\">\"Se necesitan sistemas informáticos extremadamente sofisticados. Ya pasaron los días en que podíamos ir a la Luna con el equivalente a la potencia informática de mi iPhone\".</div></div></div>\n<h2>El paso previo: antes de Marte y HAL están la Luna y la Puerta de Enlace</h2>\n<p>La NASA se ha propuesto llevar a nuevos astronautas a la Luna, por primera vez en décadas, en 2024. El objetivo no es la Luna en sí, sino poder contar con un \'trampolín\' para el viaje hacia el planeta rojo. <strong>Ese trampolín será la Puerta de Enlace Lunar</strong>, una nave (más bien una microestación) espacial que orbitará el satélite y podrá albergar hasta cuatro astronautas.</p>\n<!-- BREAK 9 -->\n<p>\"Más tarde usaremos lo que aprendamos orbitando la Luna para dar el siguiente gran salto: <strong>enviar astronautas a Marte</strong>\", explica Frank en el documento.</p>\n<!-- BREAK 10 -->\n<p>Esas pequeñas tripulaciones no podrán asumir por sí mismas la gestión de todas las funciones de la Puerta de Enlace, \"por lo que <strong>los vehículos deberán estar más automatizados para reducir la carga de trabajo</strong> de los astronautas\".</p>\n<!-- BREAK 11 -->\n\n<p>Tanto, que la nave seguirá funcionando por sí sola durante varias semanas, como un sistema autogestionado, en los períodos en que no esté habitada por ninguna misión... <strong>gracias al uso de robots</strong>.</p>\n<!-- BREAK 12 -->\n<p>Además, la inteligencia artificial <strong>permitirá monitorizar, planificar y ejecutar programas a bordo</strong> y, eventualmente, detectar y corregir errores en los mismos. Gracias a eso, serán los algoritmos los responsables de procurar en todo momento que haya agua potable disponible, o de ofrecer información actualizada sobre el estado de la nave.</p>\n<!-- BREAK 13 -->\n<blockquote>\n<p>\"Debido a la naturaleza crítica de las misiones espaciales tripuladas, la tecnología autónoma debe ser robusta y resistente, preparada tanto para los cambios imprevistos del entorno, como también para hacer frente a la impredecible naturaleza de la colaboración hombre-máquina. [...] El razonamiento automatizado y el <a href=\"https://www.xataka.com/robotica-e-ia/machine-learning-y-deep-learning-como-entender-las-claves-del-presente-y-futuro-de-la-inteligencia-artificial\">machine learning</a> serán tecnologías clave para permitir gestionar los fallos\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://www.theregister.co.uk/2019/10/10/nasa_ai_moon/\">The Register</a> &amp; <a href=\"https://www.smithsonianmag.com/science-nature/when-we-go-mars-will-we-want-2001-space-odysseys-hal-9000-us-180963694/\">The Smithsonian</a></p>\n<!-- BREAK 14 -->\n<p>Imagen | <a href=\"https://commons.wikimedia.org/wiki/File:Mars_orbit_rendez_vous_S95_01407.jpg\">Pat Rawlings</a> (Science Applications International Corporation for NASA)</p>\n<!-- BREAK 15 --> </div>', '2019-10-12 03:43:44', '2019-10-12 03:43:44', 47, 'portada15.jpg', '¿Pero es que la NASA no aprendió nada de lo de HAL en \'2001\'?'),
(17, 'Esta inteligencia artificial creada por el MIT puede verte (y saber qué estás haciendo) desde el otro lado de la pared', '<div class=\"blob js-post-images-container\">\n<p><strong>Hace tiempo que los sistemas de inteligencia artificial pueden usar la visión artificial</strong> para detectar la presencia de humanos en una habitación, e incluso identificar qué están haciendo o que sentimiento expresan, con solo analizar su postura y patrones de movimiento.</p>\n<!-- BREAK 1 -->\n<p>Pero ¿y si la habitación está en total oscuridad? O peor <strong>¿y si la persona no está en la misma habitación, sino en la contigua?</strong> Pues ahora eso no supone ningún impedimento para la IA: tan sólo ha de buscar una fuente de datos diferente y seguir analizando patrones. <strong>Y esa nueva fuente de datos son las señales de radio en las frecuencias WiFi</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Y eso se lo debemos a un equipo de investigadores del Laboratorio de Ciencias de la Computación e Inteligencia Artificial (CSAIL) del MIT que recientemente <a href=\"https://arxiv.org/abs/1909.09300\">ha publicado un \'paper\'</a> que muestra el modo en que dichas señales pueden ser analizadas por <a href=\"https://www.xataka.com/robotica-e-ia/las-redes-neuronales-que-son-y-por-que-estan-volviendo\">redes neuronales</a> para <strong>permitirnos \'ver\' en ausencia de luz</strong>.</p>\n<!-- BREAK 3 -->\n\n<blockquote>\n<p>\"Nuestro modelo utiliza como datos de entrada la señales de radiofrecuencia, genera esqueletos humanos en 3D a partir de las mismas y es capaz de reconocer las acciones e interacciones de varias personas a lo largo del tiempo\".</p>\n</blockquote>\n<p>Ese paso intermedio de convertir las señales de radio en esqueletos 3D (como los mostrados en la imagen que encabeza el artículo) <strong>resulta clave para el éxito de este sistema</strong>, al permitir que el modelo no sólo aprenda a partir de los datasets basados ​​en radiofrecuencia, sino que también sea capaz de aplicar todos los datos recopilados hasta la fecha basados en la captura con visión computerizada.</p>\n<!-- BREAK 4 -->\n<p>De hecho, los investigadores <strong>empezaron grabando simultáneamente imágenes con luz visible y basadas en ondas de radio</strong>, con el objetivo de sincronizarlas y que la red neuronal se entrenara también simultáneamente con ambas: así podría establecer correlaciones entre los datos mostrados por una y otra. La introducción de los esqueletos 3D tuvo lugar cuando detectaron que al sistema <strong>le costaba aprender a diferenciar a una persona de su entorno circundante</strong>.</p>\n<!-- BREAK 5 -->\n<h2>¿Facilitando la vigilancia?</h2>\n<p>Según explica Tianhong Li, uno de los coautores del \'paper\', este modelo, bautizado como \'RF-Action\', tiene <strong>\"numerosas aplicaciones potenciales\" en el campo de los hogares inteligentes</strong> y de la comprensión del comportamiento humano:</p>\n<!-- BREAK 6 -->\n<blockquote>\n<p>\"Por ejemplo, detectar comportamientos anormales en los ancianos (para detectar si se caen dentro de su casa), monitorizar que los pacientes tomen sus medicamentos de manera adecuada o controlar a distancia dispositivos domésticos inteligentes mediante gestos\".</p>\n</blockquote>\n\n<p>En cuanto a la privacidad, todavía no hay que preocuparse por la posibilidad de que alguien nos someta a reconocimiento facial desde la casa de al lado, pues la baja resolución de las imágenes conseguidas no permite apreciar los rasgos faciales. Sin embargo, sí <strong>deberíamos temer una posible integración futura con la tecnología de reconocimiento postural</strong> desarrollada por los chinos. </p>\n<!-- BREAK 7 -->\n<p>Pero Li reconoce que aún les queda mucho trabajo por delante: hasta ahora, RF-Action sólo se ha probado usando una única pared y dentro de un rango de detección de 1-11 metros:</p>\n<!-- BREAK 8 -->\n<blockquote>\n<p>\"La señal de radiofrecuencia se debilitará progresivamente según añadamos paredes, por lo que <strong>puede ser difícil conservar un equilibrio adecuado señal / ruido si añadimos demasiadas paredes</strong>\".</p>\n</blockquote>\n<p>Vía| <a href=\"https://medium.com/syncedreview/watch-out-mits-new-ai-model-knows-what-you-re-doing-behind-that-wall-a8473420f2d0\">SyncedReview</a></p>\n<!-- BREAK 9 -->\n<p>Imagen | MIT-CSAIL</p>\n</div>', '2019-10-11 00:31:40', '2019-10-11 00:31:40', 35, 'portada16.jpg', 'Se acerca un distópico futuro en el que el altavoz inteligente de nuestro salón podrá saber qué estamos haciendo en el baño o en nuestra habitación sin necesidad de cámaras.'),
(18, 'El presidente de Microsoft pide contar con \'botones de apagado de emergencia\' para la inteligencia artificial', '<div class=\"blob js-post-images-container\">\n<p>Skynet, la inteligencia artificial rebelde que casi aniquila a la raza humana en la saga cinematográfica \'Terminator\' <strong>puede ser (y es) un tópico mil veces repetido cuando abordamos los peligros potenciales de la IA</strong>, una apocalíptica posibilidad meramente teórica, si se quiere... pero muchos expertos en IA siguen pensando en ella cuando analizan la mejor manera de evitar que esta tecnología se nos pueda ir un día de las manos.</p>\n<!-- BREAK 1 -->\n<p>La Cumbre GeekWire 2019, clausurada este martes en Seattle, ha reunido a más de 800 líderes empresariales y tecnológicos para explorar las tendencias futuras de la nueva economía, y algunos de ellos han vuelto a pensar en Skynet <strong>gracias a las palabras del mismísimo presidente de Microsoft, Brad Smith</strong>. Lo curioso es que todo ha empezado con un comentario sobre economía regional y e industria aeronáutica:</p>\n<!-- BREAK 2 -->\n<!--more-->\n<blockquote>\n<p>\"¿Cuál es el mayor problema relacionado con el software que afecta hoy en día sobre la economía del Estrecho de Puget (la región en la que se encuentra Seattle)? Un software para cabinas de aviones, un software que los pilotos no podían apagar\".</p>\n</blockquote>\n\n<h2>El ejemplo de Boeing debe servir \"para toda la industria\"</h2>\n<p>Smith se refería aquí a la crisis sufrida hace unos meses por la compañía Boeing, tras saberse que <strong>el software con el que habían equipado secretamente a los aviones 737 Max</strong> había sido <a href=\"https://www.xataka.com/vehiculos/boeing-sus-737-max-caos-software-mcas-se-situa-como-claro-culpable-hubo-otros-ingredientes-estas-tragedias\">el motivo de dos grandes accidentes de aviación</a> en los que murieron 346 personas.</p>\n<!-- BREAK 3 -->\n<p>Se trataba de un sistema que bajaba el morro del avión cuando detectaba una inclinación muy elevada. Al recibir información incorrecta por parte de las sondas, el morro bajó más de la cuenta por efecto del software y <strong>la tripulación no fue capaz de desactivar este sistema automatizado</strong>, lo que provocó la caída de los aviones.</p>\n<!-- BREAK 4 -->\n<p>La compañía, que tiene su sede en las inmediaciones de Seattle, <strong>ha tenido que pagar ya más de 8.000 millones de dólares en concepto de indemnización</strong>... y su efecto a medio/largo plazo tendrá, por lo que parece, un impacto mucho mayor.</p>\n<!-- BREAK 5 -->\n<p>Pero, para Smith, las lecciones que tenemos que sacar de esto \"no deben estar dirigidas a una única empresa o a una única industria, sino a todos los que crean y usan tecnología. Es una lección que todos los sectores de la sociedad deberán recordar: <strong>debemos ser capaces de crear buena tecnología... y de poder desactivarla</strong>\".</p>\n<!-- BREAK 6 -->\n<p>A continuación, hablando sobre el libro que lanzó el mes pasado (\'Tools and Weapons: The Promise and the Peril of the Digital Age\'), Smith hizo referencia explícita a <strong>la necesidad de que tanto la industria tecnológica como el gobierno se unan para abordar los peligros de la IA</strong> y el machine learning.</p>\n<!-- BREAK 7 -->\n\n<blockquote>\n<p>\"Somos la primera generación que decidirá el modo en que las máquinas habrán de tomar estas decisiones y qué tipo de principios éticos las guiarán su toma de decisiones. No es por presionar, pero sería mejor que lo hagamos bien\".</p>\n</blockquote>\n<p>Cabe señalar que <strong>la Unión Europea ya puso encima de la mesa el debate sobre la necesidad de este sistema de apagado de emergencia en 2017</strong> y que, a día de hoy, la existencia de este mecanismo es una de las \"<a href=\"https://www.xataka.com/robotica-e-ia/ue-presenta-sus-directrices-para-desarrollo-etico-inteligencia-artificial\">Pautas de ética para la IA confiable</a>\", un documento oficial aprobado este mismo año. Según el mismo, su fin debe ser cesar inmediatamente las operaciones de la IA en ejecución, así como <strong>delegar el control de las nuevas en manos de operadores humanos</strong>.</p>\n<!-- BREAK 8 -->\n<p>Pero ni aún desarrollando esta clase de sistemas nuestra especie estará segura frente a una IA incontrolada: en 2017, dos investigadores (Laurent Orseau de Deep Mind, y Stuart Amstrong, de la Univ. de Oxford) <a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/Interruptibility.pdf\">estudiaron</a> la solución de los botones de apagado, destinados a \"evitar que la máquina siga una secuencia dañina de acciones para el ser humano o para el medio ambiente\".</p>\n<!-- BREAK 9 -->\n<p>Pero no estaban seguros de que una IA no pudiera terminar hallando la manera de protegerse de dicho apagado forzoso. Ya en su artículo mencionan <strong>el caso de un bot que aprendió a detener el juego Tetris para evitar perder</strong>. Por desgracia, esto podría extrapolarse a otros ámbitos potencialmente más dañinos para los seres humanos.</p>\n<!-- BREAK 10 -->\n<p>Vía | <a href=\"https://www.geekwire.com/2019/microsofts-brad-smith-cites-boeing-crisis-cautionary-tale-intelligent-machines-calls-ai-kill-switch/\">GeekWire</a></p>\n<!-- BREAK 11 -->\n<p>Imagen | <a href=\"https://www.flickr.com/photos/dumbledad/3225255407\">Tim Regan</a></p>\n</div>', '2019-10-10 03:15:29', '2019-10-10 03:15:29', 32, 'portada17.jpg', 'Brad Smith ha puesto como ejemplo la última crisis de la compañía Boeing, tras haber instalado software que la tripulación no podía anular.');
INSERT INTO `posts` (`id`, `titulo`, `cuerpo`, `created_at`, `updated_at`, `user_id`, `imagen`, `descripcion`) VALUES
(19, 'El 96% de los deepfakes online son porno pero preocupan más los de carácter electoral: ahora, California ha legislado contra ambos', '<div class=\"blob js-post-images-container\">\n<p>El gobernador de California firmó la semana pasada dos leyes, bautizadas como AB 730 y AB 602, y relacionadas con la <strong>creciente moda de los deepfakes</strong> (las imágenes, vídeos y audios falsificados mediante el uso de inteligencia artificial) .</p>\n<!-- BREAK 1 -->\n<p>La primera de esas normativas <strong>prohíbe difundir vídeos manipulados para desacreditar a candidatos</strong> políticos durante los 60 días previos a unas elecciones, mientras que la segunda otorga a los californianos el derecho a <strong>demandar a todo aquel que difunda sin consentimiento deepfakes pornográficos</strong> basados en su imagen.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Sin embargo, la gran pregunta ahora es ¿<strong>cómo van a poder aplicar de forma efectiva ambas leyes</strong>, cuando las grandes plataformas de Internet se están viendo superadas en esa tarea? Zuckerberg, CEO de Facebook, admitió el pasado mes de junio que los sistemas de su compañía eran demasiado lentos para detectar y eliminar los deepfakes de vídeo.</p>\n<!-- BREAK 3 -->\n\n<p>Unos meses antes, la plataforma para adultos Pornhub prohibía subir deepfakes a su plataforma a comienzos de este año y, efectivamente, no sale ningún resultado cuando se busca por la palabra \"deepfake\"... pero no hace falta una gran inventiva para <strong>acceder, pese a ello, a numerosas falsificaciones porno</strong> en dicho sitio web.</p>\n<!-- BREAK 4 -->\n<p>\"El hecho es que tratar de protegerse de Internet y su depravación <strong>es básicamente una causa perdida</strong>\", <a href=\"https://www.xataka.com/otros-dispositivos/scarlett-johansson-se-rinde-deep-fakes-pornograficos-basicamente-causa-perdida\">según afirmaba Scarlett Johansson a comienzos de este año</a>, en referencia a la proliferación de los deepfakes de carácter pornográfico con los rostros de famosas. La estrella de \'Lost in Traslation\' o \'Los Vengadores\' explicaba así su postura:</p>\n<!-- BREAK 5 -->\n<blockquote>\n<p>\"Nada puede impedir que alguien corte y pegue mi imagen o la de otra persona en otro cuerpo y haga que se vea tan realista como desee\".</p>\n<p>\"Creo que es algo inútil, legalmente, sobre todo porque Internet es un enorme agujero de gusano de oscuridad que se devora a sí mismo\".</p>\n</blockquote>\n<h2>¿Debemos preocuparnos por los deepfakes políticos?</h2>\n<p>Un <a href=\"https://deeptracelabs.com/resources/\">reciente informe</a> elaborado por Deeptrace muestra que en verano la cantidad de deepfakes de vídeo disponibles online se había duplicado con respecto a diciembre de 2018 (14.698 contra 7.964), <strong>pero ese aumento se debió fundamentalmente a la pornografía, no a vídeos de carácter político</strong>. De hecho, el porcentaje de deepfakes pornográficos asciendo hasta el 96%, en su mayor parte \"protagonizados\" por famosas del ámbito de la música y la actuación.</p>\n<!-- BREAK 6 -->\n<div class=\"base-asset-media\"><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"200\" id=\"audio_32043162\" scrolling=\"no\" src=\"https://www.ivoox.com/player_ej_32043162_4_1.html?c1=ff6600\" style=\"border:1px solid #EEE; box-sizing:border-box; width:100%;\"></iframe></div>\n<p>Sin embargo, <strong>la gran preocupación de los legisladores estadounidenses parece girar en torno a las próximas elecciones</strong>, y a los fantasmas de una manipulación electoral que pueda influir de forma decisiva en los resultados de las mismas.</p>\n<!-- BREAK 7 -->\n<p><strong>Y aún está fresco el recuerdo del vídeo viral manipulado de Nancy Pelosi</strong>, la presidenta de la Cámara de Representantes, en el que <a href=\"https://www.xataka.com/inteligencia-artificial/polemica-videos-nancy-pelosi-manipulados-para-hacerle-parecer-borracha-vaticinan-que-nos-espera-deepfakes\">una sencilla modificación del mismo</a> permitió dar entender (falsamente) que esta veterana referente del Partido Demócrata estaba había dado una rueda de prensa \'perjudicada\' por alguna sustancia.</p>\n<!-- BREAK 8 -->\n<p>Sin embargo, <strong>el vídeo de Pelosi ni siquiera era un \'deepfake\'</strong>, pues no hubo ninguna inteligencia artificial implicada en el proceso de manipulación; era una manipulación digital tradicional, lo que ahora empiezan a denominar \"cheap fakes\" (\'falsificaciones baratas\').</p>\n<!-- BREAK 9 -->\n<p>Aun así, Marc Berman, el legislador estatal que presentó la propuesta AB 730, explica que actuó motivado por su convicción de que los deepfakes constituyen \"una nueva y poderosa tecnología que puede ser <strong>enarbolada para sembrar desinformación entre el electorado</strong>, haciendo extremadamente difícil distinguir los eventos y acciones reales de la ficción y la fantasía\".</p>\n<!-- BREAK 10 -->\n\n<p>Henry Ajder, de Deeptrace, señala el que podría ser el modo más sencillo de que los deepfakes influyan en política: <strong>\"proporcionan una negación pausible\"</strong> a aquellos políticos que son sorprendidos y grabados realizando algo ilegal o vergonzoso. Así, en lugar de imputar hechos falsos, permitirían exculpar acusaciones verdaderas.</p>\n<!-- BREAK 11 -->\n<h2>¿De qué sirven estas leyes?</h2>\n<p>Jane Kirtley, especialista en ética de los medios de comunicación en la Escuela Hubbard, advierte que existen impedimentos legales: debido a las garantías estadounidenses a la libertad de expresión, <strong>podría resultar más sencillo impedir la difusión de vídeos manipulados a través de denuncias por cuestiones de copyright</strong> que a través de las nuevas leyes.</p>\n<!-- BREAK 12 -->\n<blockquote>\n<p>\"El deseo de proteger a la gente del contenido engañoso en el período previo a una elección es muy fuerte y muy comprensible, pero soy escéptica sobre si van a ser capaces de hacer cumplir esta ley\".</p>\n</blockquote>\n<p>Paul Bischoff, de Comparitech.com, advierte que prohibir la producción de deepfakes, aunque sea de forma limitada en el tiempo, es una opción poco práctica, y afirma que estamos ante \"una tecnología que no va a desaparecer, por lo que debemos prepararnos para un mundo donde las falsificaciones sean comunes\". <strong>¿La mejor solución para él? Concienciar y fomentar un saludable escepticismo entre el público</strong>.</p>\n<!-- BREAK 13 -->\n<p>Vía | <a href=\"https://www.theguardian.com/us-news/2019/oct/07/california-makes-deepfake-videos-illegal-but-law-may-be-hard-to-enforce\">The Guardian</a> &amp; <a href=\"https://www.wired.com/story/most-deepfakes-porn-multiplying-fast/\">Wired</a> &amp; <a href=\"https://www.forbes.com/sites/daveywinder/2019/10/08/forget-2020-election-fake-news-deepfake-videos-are-all-about-the-porn/\">Forbes</a></p>\n<!-- BREAK 14 -->\n<p>Imagen | Pixabay</p>\n</div>', '2019-10-09 11:01:17', '2019-10-09 11:01:17', 22, 'portada18.jpg', 'Ningún verdadero deepfake (por contradictorio que resulte) ha permitido hasta ahora llevar a cabo una manipulación viral en el ámbito político.'),
(20, 'No desconfiamos de las decisiones de los coches autónomos porque sean incorrectas, sólo porque las percibimos como poco \'humanas\'', '<div class=\"blob js-post-images-container\">\n<p>Los coches autónomos llevan han acumulado millones de viajes en los últimos años, la mayoría de ellos transcurridos <a href=\"https://www.xataka.com/automovil/asi-justifican-escepticos-coche-autonomo-que-este-no-vaya-a-terminar-completo-accidentes-trafico\">sin incidentes</a>. Pero los estudios demuestran que los usuarios siguen siendo reacios a comprarlos, y <strong>en 2017 la mayoría de los estadounidenses (el 56%) se seguían negando a viajar en uno</strong>.</p>\n<!-- BREAK 1 -->\n<p>El principal motivo de esto es que son <strong>reacios a ceder el control de una potencial situación de vida o muerte a una máquina</strong>, aunque no tengan problema en ceder dicho control a otro humano (como muestra el uso de taxis, de servicios como Uber o Blablacar, etc).</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>April D.Young y Andrew E.Monroe, investigadores de la Appalachian State University, <a href=\"https://www.sciencedirect.com/science/article/pii/S0022103118305201\">afirman</a> que <strong>el motivo de dicho doble rasero</strong> estriba en el modo en que percibimos que humanos y máquinas pueden <a href=\"https://www.xataka.com/automovil/el-mayor-estudio-sobre-la-etica-de-los-coches-autonomos-trae-malas-noticias-para-los-ancianos-e-incertidumbre-para-la-industria\">valorar disyuntivas morales complejas</a>.</p>\n<!-- BREAK 3 -->\n<p>Así, mientras que percibimos a otros humanos como poseedores de mentes altamente complejas capaces de planificar y experimentar emociones profundas, lo que nos viene a la cabeza cuando se habla de máquinas son mentes rudimentarias capaces de tomar decisiones, sí... pero <strong>de forma consecuencialista y \"desprovista de valores y sentimientos\"</strong>.</p>\n<!-- BREAK 4 -->\n\n<h2>Alexa y Siri marcan el camino a los coches autónomos</h2>\n<p>Con el objetivo de analizar cómo juzgábamos los seres humanos el modo de afrontar dilemas morales dependiendo de si quien los afrontaba era una máquina o un congénere, llevaron a cabo dos estudios, y la comparación de los resultados entre sí resultó reveladora.</p>\n<!-- BREAK 5 -->\n<p>Mientras que el primer estudio demostró que, <strong>ante decisiones idénticas, los vehículos autónomos eran juzgados como \"más inmorales y menos fiables\"</strong> que los humanos, el segundo mostraba a los sujetos los mismos datos sobre la toma de decisiones, pero expresados en términos mentalistas y no mecanicistas: al fomentar que se percibiera a los vehículos como algo dotado en cierta forma de una mente, <strong>el rechazo a sus decisiones se redujo sensiblemente</strong>.</p>\n<!-- BREAK 6 -->\n<p>Los investigadores descubrieron que cuando se nos plantea una misma reacción ante la misma disyuntiva (<strong>decidir atropellar a una persona para salvar a otras cinco</strong>), a los humanos nos causa mucho más rechazo imaginar que dicha decisión ha sido tomada por una IA, porque <strong>no la encontramos legitimada por valores altruistas</strong>:</p>\n<!-- BREAK 7 -->\n<blockquote>\n<p>\"No basta con que los vehículos autónomos sean sencillamente capaces de tomar decisiones \"correctas\", la gente también demanda que la tecnología emergente tenga las motivaciones correctas para sus elecciones. Estos hallazgos sugieren que las reservas morales de la gente sobre los vehículos autónomos pueden derivar de dudar que cuenten las capacidades mentales necesarias para realizar un juicio moral\".</p>\n</blockquote>\n\n<p>Young y Monroe afirman que estos resultados ofrecen una posible vía para que confiemos en los vehículos autónomos a la hora de tomar decisiones moralmente conflictivas: <strong>diseñarlos de tal modo que parezca que tuvieran rasgos mentales similares a los de los humanos</strong>.</p>\n<!-- BREAK 8 -->\n<blockquote>\n<p>\"Los diseñadores de asistentes personales digitales (como Siri, Cortana o Alexa) han dedicado considerables recursos a enseñar a estos asistentes a cantar, contar chistes o a ser frívolamente sarcásticos. Ninguna de estas características mejora la utilidad de los dispositivos; sin embargo, nuestros datos sugieren que estas imitaciones de lo humano pueden hacer que la gente confíe e interactúe con los dispositivos con mayor frecuencia\".</p>\n<p>\"Las personas son más propensas a antropomorfizar y prefieren un robot con una voz similar a la humana en lugar de interactuar con un robot con una voz artificial. Nuestros datos sobre los vehículos autónomos sugieren que, incluso los cambios más sutiles pueden empujar a las personas a percibir cualidades similares a las humanas en los agentes artificiales y, por lo tanto, a lograr que los consumidores indecisos sean más propensos a usar vehículos autónomos\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://www.sciencedirect.com/science/article/pii/S0022103118305201\">ScienceDirect</a></p>\n<!-- BREAK 9 -->\n<p>Imagen | Imagen promocional de \"El coche fantástico\", de la NBC</p>\n</div>', '2019-10-08 14:01:00', '2019-10-08 14:01:00', 49, 'portada19.jpg', 'Los humanos mostramos un doble rasero al valorar la misma decisión de un vehículo, dependiendo de si la atribuimos a una mente o a un algoritmo.'),
(21, 'Cariño, he encogido a la IA: que ocupe poco puede hacerla más rápida, más ecológica y más segura para la privacidad del usuario', '<div class=\"blob js-post-images-container\">\n<p>El procesamiento de lenguaje natural es el campo de la computación que, en los últimos tiempos, está viendo surgir <strong>los mayores modelos de inteligencia artificial jamás creados</strong> (es decir, aquellos con un mayor número de parámetros y cuyo entrenamiento, consecuentemente, requiere de más tiempo y capacidad de cómputo) .</p>\n<!-- BREAK 1 -->\n<p>Hace ahora un año, Google lanzó BERT, una IA especializada en la comprensión de frases complejas, cuyo modelo más complejo constaba de 340 millones de parámetros: <strong>el gasto eléctrico necesario para entrenarlo equivalía al de un hogar estadounidense promedio durante 50 días</strong>. En ese momento, BERT era la \'ballena azul\' de los modelos de IA...</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>...pero su estancia en dicho puesto fue breve. Tan sólo cuatro meses más tarde, OpenAI publicaba su <a href=\"https://www.xataka.com/inteligencia-artificial/gpt-2-que-sabemos-que-no-generador-textos-ia-que-openai-dice-haber-censurado-ser-demasiado-peligroso\">\'peligroso\' generador de textos fake creíbles conocido como GPT-2,</a> que ya contaba con <strong>1.500 millones de parámetros</strong>. Y la cosa se nos ha terminado de ir de las manos con el lanzamiento de MegatronLM, el más reciente modelo de IA creado por Nvidia, que cuenta con <strong>8.300 millones de parámetros</strong>.</p>\n<!-- BREAK 3 -->\n\n<p>Hace unos meses, un estudio de la Universidad de Massachusets analizó la huella de carbono de dichos modelos: calcularon que entrenar BERT había supuesto <strong>un gasto equivalente al de un vuelo de ida y vuelta entre Nueva York y San Francisco</strong>. Mejor no pensemos en la \'huella\' que tendría MegatronLM. Según exponían los investigadores en el paper:</p>\n<!-- BREAK 4 -->\n<blockquote>\n<p>\"Adiestrar un modelo de última generación requiere de recursos informáticos sustanciales que demandan un considerable consumo energético, con el coste económico y medioambiental que eso conlleva\".</p>\n<p>\"Investigar y desarrollar nuevos modelos multiplica estos costes cientos de veces al requerir un reentrenamiento para experimentar con nuevas arquitecturas e hiperparámetros\".</p>\n</blockquote>\n<p>De continuar profundizando en esta tendencia, la principal consecuencia de ello sería -dejando a un lado la huella de carbono- <strong>impulsar la concentración de la capacidad investigadora sobre inteligencia artificial en manos de un pequeño grupo de grandes compañías</strong> con los recursos suficientes como para desarrollar modelos tan costosos.</p>\n<!-- BREAK 5 -->\n\n<h2>Como decía Schumacher, \"lo pequeño es hermoso\"</h2>\n<p>Por ello, muchos investigadores se han embarcado en la tarea de buscar métodos que permitan reducir el tamaño de éstos sin ver mermadas también sus capacidades.</p>\n<!-- BREAK 6 -->\n<p>Un reciente logro de dicha tarea ha sido <a href=\"https://arxiv.org/abs/1909.10351?context=cs.AI\">\'TinyBERT\'</a>, una versión de BERT desarrollada por investigadores de Huawei que alcanza prácticamente <strong>el mismo nivel de comprensión del lenguaje que su \'hermano mayor\' pese a ser 7,5 veces más pequeño</strong>; también es casi 10 veces más rápido que el original.</p>\n<!-- BREAK 7 -->\n<p>Un día después de publicarse el paper de TinyBERT, un equipo de investigadores de Google publicaban otro explicando cómo han logrado <strong>otra versión de BERT 60 veces más pequeña</strong>, aunque su comprensión del lenguaje sea algo peor que la del modelo \'maestro\'.</p>\n<!-- BREAK 8 -->\n<p>En ambos \'papers\', los investigadores recurren a distintas variaciones de una técnica de compresión común conocida como \"destilación de conocimiento\", basada en <strong>recurrir al modelo grande que deseamos reducir (el \"maestro\") para entrenar un modelo más pequeño (el \"alumno\")</strong> a su imagen: se introducen los mismos datos de entrada en ambos y luego se ajusta la configuración del \'alumno\' hasta que sus resultados coinciden con lo del maestro.</p>\n<!-- BREAK 9 -->\n\n<p>Semanas antes de la publicación de ambos artículos, Google AI ya había marcado el camino con <a href=\"https://medium.com/syncedreview/googles-albert-is-a-leaner-bert-achieves-sota-on-3-nlp-benchmarks-f64466dd583\">el lanzamiento de ALBERT</a> (siglas de \'A Lite BERT\' o \'Un BERT ligero\'), desarrollado mediante técnicas similares una vez los investigadores se dieron cuenta de que, a partir de cierto límite, aumentar el número de capas ocultas en el modelo BERT original <strong>no sólo no mejoraba su rendimiento sino que lo empobrecía</strong>.</p>\n<!-- BREAK 10 -->\n<p>Pero, ¿qué logramos con esto, más allá de reducir el consumo eléctrico a la hora de entrenar los modelos de IA? ¿Cómo repercute esto sobre el usuario? Muy sencillo: un modelo de IA de pequeño tamaño es <strong>mucho más versátil que uno grande a la hora de ejecutarse en una amplia gama de dispositivos</strong>, y <a href=\"https://iahuawei.xataka.com/edge-computing-inteligencia-artificial-se-baja-nube/\">deja de depender en exclusiva de su ejecución en la nube</a>.</p>\n<!-- BREAK 11 -->\n<p>Así, podrían estar integrados en <a href=\"https://iahuawei.xataka.com/inteligencia-artificial-se-apodera-poco-moviles-resultado-espectacular/\">dispositivos móviles</a> y gadgets, <a href=\"https://www.xataka.com/internet-of-things/edge-computing-que-es-y-por-que-hay-gente-que-piensa-que-es-el-futuro\">lo que conocemos como \'edge computing\'</a>, <strong>ganando rapidez de respuesta</strong> (al no tener que estar conectándose a un servidor remoto) <strong>y protección de la privacidad</strong> (los datos del usuario nunca saldrán de su dispositivo).</p>\n<!-- BREAK 12 -->\n<p>Pensemos en <strong>las posibilidades que esto abre</strong> para los sistemas de traducción, corrección y transcripción mediante IA, o para los asistentes digitales tipo Alexa, o...</p>\n<!-- BREAK 13 -->\n<p>Vía | <a href=\"https://www.technologyreview.com/f/614473/tiny-ai-could-supercharge-autocorrect-voice-assistants-on-your-phone/\">MIT Technology Review</a></p>\n<!-- BREAK 14 -->\n<p>Imagen | Pixabay</p>\n</div>', '2019-10-05 20:01:59', '2019-10-05 20:01:59', 47, 'portada00.jpg', 'En algunos campos de la IA, los modelos entrenados por los investigadores son cada vez más gigantescos e inmanejables, y cada vez nos encontramos con más intentos (exitosos) para reducirlos sin reducir su eficiencia.'),
(22, 'Volver a andar gracias a la IA: Intel busca desarrollar un \'bypass inteligente\' para devolver el movimiento a lesionados de médula', '<div class=\"blob js-post-images-container\">\n<p>Dentro de poco, a las múltiples aplicaciones de la inteligencia artificial se le podría añadir una más, quizás uno de los más complejos desafíos que enfrentan hoy los investigadores en medicina: <strong>devolver a los paralíticos su capacidad de movimiento</strong>.</p>\n<!-- BREAK 1 -->\n<p>Intel y la Universidad de Brown acaban de anunciar el inicio de un proyecto, respaldado por la DARPA, para desarrollar <strong>una interfaz inteligente que permita reconectar las dos partes de la columna vertebral</strong> en pacientes que hayan sufrido lesiones graves de la médula espinal.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>El cuerpo humano no es capaz de regenerar las fibras nerviosas seccionadas, de modo que, en caso de una lesión espinal severa, <strong>las señales eléctricas enviadas por el cerebro dejan de llegar a los músculos</strong>, provocando la parálisis (así como la pérdida de control de la vejiga, uno de los aspectos que más suele preocupar a los afectados).</p>\n<!-- BREAK 3 -->\n\n<p>\"Una lesión de la médula espinal es algo devastador, y se sabe muy poco acerca de <strong>cómo pueden aprovecharse los circuitos espinales que perviven en las inmediaciones de la lesión</strong> para apoyar la rehabilitación y la restauración de la función perdida\". explica David Borton, investigador de Brown. \"Sabemos que los circuitos alrededor de la lesión espinal a menudo permanecen activos y funcionales\".</p>\n<!-- BREAK 4 -->\n<h2>Un gran avance, incluso aunque fracase</h2>\n<p>Por ello, durante dos años los científicos del proyecto se dedicarán a <strong>capturar las señales motoras y sensoriales transmitidas por la médula espinal</strong>, gracias a la implantación de electrodos en la columna de los pacientes.</p>\n<!-- BREAK 5 -->\n<p>El papel de Intel será el de aportar las herramientas de software (como la aplicación open source nGraph) y hardware que permitan la ejecución de <strong>redes neuronales que, si todo va según lo previsto, aprenderán a interpretar las señales que llegan hasta la zona lesionada</strong> y, gracias a un \"bypass inteligente\", las retransmitirán en tiempo real hasta el extremo \'aislado\' de la columna.</p>\n<!-- BREAK 6 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Espina Dorsal Intel\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/88127d/espina_dorsal_intel/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/88127d/espina_dorsal_intel/450_1000.jpg 450w, https://i.blogs.es/88127d/espina_dorsal_intel/650_1200.jpg 681w, https://i.blogs.es/88127d/espina_dorsal_intel/1024_2000.jpg 1024w, https://i.blogs.es/88127d/espina_dorsal_intel/1366_2000.jpg 1366w\"/><noscript><img alt=\"Espina Dorsal Intel\" src=\"https://i.blogs.es/88127d/espina_dorsal_intel/450_1000.jpg\"/></noscript> <span> Imagen cortesía de Intel </span> </div></div></div>\n<p>\"Este estudio exploratorio tiene como objetivo construir la combinación adecuada de hardware, software y comprensión funcional de la médula espinal para hacer posible ese sistema\", explica Borton.</p>\n<!-- BREAK 7 -->\n<p>En una primera etapa del proyecto, los investigadores tendrán que <strong>recurrir a un hardware externo para interpretar las señales de la médula espinal</strong>, pero confían en lograr, a largo plazo, desarrollar un sistema completamente implantable para ello.</p>\n<!-- BREAK 8 -->\n<p>Intel y la Universidad de Brown no prometen el éxito en su proyecto, pero sí afirman que, incluso en caso de fracasar, <strong>la información que logren recopilar supondrá un avance</strong> que permita acercar más a otros hacia un tratamiento efectivo para tratar las lesiones de médula.</p>\n<!-- BREAK 9 -->\n<p>Vía | <a href=\"https://www.zdnet.com/article/intel-brown-university-collaborate-on-intelligent-spine-technology/\">ZDnet</a></p>\n<!-- BREAK 10 -->\n<p>Imagen | <a href=\"https://www.flickr.com/photos/chiropractic/3813001430\">Michael Dorausch</a></p>\n</div>', '2019-10-04 11:01:42', '2019-10-04 11:01:42', 17, 'portada01.jpg', 'La respuesta podría estar es un dispositivo que interprete y retransmita las señales eléctricas de un lado a otro de la lesión.'),
(23, 'UPS ya es oficialmente una línea aérea: tiene luz verde en EE.UU. para operar su propia flota comercial de drones autónomos', '<div class=\"blob js-post-images-container\">\n<p>UPS acaba de convertirse en la primera empresa estadounidense a la que la FAA (la Agencia Federal de Aviación) autoriza para operar una flota comercial de drones, lo que le permitirá usarlos para <strong>entregar mercancías por todo el país integrándose de manera segura en el espacio aéreo</strong>.</p>\n<!-- BREAK 1 -->\n<p>El visto bueno de la FAA se produce después de que UPS se hubiera integrado en uno de los proyectos pilotos puestos en marcha por la actual administración para comprobar cómo podrían interactuar los drones con las personas cuyos vecindarios cruzan y con las otras naves con las que comparten espacio. Durante la realización de este proyecto piloto, <strong>los drones de UPS entregaron más de 1.100 muestras médicas</strong> en un hospital de Carolina del Norte.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p><strong>Los drones responsables de todas esas entregas eran modelos autónomos</strong> fabricados por la startup Matternet: no eran operados directamente por un piloto, sino únicamente monitorizados, y se desplazaban por rutas de vuelo predeterminadas.</p>\n<!-- BREAK 3 -->\n<p>Hay que señalar que hace 4 años que <strong>Matternet viene colaborando con el servicio postal suizo en su propio proyecto piloto de reparto</strong>... un proyecto suspendido hace dos meses tras <a href=\"https://www.xataka.com/vehiculos/dos-accidentes-drones-mensajeros-swiss-post-provocan-supension-esas-pruebas\">sufrir accidentes dos de los drones de Matternet</a>.</p>\n<!-- BREAK 4 -->\n\n<h2>El futuro de los drones de UPS (y de toda su industria)</h2>\n<p>Por ahora, <strong>las primeras operaciones comerciales de UPS seguirán enfocándose en la entrega de suministros sanitarios</strong> en campus médicos de todo Estados Unidos (más de un centenar en los próximos meses), pasando dentro de unos años a cubrir entregas de mercancías a consumidores.</p>\n<!-- BREAK 5 -->\n<p>A partir de este momento, la FAA permitirá a UPS hacer volar sus drones durante la noche y transportar paquetes de más de 55 libras (25 kilogramos), cosas que no tenían permitido hacer durante la vigencia del proyecto piloto. <strong>Seguirán, eso sí, sin poder sobrevolar zonas urbanas</strong>, como indica la normativa vigente de la FAA. </p>\n<!-- BREAK 6 -->\n<p>En declaraciones a la CNBC, el CEO de UPS, David Abney, se mostró convencido de que <strong>la evolución de la normativa terminaría ofreciendo oportunidades para distribuir en zonas residenciales</strong>. Y ello, a pesar de <a href=\"https://www.pewresearch.org/fact-tank/2017/12/19/8-of-americans-say-they-own-a-drone-while-more-than-half-have-seen-one-in-operation/\">una encuesta</a> realizada por el Pew Research Center en 2017 mostraba que el 54% de los estadounidenses se mostraban contrarios a esta posibilidad.</p>\n<!-- BREAK 7 -->\n<p><em>El siguiente vídeo muestra una prueba realizada en 2017 por UPS, al margen del proyecto piloto mencionado en este artículo:</em></p>\n<!-- BREAK 8 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/xx9_6OyjJrQ\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Por ahora, Wing Aviation (propiedad de Alphabet, la matriz de Google), Amazon y Uber <strong>están también a la espera de recibir la misma certificación que UPS</strong> de manos de la FAA. Por ahora, siguen participando en sus propios proyectos piloto supervisados por ésta.</p>\n<!-- BREAK 9 -->\n<p>Según <a href=\"https://www.morganstanley.com/ideas/autonomous-aircraft\">un estudio</a> de Morgan Stanley, <strong>las aeronaves urbanas autónomas podrían convertirse en 2040 en una industria valorada en 1,5 billones de dólares</strong> (aunque esa categoría incluye no sólo estas naves no tripuladas de entrega de paquetes, sino también los drones de despegue y aterrizaje vertical (VTOL), los taxis voladores, vehículos aéreos no tripulados militares (UAV), etc).</p>\n<!-- BREAK 10 -->\n<p>Vía | <a href=\"https://www.axios.com/newsletters/axios-autonomous-vehicles-2ae4f9f5-7b98-4b0b-b1ce-0678720df8a6.html\">Axios</a></p>\n<!-- BREAK 11 -->\n<p>Imagen | UPS</p>\n</div>', '2019-10-04 01:31:39', '2019-10-04 01:31:39', 18, 'portada02.jpg', 'Tras un proyecto piloto de varios meses y más de un millar de entregas de material sanitario a un hospital de Carolina del Norte, la FAA da el visto bueno a la flota de drones de UPS.'),
(24, 'Francia lanzará este año su sistema de identificación por reconocimiento facial y obligará a usarlo en trámites administrativos', '<div class=\"blob js-post-images-container\">\n<p>Francia quiere ser el primer país de la UE en el que la tecnología de reconocimiento facial brinde a sus ciudadanos una identidad digital segura... y el primero en <strong>imponer sus uso como condición para consultar servicios de la administración a través de sus móviles</strong>.</p>\n<!-- BREAK 1 -->\n<p>El gobierno de Emmanuel Macron lleva un tiempo impulsando la puesta en marcha de <strong>este programa, bautizado como ALICEM</strong> (siglas de Autenticación en Línea Certificada en Móviles), y quiere lograr tenerlo en marcha, aunque sea en pruebas, antes de Navidad. Por ahora, sólo será posible usar la app oficial en dispositivos móviles Android compatibles con pagos contactless.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Y eso pese a la oposición de los grupos pro-privacidad y a las reservas del regulador nacional de la privacidad de datos (el CNIL), que <strong>alegan que Alicem viola las normas europeas que regulan el consentimiento en la cesión de datos</strong>.</p>\n<!-- BREAK 3 -->\n\n<p>La asociación activista <a href=\"https://www.genbeta.com/activismo-online/propuestas-de-la-quadrature-du-net-para-proteger-la-libertad-de-comunicacion-en-internet\">La Quadrature du Net</a> presentó un recurso de anulación ante el Consejo de Estado en julio (aunque eso no frenará la puesta en marcha del proyecto, porque no se debatirá hasta dentro de año y medio). Uno de sus portavoces, Martin Drago, avisa: \"<strong>Nos dirigimos hacia la implantación masiva del reconocimiento facial</strong>\".</p>\n<!-- BREAK 4 -->\n<p>Un comunicado de esta organización, emitido en mayo, poco después de la presentación del proyecto por parte del ministro del Interior, afirmaba:</p>\n<!-- BREAK 5 -->\n<blockquote>\n<p>\"Aquí la libertad de consentimiento no existe porque la app no permite evitar el reconocimiento facial y esto es ilegal. La normativa sobre protección de datos de las informaciones biométricas define esta información como sensible, por lo tanto la recopilación y tratamiento de estos datos está prohibida\".</p>\n</blockquote>\n<p>Didier Baichere, legislador del mismo partido que Macron e integrante de la Comisión de Nuevas Tecnologías de la Asamblea Nacional francesa, reconoce que apostar por este sistema constituye <strong>\"un riesgo importante\" si se adopta de forma masiva antes de establecer los controles adecuados</strong>.</p>\n<!-- BREAK 6 -->\n\n<p>La app crea una identificación a través de un proceso de inscripción basado en comparar la foto del pasaporte biométrico del usuario con un \'vídeo selfie\' tomado por él mismo desde la app; <strong>el gobierno asegura que los datos usados para el reconocimiento facial se borrarán al terminar la inscripción</strong>. Pero eso no está evitando que muchos se preocupen por potenciales malos usos de dicha información.</p>\n<!-- BREAK 7 -->\n<p>Y es que, aunque las autoridades aseveran que la seguridad de Alicem es \"máxima\", sólo hace unos meses desde que el hacker francés Robert Baptiste <strong>tardó poco más de una hora en violar la seguridad de una app de mensajería de alta seguridad desarrollada por el estado francés</strong>. Baptiste afirma ahora, con respecto a Alicem, que el gobierno debería premiar a aquellos hackers que fueran capaces de encontrar vulnerabilidades en esta nueva tecnología antes de su lanzamiento.</p>\n<!-- BREAK 8 -->\n<h2>Los aeropuertos parisinos se suben también al carro del reconocimiento facial</h2>\n<p>Paralelamente a esta iniciativa, el consorcio que gestiona los aeropuertos de París (el ADP), junto a Air France y otra aerolínea cuyo nombre no ha transcendido, han anunciado que pondrán en marcha <strong>otro proyecto piloto para utilizar el reconocimiento facial como sistema de identificación en el aeropuerto de Orly</strong>, de tal nidi que permita depositar el equipaje y embarcar con el único requisito de mostrar la cara.</p>\n<!-- BREAK 9 -->\n\n<p>En la primera etapa del proyecto, que se iniciará en 2020, sólo 3 vuelos regulares, fundamentalmente de ámbito europeo, formarán parte de esta experiencia. Tras el primer año, el ADP confía en poder generalizar el sistema para que, <strong>a partir de 2024 o 2025, todos los aeropuertos parisinos puedan recurrir al mismo durante todo el recorrido del pasajero</strong>.</p>\n<!-- BREAK 10 -->\n<p><strong>De nuevo, el CNIL ha tomado cartas en el asunto</strong> exigiendo que \"el dispositivo no capte la mirada de los transeúntes que circulen en segundo plano\", que \"no se deberán conservar los datos\" y que el pasajero, que deberá dar su acuerdo, \"estará obligado a repetir el procedimiento cada vez que pasa por un aeropuerto\". No está previsto, en todo caso, que el ADP cruce información con la base de datos del ministerio del Interior con el fin de vincular a los pasajeros a alertas de los servicios de inmigración o aduana.</p>\n<!-- BREAK 11 -->\n<p>Vía | <a href=\"https://www.businesstimes.com.sg/government-economy/france-set-to-roll-out-nationwide-facial-recognition-id-programme\">The Business Times</a> &amp; <a href=\"https://www.france24.com/es/20191003-francia-reconocimiento-facil-tarjeta-embarque\">France24</a></p>\n<!-- BREAK 12 -->\n<p>Imagen | Rama (<a href=\"https://es.wikipedia.org/wiki/Archivo:Marianne-Jacques_France-IMG_1213.JPG\">vía Wikipedia</a>)</p>\n<!-- BREAK 13 --> </div>', '2019-10-03 17:01:32', '2019-10-03 17:01:32', 30, 'portada03.jpg', 'Paralelamente, los aeropuertos de París se preparan para permitir que, a partir de 20255, todos sus usuarios puedan embarcar recurriendo a esta tecnología.'),
(25, 'La fuga de cerebros a EE.UU. frenó las aspiraciones de liderazgo de China en el campo de la IA… pero las tornas ya están cambiando', '<div class=\"blob js-post-images-container\">\n<p>Como parte de su estrategia para <strong>lograr el pleno desarrollo de las fuerzas productivas del país</strong>, el régimen chino lleva décadas invirtiendo en igualar su industria tecnológica con las de las grandes potencias occidentales. Y no sólo eso, sino que, en los últimos años, se ha traducido en su reconocida pretensión de <strong>convertirse en líder mundial en el campo de la inteligencia artificial</strong>, como muy tarde, en 2030.</p>\n<!-- BREAK 1 -->\n<p>Este esfuerzo gubernamental se ha traducido en logros como que el número de investigadores chinos en el campo de la IA se haya multiplicado por 10 en la última década. Sin embargo, <strong>no todo este esfuerzo ha ido destinado, hasta ahora, a reforzar las pretensiones de liderazgo de China</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>¿El motivo? <strong>La mayoría de esos investigadores (alrededor de tres cuartas partes, de hecho) trabajan fuera de China</strong>.  Y de hecho, entre esos expatriados, el 85% lo hace en compañías e instituciones académicas de referencia de su gran rival tecnológico y comercial, los Estados Unidos. Qin Zhongjun, investigador en Shangai, explica que</p>\n<!-- BREAK 3 -->\n<blockquote>\n<p>\"La pérdida de un estudiante de doctorado que sale al extranjero no sólo representa perderle durante una etapa dorada de su creatividad investigadora, sino regalar métodos e ideas desarrolladas en laboratorios chinos a nuestros competidores más innovadores\".</p>\n</blockquote>\n\n<p>Así, los problemas de retención del talento interno en China ayudan a que los esfuerzos formativos de Pekín terminen beneficiando a Google, IBM o la UCLA e, <strong>indirectamente, refuerzan la posición de EE.UU. como líder en el campo de la inteligencia artificial</strong>.</p>\n<!-- BREAK 4 -->\n<h2>Una tendencia que empieza a revertirse</h2>\n<p>Pero esta tendencia ya ha comenzado a cambiar: consciente del problema, el régimen chino incluyó en su <a href=\"https://www.xataka.com/robotica-e-ia/17-areas-que-gobierno-chino-considera-clave-para-alcanzar-liderazgo-campo-inteligencia-artificial\">Estrategia Nacional de IA</a>, aprobada en 2017, medidas que <strong>incentivaran el retorno de los mejores investigadores chinos residentes en el extranjero</strong>.</p>\n<!-- BREAK 5 -->\n<p>En general, China ha ido aprobando medidas similares para todas las áreas científicas, expuestas todas ellas a dinámicas similares en lo relativo a la pérdida de cerebros. Ahora, según afirma al South China Morning Post Chen Guoqiang, director del Centro de Biología sintética de la Universidad de Tsinghua, <strong>\"el problema de la fuga de cerebros ya no existe\"</strong>.</p>\n<!-- BREAK 6 -->\n<div class=\"article-asset-summary article-asset-small\"><div class=\"asset-content\"><div class=\"article-asset-summary article-asset-small\"><div class=\"asset-content\"><p class=\"sumario_izquierda\">En China, la creciente marea de estudiantes retornados ha empezado a recibir el simbólico nombre de \'haigui\'(tortugas marinas).</p></div></div></div></div>\n<p>Pero Guoqiang no atribuye este cambio únicamente a méritos propios de China, también a deméritos de su gran rival: <strong>\"Un motivo importante es el salario, pero el otro es Trump\"</strong>.</p>\n<!-- BREAK 7 -->\n<p>Las restricciones aprobadas por la actual administración estadounidense hacia los estudiantes chinos (Trump ha llegado a dar a entender que la mayoría de ellos eran potenciales espías) se han convertido en <strong>un poderoso incentivo para renunciar a buscar trabajo en EE.UU.</strong> tras la obtención de su posgrado y optar, en su lugar, por el retorno a China.</p>\n<!-- BREAK 8 -->\n<p>En 2007, según datos del propio Ministerio de Educación chino, la mayoría (el 77%) de los chinos recién posgraduados en el extranjero preferían permanecer en Occidente, mientras que <strong>ahora sólo el 20% anteponen esta opción al retorno a su patria</strong>.</p>\n<!-- BREAK 9 -->\n\n<p>Ayuda a ello que en China las becas de investigación postdoctoral alcancen ya <strong>cifras equivalentes al doble del salario promedio para ese mismo perfil en EE.UU.</strong>. Y para reforzar esta tendencia, las universidades e institutos de investigación chinos han empezado a revisar sus criterios de evaluación de talento para <strong>reducir la importancia dada a la experiencia investigadora en Occidente</strong> a la hora de valorar currículums.</p>\n<!-- BREAK 10 -->\n<p>Todo esto empieza a ser tema a debate en los ámbitos académicos de los Estados Unidos, donde los expertos empiezan a preocuparse por que, a medida que China se vuelve más atractivo para el talento investigador (el gigante asiático está apostando por recuperar el talento sin cerrarse a captar a su vez talento extranjero), <strong>los EE.UU. estén perdiendo una de sus mayores ventajas competitivas</strong>: su capacidad para atraer a los mejores talentos del mundo.</p>\n<!-- BREAK 11 -->\n<p>Vía | <a href=\"https://www.technologyreview.com/f/614092/china-ai-domination-losing-talent-to-us/\">MIT Technology Preview</a> &amp; <a href=\"https://www.scmp.com/news/china/science/article/2163001/chinas-brain-drain-us-ending-thanks-higher-salaries-and-donald\">South China Morning Post</a> &amp; <a href=\"https://www.nbcnews.com/tech/tech-news/china-s-rising-tech-scene-threatens-u-s-brain-drain-n1029256\">NBC News</a></p>\n<!-- BREAK 12 -->\n<p>Imagen | Pixabay</p>\n</div>', '2019-10-03 12:01:31', '2019-10-03 12:01:31', 7, 'portada04.jpg', 'Uno de cada cuatro investigadores chinos de inteligencia artificial trabajan fuera de su país (mayoritariamente en EE.UU.), pero la capacidad de retención del talento de China ha aumentado de forma notable en los últimos años.'),
(26, 'Ha comenzado la carrera para crear la tecnología capaz de detectar los deepfakes, pero los falsificadores llevan ventaja', '<div class=\"blob js-post-images-container\">\n<p>Dos años después del nacimiento de los deepfakes, tanto la industria tecnológica como el ámbito académico se afanan en desarrollar soluciones que permitan <strong>automatizar la detección de material multimedia falsificado</strong> (vídeos, imágenes y grabaciones de voz) mediante el uso de inteligencia artificial.</p>\n<!-- BREAK 1 -->\n<p>El motivo detrás de estos esfuerzos es tan sencillo como preocupante: los algoritmos de detección de deepfakes <strong>siguen yendo por detrás de la tecnología usada para generarlos</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Sin ir más lejos, no hace ni un mes desde que Facebook anunció que <a href=\"https://www.xataka.com/inteligencia-artificial/nuevas-herramientas-para-detectar-deepfakes-objetivo-que-facebook-invertira-10-millones-dolares\">invertiría nada menos que 10 millones de dólares</a> en la creación de un dataset que recopilase deepfakes y en la organización de <strong>un concurso que impulsara el desarrollo de tecnología para la detección de los mismos</strong>. Microsoft es otro de los gigantes de Silicon Valley que aparece como colaborador en este proyecto.</p>\n<!-- BREAK 3 -->\n<p>Google también <a href=\"https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html\">anunció</a> la semana pasada la creación de un dataset de deepfakes visuales, elaborado en colaboración con Jigsaw, a partir de la grabación de cientos de vídeos protagonizados por actores.</p>\n<!-- BREAK 4 -->\n\n<p>Todos estos vídeos, originales y manipulados, se han incorporado al dataset, ya disponible para la comunidad investigadora. <strong>Y avisan de que irán incorporando nuevos deepfakes recurriendo a los nuevos métodos que vayan desarrollándose</strong>.</p>\n<!-- BREAK 5 -->\n<h2>Ninguna tecnología será nuestra \'bala de plata\'</h2>\n<p>Siwey Lyu, investigador de la Univ. de Albany y uno de los mayores expertos mundiales en detección de deepfakes, ha impulsado la creación de <a href=\"https://www.youtube.com/watch?v=TqbxGbpFUK4\">otro dataset de vídeos</a>, DeepFake Forensic (DFF) con el mismo objetivo: <strong>ir incorporando nuevas muestras de deepfakes</strong> representativos de la tecnología más avanzada en cada momento.</p>\n<!-- BREAK 6 -->\n<p>Lyu, que participó hace unos días en un subcomité de la Cámara de Representantes de EE.UU. que estudiaba la suplantación y la desinformación en Internet, explicaba entonces que</p>\n<!-- BREAK 7 -->\n<blockquote>\n<p>\"es importante contar con tecnologías efectivas para identificar, contener y obstruir los deepfake antes de que puedan causar daños. Esto debe hacerse centrándose en mejorar nuestras capacidades forenses y lograr que sea más difícil entrenar generadores de fakes mediante el uso de vídeos online. [...] Debido a la naturaleza compleja de los deepfakes, ningún método o tecnología en concreto será una \'bala de plata\'\".</p>\n</blockquote>\n<div class=\"article-asset-embed-giphy article-asset-normal article-asset-center\">\n<div class=\"article-asset-video\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\" id=\"_giphy_MeDr4B9ng4T3RbAmZE\">\n<iframe allowfullscreen=\"\" class=\"giphy-embed\" src=\"https://giphy.com/embed/MeDr4B9ng4T3RbAmZE\" style=\"position:absolute\"></iframe>\n</div>\n</div>\n</div>\n</div>\n<p>Él apuesta por combinar métodos de detección forense: <strong>buscar rastros del proceso de síntesis</strong> (caras deformadas para adaptarse a la anatomía del objetivo) <strong>o de inconsistencias fisiológicas</strong> (como <a href=\"https://www.xataka.com/inteligencia-artificial/posible-saber-video-deepfake-solo-abrir-cerrar-ojos-literalmente-quizas-eso-no-sea-suficiente\">la ausencia de parpadeo realista</a>), además de apostar por \"<strong>usar a la IA para cazar a la IA</strong>\", recurriendo a redes neuronales para que aprendan a detectar los patrones característicos de los deepfakes.</p>\n<!-- BREAK 8 -->\n<p>Pero, ¿cómo propone Lyu obstaculizar el uso de imágenes y vídeos para entrenar nuevas IAs de generación de deepfakes? Introduciendo <a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-inteligencia-artificial-antagonica-como-puede-manipular-a-otras-ias\">\"ruido antagónico\"</a> invisible para el ojo humano pero <strong>capaz de boicotear el uso de algoritmos de detección facial</strong>, obligando al falsificador a ir seleccionando a mano miles de puntos del vídeo.</p>\n<!-- BREAK 9 -->\n<h2>¿Un paso adelante y dos pasos atrás?</h2>\n<p>Dessa, una compañía de inteligencia artificial, <a href=\"https://medium.com/@dessa_/detecting-audio-deepfakes-f2edfd8e2b35\">ha hecho público</a> un software de código abierto enfocado a la detección de <a href=\"https://www.xataka.com/inteligencia-artificial/no-te-creas-nada-que-oigas-llegan-deep-fakes-audio\">deepfakes de audio</a>, una herramienta que hubieran agradecido tener a mano los <a href=\"https://www.xataka.com/inteligencia-artificial/crecen-ataques-deepfakes-audio-suplantacion-ceos-este-sistema-hizo-perder-millones-a-tres-grandes-companias\">CEOs que fueron suplantados hace unos meses mediante llamadas de móvil</a>.</p>\n<!-- BREAK 10 -->\n<p>Ragavan Thurairatnam, co-fundador de Dessa, cree que será \"<strong>inevitable que los actores malintencionados se muevan mucho más rápido que aquellos que quieren detenerlos</strong>\", pero confía también en que su detector libre sea un \"punto de partida\" para avanzar en la detección de deepfakes.</p>\n<!-- BREAK 11 -->\n\n<p>Y sin embargo, este movimiento podría ser \"una de cal y otra de arena\": el propio Thurairatnam reconoce que los sistemas de IA <a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-gans-redes-generativas-antagonicas\">generativos</a> pueden ser <strong>entrenados con el objetivo de engañar a un detector específico</strong>, si bien confía en que su potencial para ayudar a crear nuevas y mejores herramientas de detección compensa cualquier otro mal uso.</p>\n<!-- BREAK 12 -->\n<p>Lyu, sin embargo, no está de acuerdo con este optimista previsión: él sí cree que haya razones para pensar que, <strong>a la larga, será peor haber liberado el código de esta herramienta</strong>:</p>\n<!-- BREAK 13 -->\n<blockquote>\n<p>\"Al principio, el código ayudará a ambas partes, pero probablemente termine repercutiendo en [la creación de] mejores generadores\".</p>\n</blockquote>\n<p>En su comparecencia frente a la Cámara de Representantes, abundaba a la hora de explicar esta desventaja de los \'buenos\':</p>\n<!-- BREAK 14 -->\n<blockquote>\n<p>\"A medida que esta tecnología continúe desarrollándose, las barreras actuales a la creación de deepfakes disminuirán y la calidad de éstos seguirá mejorando\".</p>\n<p>\"Lo que también está evolucionando es el juego del gato y el ratón que experimentan todas las relaciones atacante-defensor, y los atacantes  parecen contar con una ventaja: la posibilidad de ajustar el algoritmo de generación cada vez que se hace público un nuevo método de detección\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://www.axios.com/newsletters/axios-future-c8b0ab00-4cd0-4bf1-baae-bd050d89072f.html\">Axios</a> &amp; <a href=\"https://science.house.gov/imo/media/doc/Lyu%20Testmimony.pdf\">House.gov</a></p>\n<!-- BREAK 15 -->\n<p>Imagen | <a href=\"https://pixabay.com/es/illustrations/detective-sleuth-british-detective-4088741/\">Pixabay</a></p>\n<!-- BREAK 16 --> </div>', '2019-10-01 10:00:51', '2019-10-01 10:00:51', 26, 'portada05.jpg', 'Dos años después del nacimiento de los deepfakes, tanto la industria tecnológica como el ámbito académico se afanan en desarrollar soluciones que permitan automatizar la detección de material multimedia falsificado.');
INSERT INTO `posts` (`id`, `titulo`, `cuerpo`, `created_at`, `updated_at`, `user_id`, `imagen`, `descripcion`) VALUES
(27, 'La NASA quiere sus propios \'transformers\': Está diseñando robots cambiaformas para explorar los lagos de Titán', '<div class=\"blob js-post-images-container\">\n<p>Antes de <a href=\"https://www.xataka.com/espacio/cassini-y-su-gran-adios-como-y-donde-ver-en-directo-el-gran-final-de-la-sonda-en-saturno\">morir desintegrándose en la atmósfera de Saturno</a>, la sonda Cassini tuvo ocasión de <strong>sobrevolar más de 100 veces Titán, la mayor de sus lunas</strong>. Gracias a eso, pudimos conocer su asombroso parecido superficial on la Tierra: lagos, lluvia y acantilados forman parte del paisaje de Titán, si bien allí los ríos son de metano y las rocas de hielo de agua.</p>\n<!-- BREAK 1 -->\n<p>Para conocer más sobre este satélite, la NASA anunció el pasado mes de junio que dentro de siete años lanzará la misión <a href=\"https://www.xataka.com/espacio/dragonfly-proxima-gran-mision-nasa-enviara-drone-a-titan-luna-grande-saturno-para-buscar-origen-vida\">\'DragonFly\' (\"Libélula\")</a> con el objetivo de introducir <strong>un helicóptero no tripulado (e impulsado mediante energía nuclear) en la atmósfera titaniana</strong> en 2034.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Sin embargo, la NASA ya está desarrollando otra máquina más avanzada para que algún día pueda darnos más información sobre la superficie de Titán. <strong>Su nombre provisional es \'Shapeshifter\' (\"Cambiaformas\") y será un robot compuesto de otros más pequeños que podrán combinarse entre sí</strong> en diferentes configuraciones que le permitan rodar, nadar, volar o flotar.</p>\n<!-- BREAK 3 -->\n\n<h2>Explorando en equipo (y sin brújula)</h2>\n<p>Ali Agha, investigador principal del Laboratorio de Propulsión a Chorro de la NASA, explica que</p>\n<blockquote>\n<p>\"Tenemos muy poca información sobre la superficie [...] por lo que nos pusimos a pensar en cómo crear un sistema que fue versátil y capaz de atravesar diferentes tipos de terreno, pero también suficientemente compacto como para lanzarlo en un cohete\".</p>\n</blockquote>\n<p>El sistema por el que optó el departamento de Conceptos Innovadores Avanzados de la NASA fue el uso de los <strong>\'cobots\'</strong>, las \'piezas\' del Cambiaformas, pequeños drones cuadricópteros exploradores capaces de moverse, <strong>acoplarse y desacoplarse de manera autónoma y cooperativa</strong>, según las condiciones del terreno en cada momento, sea éste sólido o líquido.</p>\n<!-- BREAK 4 -->\n<p>Así, por ejemplo, parte de los cobots podrían ensamblarse formando una semiesfera capaz de flotar en el helio líquido, mientras otros permanecen en el aire mapeando posibles obstáculos con los que puedan encontrarse más adelante. Dicha información servirá para que el conjunto de cobots <strong>se coordinen para adoptar, si es necesario, una nueva forma que facilite su avance</strong>.</p>\n<!-- BREAK 5 -->\n\n<p>Esta capacidad para mapear el terreno es fundamental, porque <strong>será el único método de orientación disponible para el Cambiaformas</strong>: dado que no tenemos satélites orbitando Titán, la tecnología GPS no es una opción, y el satélite carece incluso de un campo magnético que pudiera servir de referencia.</p>\n<!-- BREAK 6 -->\n<p>El prototipo actualmente operativo es, por ahora, sólo semiautónomo, pero los científicos se han marcado como objetivo lograr que <strong>su inteligencia artificial les permita operar sin necesidad de ninguna orientación humana</strong>.</p>\n<!-- BREAK 7 -->\n<p>Jason Hofgartner, científico jefe del proyecto Shapeshifter, explica que</p>\n<blockquote>\n<p>\"La gran versatilidad del Cambiaformas permitirá acceder a todos los espacios científicamente relevantes [...] que a menudo son los de más difícil acceso\".</p>\n</blockquote>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/DZ6PLllJFzI\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Vía | <a href=\"https://www.inverse.com/article/59632-nasa-creating-shape-shifting-robots-to-explore-titan\">Inverse</a></p>\n<!-- BREAK 8 -->\n<p>Imagen | NASA</p>\n</div>', '2019-09-29 16:06:52', '2019-09-29 16:06:52', 28, 'portada06.jpg', 'Pequeños drones cuadricópteros capaces de ensamblarse entre sí mediante inteligencia artificial: éste es el plan de la NASA para explorar la superficie de Titán, la mayor luna de Saturno.'),
(28, 'Jeff Bezos anuncia que Amazon está redactando sus propias leyes sobre reconocimiento facial que presentará después a los políticos', '<div class=\"blob js-post-images-container\">\n<p>El pasado mes de febrero, en plena controversia sobre <a href=\"https://www.xataka.com/seguridad/amazon-ha-entrado-en-el-negocio-de-la-vigilancia-vende-tecnologia-de-reconocimiento-facial-a-policias-de-eeuu\">el uso de su software de reconocimiento facial Rekognition</a>, Amazon hizo público en su blog <a href=\"https://aws.amazon.com/blogs/machine-learning/some-thoughts-on-facial-recognition-legislation/\">una lista de \"pautas éticas\"</a> sobre dicha tecnología, y <strong>pidió que los políticos estadounidenses las tuvieran en cuenta a la hora de legislar</strong>.</p>\n<!-- BREAK 1 -->\n<p>Pero ahora, la compañía ha querido ir un paso más allá: su CEO, <a href=\"https://www.xataka.com/empresas-y-economia/jeff-bezos-se-convierte-hombre-rico-historia-al-poseer-fortuna-que-supera-150-000-millones-dolares\">Jeff Bezos</a>, ha afirmado que están redactando un paquete de propuestas de ley destinadas a regular el uso la tecnología de reconocimiento facial; unas <strong>propuestas que que planean compartir con los legisladores estadounidenses para convencerlos de que las conviertan en leyes</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Tras una pregunta de un periodista, Bezos afirmó que</p>\n<blockquote>\n<p>\"tiene mucho sentido regular esto: es un ejemplo perfecto de algo que tiene aplicaciones realmente positivas, por lo que no debes frenarlo; pero, al mismo tiempo, también existe la posibilidad de abusar de esta clase de tecnología, por lo que las regulaciones son deseables\".</p>\n</blockquote>\n\n<h2>Amazon, sujeto y objeto de la labor de lobby</h2>\n<p>Entra dentro de lo habitual que los grupos de presión (empresariales o de cualquier otra clase) intenten influir sobre los políticos para dirigir la legislación en uno u otro sentido, pero <strong>reconocer ante los medios que mandas a los congresistas las leyes hechas y listas para aprobar</strong> parece estar un paso más allá de todo eso.</p>\n<!-- BREAK 3 -->\n<p>Este movimiento constituye un reconocimiento por parte de Amazon de la <strong>creciente importancia de Rekognition para su servicio AWS</strong> (Amazon Web Services) y, al mismo tiempo, de que son conscientes de la <strong>necesidad de tranquilizar tanto a los movimientos activistas como a sus propios trabajadores</strong>, para evitar así sufrir la misma clase de <a href=\"https://www.xataka.com/empresas-y-economia/que-es-project-maven-y-por-que-3-100-empleados-de-google-le-piden-a-la-empresa-que-lo-abandone\">protestas internas que ya han forzado a otras compañías a abandonar proyectos</a> sensibles relacionados con la IA.</p>\n<!-- BREAK 4 -->\n<p>Bezos, en cualquier caso, <strong>Bezos no ha dado detalles sobre el enfoque o las demandas de su propuesta normativa</strong>, por lo que todavía podría estar lejos de las exigencias de los grupos que protestaron hace unos meses contra Rekognition.</p>\n<!-- BREAK 5 -->\n<p>La Unión Americana de Libertades Civiles (ACLU), por ejemplo, que ya en febrero cargó contra Rekognition no sólo por sus consecuencias sobre la privacidad, sino también por supuestos <a href=\"https://www.xataka.com/privacidad/sistema-reconocimiento-facial-amazon-falla-confunde-28-congresistas-delincuentes\">sesgos raciales</a>, y que <strong>juzgó como \"huecas\" las pautas éticas publicadas por Amazon</strong>, es ahora poco entusiasta a la hora de valorar el movimiento de la compañía. Uno de sus abogados ha afirmado lo siguiente en un comunicado:</p>\n<!-- BREAK 6 -->\n<blockquote>\n<p>\"Es una buena señal que Amazon finalmente reconozca los peligros de la vigilancia mediante reconocimiento facial. Pero hemos visto este libro de jugadas antes: una vez que las empresas se dan cuenta de que la gente exigen fuertes protecciones para defender su privacidad, ellas intervienen y aplican reglas débiles que no protegerán ni privacidad ni los derechos de los consumidores\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://www.vox.com/recode/2019/9/25/20884427/jeff-bezos-amazon-facial-recognition-draft-legislation-regulation-rekognition\">Recode</a></p>\n<!-- BREAK 7 -->\n<p>Imagen | <a href=\"https://www.flickr.com/photos/home_of_chaos/37909812316\">thierry ehrmann</a></p>\n</div>', '2019-09-27 18:01:31', '2019-09-27 18:01:31', 44, 'portada07.jpg', 'Presionado por los activistas pro-privacidad, Amazon trata de influir sobre los límites que se pondrán en EE.UU. a la tecnología de reconocimiento facial una vez que se legisle. Y, para ello, quiere ser quien escriba esas leyes.'),
(29, 'Este sistema de inteligencia artificial antirrobo resume varios de los problemas de sesgos atribuidos a los algoritmos', '<div class=\"blob js-post-images-container\">\n<p>Una startup estadounidense de inteligencia artificial llamada Standard Cognition ha anunciado que está probando una tecnología desarrollada para permitir comprar en tiendas sin necesidad cajeros ni de detenerse a pagar de ninguna manera: <strong>sólo llegar, coger el producto deseado y salir</strong>. Al estilo de los supermercados Amazon Go.</p>\n<!-- BREAK 1 -->\n<p>Y todo gracias al <strong>uso de cámaras y algoritmos para controlar de cerca los movimientos de los clientes</strong> y su interacción con los productos de los estantes. Pero lo relevante de la noticia no es la tecnología en sí, ni las nuevas dinámicas de consumo que podría favorecer, ni nada por el estilo.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p><strong>No: lo relevante es su modo de prevenir el robo en tiendas</strong>. Los algoritmos de IA de Standard Cognition pueden, según explica la compañía, reconocer \"signos reveladores\" en los movimientos y actitudes de los clientes, su velocidad, sus miradas o movimientos. Todos estos factores se comunican inmediatamente al personal de la tienda. Pero, ¿cómo ha llegado a reconocerlos?</p>\n<!-- BREAK 3 -->\n\n<h2>El problema de los datos de entrenamiento</h2>\n<p><strong>El machine learning es especialmente efectivo reconociendo patrones</strong>: si se le ofrecen grandes cantidades de datos reales de ejemplo, <a href=\"https://www.xataka.com/robotica-e-ia/machine-learning-y-deep-learning-como-entender-las-claves-del-presente-y-futuro-de-la-inteligencia-artificial\">es capaz de \"aprender\"</a> qué tienen en común dichos datos y aplicar dicho \"aprendizaje\" a reconocer nuevos datos relacionados.</p>\n<!-- BREAK 4 -->\n<p>Standard Cognition decidió aportar a su IA, como método para ayudarla a reconocer robos, las <strong>grabaciones de miles de robos protagonizados por un centenar de actores contratados</strong>, refinando el sistema hasta obtener una tasa de precisión superior al 99%.</p>\n<!-- BREAK 5 -->\n<p>Las reacciones en Twitter no se han hecho esperar:</p>\n<blockquote>\n<p>\"Personas inocentes serán arrestadas, encarceladas, acusadas y posiblemente presionadas para declarase culpables, por culpa, tan sólo, de la idea que un actor tenía sobre cómo podría ser un robo\"</p>\n</blockquote>\n<p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">Innocent people are going to be arrested, jailed, indicted, and probably pressured into accepting a plea bargain, because of an actor\'s idea of what theft might look like.<br/><br/>Tell me again how algorithms aren\'t political... <a href=\"https://t.co/V2uELu8Zsi\">https://t.co/V2uELu8Zsi</a></p>— Eric McCorkle (@eric_mccorkle) <a href=\"https://twitter.com/eric_mccorkle/status/1177015362040283136?ref_src=twsrc%5Etfw\">September 26, 2019</a></blockquote> <script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script></p>\n<!-- BREAK 6 -->\n<blockquote>\n<p>\"Buena suerte a cualquiera que pasee, mire o se comporte como lo hicieron esos actores\".</p>\n</blockquote>\n<p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">How did they train their shoplifting detection, you might ask? <br/><br/>They had 100 actors shop “for hours” and presumably acted like people shoplifting. <br/><br/>Good luck to anyone who happens to walk, gaze, or behave in the way that these actors did. <a href=\"https://t.co/OlSORjzEiT\">pic.twitter.com/OlSORjzEiT</a></p>— One Ring (doorbell) to surveil them all... (@hypervisible) <a href=\"https://twitter.com/hypervisible/status/1176567319872622593?ref_src=twsrc%5Etfw\">September 24, 2019</a></blockquote> <script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script></p>\n<!-- BREAK 7 -->\n<blockquote>\n<p>\"Hacer que los actores finjan estar robando y usar eso para entrenar tus datos es el equivalente a entrenar un sistema de reconocimiento de criminales con imágenes de villanos de dibujos animados\".</p>\n</blockquote>\n<p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">Quote from <a href=\"https://twitter.com/provisionalidea?ref_src=twsrc%5Etfw\">@provisionalidea</a> \"...getting actors to pretend to steal and using that to train your data is the equivalent of training an image recognition on cartoon villains to identify bad guys.\" ???? <a href=\"https://t.co/hvDpTMw2IC\">pic.twitter.com/hvDpTMw2IC</a></p>— One Ring (doorbell) to surveil them all... (@hypervisible) <a href=\"https://twitter.com/hypervisible/status/1176582348739108865?ref_src=twsrc%5Etfw\">September 24, 2019</a></blockquote> <script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script></p>\n<!-- BREAK 8 -->\n<p>Ya hemos abordado varias veces <strong>polémicas sobre los sesgos de algoritmos de IA</strong>, y la respuesta suele ser casi siempre la misma: <a href=\"https://www.xataka.com/inteligencia-artificial/problema-esta-ia-google-no-gira-torno-a-ningun-sesgo-racial-sino-a-polemica-socio-politica\">el sesgo no está en el algoritmo, sino en la calidad de los datos</a>. En muchos casos, el mal funcionamiento de los algoritmos de identificación es atribuible al uso de pocos datos en el entrenamiento (y <a href=\"https://www.xataka.com/robotica-e-ia/30-millones-ejemplos-conduccion-humana-suficientes-para-ensenar-al-coche-autonomo-a-gestionar-imprevistos\">a veces millones pueden ser pocos</a>), o a no usar datos extraídos de la realidad.</p>\n<!-- BREAK 9 -->\n<p>Pero... ¿acaso <strong>el riesgo de sesgo desaparecería sólo con usar muchos vídeos de robos reales</strong>? No: aún entonces, es posible mostrar sesgo en la selección y privilegiar, por ejemplo, vídeos que muestran únicamente a delincuentes vinculados a determinados grupos sociales.</p>\n<!-- BREAK 10 -->\n<p>Vía | <a href=\"https://venturebeat.com/2019/09/24/standard-cognition-to-deploy-cashierless-tech-at-polar-stadium/\">VentureBeat</a> </p>\n<!-- BREAK 11 -->\n<p>Imagen | <a href=\"https://www.flickr.com/photos/c1ssou/3366604069\">Cyril Caton</a></p>\n</div>', '2019-09-27 03:51:22', '2019-09-27 03:51:22', 17, 'portada08.jpg', 'Una tecnología para cobrar a los clientes por los productos que se llevan de una tienda (sin que tengan que hacer cola en ninguna caja registradora) oculta un buen ejemplo de lo que no hay que hacer a la hora de crear IAs \'precrimen\'.'),
(30, 'Alibaba ha creado su primer chip de IA propio: sigue el ejemplo de Huawei y reduce su dependencia de la tecnología estadounidense', '<div class=\"blob js-post-images-container\">\n<p>El gigante chino del comercio electrónico, Alibaba, presentó ayer <strong>su primer chip de inferencia de inteligencia artificial</strong> durante la Conferencia de Computación de Apsara que Alibaba Cloud organiza todos los años en la ciudad de Hangzhou.</p>\n<!-- BREAK 1 -->\n<p>El chip, <strong>bautizado con el nombre de Hanguang 800</strong>, se ha desarrollado con el objetivo de que mejorar los servicios de la división de computación en la nube del holding chino. De hecho, la compañía ya está usando este modelo para ofrecer recomendaciones personalizadas, traducciones automáticas y búsquedas de productos.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>El Hanguang 800 permite, según Alibaba, realizar tareas vinculadas al machine learning con mayor velocidad y un menor gasto energético. Hasta ahora, según la compañía, su portal Taobao necesitaba invertir una hora entera para clasificar los 1000 millones de imágenes de productos que se suben diariamente al mismo... pero <strong>gracias al nuevo chip, fue posible completar dicha tarea en tan sólo 5 minutos</strong>.</p>\n<!-- BREAK 3 -->\n\n<h2>Un chip que llega en plena guerra comercial</h2>\n<p>Alibaba ha confirmado que <strong>no venderá este chip como un producto independiente</strong>: en su lugar, la compañía tiene planes de <strong>utilizarlo únicamente en sus productos de cloud computing</strong>, utilizándolo para impulsar la que constituye la división de negocio con mayor ritmo de crecimiento del grupo Alibaba y lo que ha definido como \"un área crítica para el futuro de la compañía\".</p>\n<!-- BREAK 4 -->\n<p>La compañía no ha revelado cuándo abrirá el acceso a las capacidades de estos chips para los clientes de su nube, pero una vez que lo haga resultará fundamental para ayudar a que <strong>las empresas chinas reduzcan su dependencia tecnológica respecto a los Estados Unidos</strong>, un paso fundamental en el actual contexto de guerra tecnológica y comercial entre China y la potencia americana.</p>\n<!-- BREAK 5 -->\n<div class=\"base-asset-media\"><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"200\" id=\"audio_36098470\" scrolling=\"no\" src=\"https://www.ivoox.com/player_ej_36098470_4_1.html?c1=ff6600\" style=\"border:1px solid #EEE; box-sizing:border-box; width:100%;\"></iframe></div>\n<p>El propio régimen de <strong>Pekín está incentivando a sus gigantes tecnológicos para que promuevan la industria local de semiconductores</strong> y reduzcan así la dependencia del país frente a las exportaciones americanas. De hecho, Alibaba no hace más que seguir el ejemplo de sus compatriotas de Huawei, que hace poco presentaron su propio chip con IA, el llamado Ascend 910. Hace un año, Baidu fue pionera entre las grandes tecnológicas chinas <a href=\"https://www.xataka.com/robotica-e-ia/baidu-google-chino-tiene-listo-su-primer-chip-inteligencia-artificial-conquista-china-mundo\">lanzando el Kunlun</a>.</p>\n<!-- BREAK 6 -->\n<p>El desarrollo de este chip es uno de los resultados de la inversión de más de 15.000 millones de dólares llevada a cabo por Alibaba en <strong>su gran proyecto de I+D+i, el Alibaba DAMO Academy</strong>, con el que la compañía afirma aspirar a crear 100 millones de empleos en los próximos 16 años.</p>\n<!-- BREAK 7 -->\n<p>En un comunicado, el presidente de Alibaba Cloud Intelligence, Jeff Zhang afirmó que</p>\n<blockquote>\n<p>\"El lanzamiento del Hanguang 800 constituye un paso importante en nuestra búsqueda de tecnologías de nueva generación, uno que aumenta las capacidades de computación tanto de nuestros negocios presentes como futuros al tiempo que mejora la eficiencia energética\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://techcrunch.com/2019/09/24/alibaba-unveils-hanguang-800-an-ai-inference-chip-it-says-significantly-increases-the-speed-of-machine-learning-tasks/\">TechCrunch</a></p>\n<!-- BREAK 8 -->\n<p>Imagen | Alibaba</p>\n</div>', '2019-09-26 14:01:27', '2019-09-26 14:01:27', 30, 'portada09.jpg', 'Este chip, bautizado como Hanguang 800, ha sido desarrollado por DAMO Academy, un instituto de I+D que Alibaba creó hace dos años.'),
(31, 'El discurso de Boris Johnson en la ONU acerca de la inteligencia artificial termina divagando sobre \"pollos sin extremidades\"', '<div class=\"blob js-post-images-container\">\n<p>Horas antes de verse obligado a volver deprisa y corriendo a Londres para participar en la reapertura de la Cámara de los Comunes que <a href=\"https://magnet.xataka.com/en-diez-minutos/boris-johnson-quiere-brexit-a-toda-costa-para-ello-tenga-que-cerrar-parlamento\">él mismo intentó impedir</a>, el primer ministro británico <strong>Boris Johnson realizó un discurso ante la Asamblea General de la ONU que giró en torno a los peligros de la inteligencia artificial</strong>. Supuestamente.</p>\n<!-- BREAK 1 -->\n<p>Hace apenas tres semanas desde que <a href=\"https://www.xataka.com/inteligencia-artificial/mejores-momentos-debate-jack-ma-elon-musk-inteligencia-artificial-waic-2019\">Elon Musk, en pleno debate sobre IA contra Jack Ma</a> (entonces aún a la cabeza de Alibaba), empezó a preguntarse por qué no habíamos encontrado aún alienígenas, ante la extrañeza general del auditorio. Pues bien, el discurso de Johnson ha generado <strong>niveles de extrañeza aún mayores</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Para muestra un botón... esto es lo que piensa el \'premier\' británico acerca de cómo será Alexa, el asistente digital de Amazon, en el futuro:</p>\n<!-- BREAK 3 -->\n<blockquote>\n<p>\"Una futura Alexa fingirá recibir órdenes. Pero esta Alexa te estará mirando, chasqueando la lengua y golpeando el suelo con el pie\".</p>\n</blockquote>\n<p>¿Extrañado? Aún no has visto nada.</p>\n\n<h2>Pollos sin extremidades y colchones lectores de pesadillas</h2>\n<p>Otra de las grandes frases del discurso mezcla referencias al circo romano y a la comedia televisiva \'Little Britain\' (ese \"el ordenador dice que no\"):</p>\n<!-- BREAK 4 -->\n<blockquote>\n<p>\"¿Deberían las máquinas, y sólo ellas, decidir si somos elegibles para optar a una hipoteca, un seguro o una cirugía?</p>\n<p>¿Estamos condenados a un futuro frío y sin corazón en el que el ordenador dice que sí (o el ordenador dice que no) con la sombría resolución de un emperador en la arena?\".</p>\n</blockquote>\n<p>No sabemos si es peor que Johnson no sepa <strong>de qué color tienen realmente los ojos los terminators</strong> de <a href=\"https://www.xataka.com/cine-y-tv/espectacular-trailer-terminator-destino-oscuro-nos-trae-vuelta-a-linda-hamilton-a-schwarzenegger\">la saga homónima</a>, o que se plantee como una hipótesis plausible la opción de que algún sucedáneo de Skynet vaya a cometer genocidio recurriendo a viajes en el tiempo:</p>\n<!-- BREAK 5 -->\n<blockquote>\n<p>\"¿En qué se traducirá la inteligencia artificial? ¿En útiles robots que cuiden y laven a una población envejecida, o en terminators de ojos rosados enviados desde el futuro a sacrificar la raza humana?\".</p>\n</blockquote>\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Terminator\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/9b13c2/2144882415_7a8d4999e7_z/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/9b13c2/2144882415_7a8d4999e7_z/450_1000.jpg 450w, https://i.blogs.es/9b13c2/2144882415_7a8d4999e7_z/650_1200.jpg 681w, https://i.blogs.es/9b13c2/2144882415_7a8d4999e7_z/1024_2000.jpg 1024w, https://i.blogs.es/9b13c2/2144882415_7a8d4999e7_z/1366_2000.jpg 1366w\"/><noscript><img alt=\"Terminator\" src=\"https://i.blogs.es/9b13c2/2144882415_7a8d4999e7_z/450_1000.jpg\"/></noscript> <span> \"Boris, mírame a los ojos\". (Imagen extraída de la saga \'Terminator\') </span> </div></div></div>\n<p>La siguiente frase tiene, al menos, sentido... e incluso podría revelar un Johnson profundamente concienciado con <a href=\"https://www.xataka.com/privacidad/he-mirado-todos-los-datos-que-google-tiene-sobre-mi-y-confirmo-que-es-el-gran-hermano-definitivo\">la amenaza a la privacidad representada por el uso del big data</a>:</p>\n<!-- BREAK 6 -->\n<blockquote>\n<p>\"Es posible esconder secretos a tus amigos, a tus padres, a tus hijos, a tu médico - incluso a tu entrenador personal - pero se necesita un verdadero esfuerzo para ocultar tus pensamientos de Google\".</p>\n</blockquote>\n<p>La mayoría de los medios entienden la siguiente frase como <strong>una referencia crítica a la carne cultivada en laboratorio</strong>, pero nadie pondría la mano en el fuego... máxime cuando también puede ser entendida como una referencia a la distopía satírica <a href=\"https://www.xataka.com/cine-y-tv/nueve-libros-margaret-atwood-que-muy-adaptables-a-television-exito-cuento-criada\">\'Oryx y Crake\', de Margaret Atwood</a> (la autora del \'Cuento de la criada\'):</p>\n<!-- BREAK 7 -->\n<blockquote>\n<p>\"¿Qué nos aportará la biología sintética? ¿La recuperación de nuestro hígado y nuestros ojos gracias a una milagrosa regeneración de tejidos? ¿O sólo traerá aterradores pollos sin extremidades a nuestras mesas?\".</p>\n</blockquote>\n\n<p>Después de eso, Johnson plantea una serie de pinceladas con sus... ¿previsiones para el futuro de de la domótica?:</p>\n<!-- BREAK 8 -->\n<blockquote>\n<p>\"En el futuro [...] tu colchón monitorizará tus pesadillas; tu refrigerador emitirá un pitido para avisarte de que necesitas más queso, tu puerta de entrada se abrirá en cuanto te acerques, como un silencioso mayordomo\".</p>\n</blockquote>\n<p>Entendemos lo que Johnson quiere dar a entender (o eso creemos), pero <strong>la ONU no parece el mejor marco para recurrir a un lenguaje tan exagerado y repleto de referencias a la cultura pop</strong>, menos aún en un momento en el que los peligros de esta tecnología constituyen un tema de debate serio por culpa de <a href=\"https://www.xataka.com/inteligencia-artificial/presidente-microsoft-considera-imparables-a-robots-asesinos-pide-frenarlos-nueva-convencion-ginebra\">la proliferación de armas equipadas con inteligencia artificial</a>.</p>\n<!-- BREAK 9 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/5z4K_G14_Ho\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Vía | <a href=\"https://futurism.com/insane-quotes-boris-johnson-un-speech\">Futurity</a></p>\n<p>Imagen | <a href=\"https://www.flickr.com/photos/chathamhouse/26147764939\">Chatham House</a></p>\n</div>', '2019-09-26 04:46:23', '2019-09-26 04:46:23', 24, 'portada010.jpg', 'Es una buena noticia que en la Asamblea de la ONU se pongan encima de la mesa las potenciales amenazas de la inteligencia artificial. Pero el último discurso al respecto de Boris Johnson en esa institución ha causado… extrañeza.'),
(32, 'El presidente de Microsoft considera \"imparables\" a los robots asesinos y pide frenarlos con una nueva Convención de Ginebra', '<div class=\"blob js-post-images-container\">\n<p>Desde el estreno de \'Terminator\', <strong>los robots asesinos han sido un tópico</strong> en los debates sobre los peligros potenciales de la inteligencia artificial. Pero en los últimos tiempos han dejado de ser un argumento propio de <a href=\"https://www.xataka.com/historia-tecnologica/la-verdadera-historia-de-los-luditas-no-era-tecnofobia-era-lucha-de-clases\">neoluditas</a> para convertirse en un tema de preocupación real para políticos y líderes de la industria tecnológica.</p>\n<!-- BREAK 1 -->\n<p>Durante una reciente entrevista para el Daily Telegraph, el mismísimo presidente de Microsoft, Brad Smith, afirmó que, a día de hoy, <strong>los robots asesinos son \"imparables\"</strong>, porque las grandes potencias militares (EE.UU., China, Rusia, Reino Unido, Israel, Corea del Sur, etc) ya han iniciado <a href=\"https://www.xataka.com/inteligencia-artificial/henry-kissinger-advierte-que-controlar-uso-armas-inteligentes-sera-complicado-que-nucleares\">una nueva carrera armamentística</a> en el campo de las armas dotadas de IA. </p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Y, <strong>como ya ocurrió con las armas nucleares</strong>, una carrera en la que muchos pretenden ser los primeros y mejores puede llevar a la asunción de excesivos riesgos por parte de los corredores. Por no mencionar que un escenario en el que se logre salvaguardar a las tropas humanas del campo de batalla hace que resulte \"más barato\" para los gobiernos declarar guerras.</p>\n<!-- BREAK 3 -->\n<p>Corea del Sur, por ejemplo, ya ha dispuesto sus robots <strong>SGR-1</strong>, desarrollados conjuntamente por Samsung Techwin y la Universidad de Corea, en el mismo borde de la Zona Desmilitarizada. Estos robots serían capaces de detectar a los soldados norcoreanos que crucen la frontera y, <strong>técnicamente, podrían disparar sin necesidad de intervención humana</strong>.</p>\n<!-- BREAK 4 -->\n\n<p>Al otro extremo de Asia, Israel Aerospace Industries ha creado un misil inteligente llamado <strong>Harpy</strong>, programado para merodear durante horas hasta detectar emisiones de un sistema de radar hostil.</p>\n<!-- BREAK 5 -->\n<p>Mientras, en los Estados Unidos se está desarrollando el programa <strong>Squad X</strong>, basado en el uso de robots con IA en entrenamientos conjuntos con los marines, en los que estas máquinas operan de manera autónoma a menos que se les dé órdenes.</p>\n<!-- BREAK 6 -->\n<h2>Hacen falta nuevas reglas para un nuevo mundo</h2>\n<p>Smith recordó que estamos ante tecnologías que están avanzando muy rápidamente, que en breve presenciaremos cómo los drones (voladores, nadadores y caminantes) empiezan a ser equipados con misiles u otras armas, capaces de operar de manera autónoma.</p>\n<!-- BREAK 7 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/7d87VFSQKjc\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Muchos tecnólogos están empezando a alzar la voz pidiendo a sus gobiernos que <strong>ninguna IA pueda tomar decisiones de combate de forma totalmente autónoma</strong>, sin depender en ningún punto de la aprobación de un humano, porque un \"error de juicio\" de un robot inteligente no sólo es igual de probable que el de un humano, sino que <a href=\"https://www.xataka.com/inteligencia-artificial/china-avisa-peligro-guerra-accidental-culpa-armas-inteligentes-real-creciente\">sus consecuencias pueden ser mucho peores</a>.</p>\n<!-- BREAK 8 -->\n<p>Y por ello, para evitar en la medida de lo posible estos peligros, Smith juzga necesaria <strong>una nueva Convención de Ginebra adaptada al actual mundo tecnológico</strong>, que nos dote de \"normas que protejan tanto a civiles como a soldados\". </p>\n<!-- BREAK 9 -->\n<p>Hoy en día existen ya cuatro de estas convenciones, todas ellas firmadas en la ciudad suiza homónima, con las que desde 1864 se ha ido edificando <strong>un consenso internacional de mínimos en cuanto a los límites éticos de la guerra</strong>:</p>\n<!-- BREAK 10 -->\n<ol>\n<li>\"Primera Convención de Ginebra, para el mejoramiento de la suerte que corren los militares heridos en los ejércitos en campaña\" (1864).</li>\n<li>\"Segunda Convención de Ginebra, para el mejoramiento de la suerte de los militares heridos, enfermos o náufragos en las fuerzas armadas en el mar\" (1906).</li>\n<li>\"Tercera Convención de Ginebra, para mejorar la suerte de los heridos y enfermos de los ejércitos en campaña y el trato de los prisioneros de guerra\" (1929).</li>\n<li>\"Cuarta Convención de Ginebra, relativa a la Protección de Personas Civiles en Tiempo de Guerra\" (1949).</li>\n</ol>\n\n<p>Según Smith, ahora que se cumplen 70 años desde la aprobación de la última de estas convenciones, ha llegado el momento de poner de acuerdo a las grandes potencias mundiales en lo que respecta a las normas aceptables a la hora de aplicar la inteligencia artificial a la guerra.</p>\n<!-- BREAK 11 -->\n<p>En el libro que acaba de publicar este mismo mes, llamado \"Tools and Weapons\" (Herramientas y armas), Smith también defiende que la Humanidad se dote de <strong>reglas más estrictas sobre el uso de la tecnología de reconocimiento facial \"para protegernos contra posibles abusos</strong>\".</p>\n<!-- BREAK 12 -->\n<p>Cabe destacar que, el pasado mes de agosto, un informe de la ONG holandesa PAX señalaba a las principales compañías tecnológicas de estar poniendo el mundo en peligro con su colaboración en el desarrollo de IAs asesinas, y <strong>situaba a Microsoft y Amazon como las compañías de mayor riesgo</strong>.</p>\n<!-- BREAK 13 -->\n<p>Vía | <a href=\"https://www.telegraph.co.uk/technology/2019/09/21/microsoft-chief-brad-smith-says-rise-killer-robots-unstoppable/\">Telegraph</a> &amp; <a href=\"https://www.vice.com/en_in/article/zmpa59/ai-controlled-robots-are-training-alongside-us-marines\">VICE</a></p>\n<!-- BREAK 14 -->\n<p>Imagen | <a href=\"https://www.needpix.com/photo/1416349/ghibli-robot-laputa-castleinthesky-tokyo-japan-miyazaki\">John Breslin</a></p>\n<!-- BREAK 15 --> </div>', '2019-09-25 10:01:01', '2019-09-25 10:01:01', 37, 'portada011.jpg', 'El presidente de Microsoft, Brad Smith, defiende que la normativa que refleja el consenso internacional de mínimos en cuanto a los límites éticos de la guerra se actualice para incorporar los usos de la IA en la misma.'),
(33, 'El regulador de Internet chino obligará a que los algoritmos de recomendación promuevan los \"valores sociales predominantes\"', '<div class=\"blob js-post-images-container\">\n<p>La Agencia del Ciberespacio de China, el principal organismo regulador de Internet del gigante asiático, quiere que <strong>los algoritmos de recomendación ayuden a promover los \"valores sociales mayoritarios\"</strong> dirigiendo a los usuarios de webs, redes sociales, juegos y apps hacia material compatible dichos valores.</p>\n<!-- BREAK 1 -->\n<p>Este objetivo <strong>se recoge en el borrador de una normativa \"de gestión del ecosistema digital\"</strong> que la agencia ha dado a conocer recientemente con el fin de recoger propuestas a lo largo del próximo mes; el procedimiento legal probablemente permita que estas regulaciones entren en vigor de aquí a un año.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Desde que Xi Jinping llegó al poder, el Partido Comunista Chino intensificó sus esfuerzos para <a href=\"https://www.xataka.com/inteligencia-artificial/bots-censores-internet-china-trabajan-a-toda-maquina-visperas-30-aniversario-sucesos-tiananmen\">\"limpiar\" la Red de contenido \"perjudicial\"</a>, llegando a cerrar cuentas y servidores, y a imponer fuertes multas a los operadores. <strong>Llevar ahora dicha misión hasta los mismos algoritmos</strong> basados en inteligencia artificial no hace sino confirmar que el Partido Comunista está decidido a <strong>reforzar su capacidad de influencia ideológica</strong>.</p>\n<!-- BREAK 3 -->\n<p>Pero, ¿<strong>a qué se refiere éste cuando habla de esos \"valores sociales predominantes\"</strong> que deberán privilegiar los algoritmos de recomendación?</p>\n<!-- BREAK 4 -->\n\n<h2>La IA no puede ser \'neutral\' ante los valores de la cultura china</h2>\n<p>Pues, según el South China Morning Post, eso incluye tanto \"las políticas del Partido\" como \"los valores esenciales del socialismo\", enfocados en <strong>mostrar y promover \"el desarrollo socioeconómico y la estabilidad del país\"</strong>, así como la propia <a href=\"https://magnet.xataka.com/preguntas-no-tan-frecuentes/como-la-vuelta-del-confucianismo-esta-ayudando-a-china-con-el-envejecimiento-de-la-poblacion\">cultura china</a>.</p>\n<!-- BREAK 5 -->\n<p>Así, <strong>cualquier material que socave la seguridad y los intereses de la nación o promueva \"estilos de vida extravagantes\"</strong> (una categoría que incluye cosas como el exceso de tatuajes), los contenidos \"sexualmente sugestivos\", los \"alardes de riqueza\" y \"los chismes y escándalos\" entrarían directamente en la <strong>lista de contenidos prohibidos</strong>.</p>\n<!-- BREAK 6 -->\n<p>Como tantas otras cosas que ocurren en China, este proyecto es la consecuencia de un \'clima de opinión\' previamente iniciado desde medios y organismos públicos: el año pasado, el periódico oficial del Partido Comunista Chino publicó un artículo criticando <strong>el fracaso de las plataformas de Internet a la hora de evitar la difusión de \"información falsa y contenido vulgar\"</strong>.</p>\n<!-- BREAK 7 -->\n\n<p>Esto puede sorprender cuando se sabe que, en algunos casos, <strong>la mitad de la plantilla de estas empresas puede estar dedicada a labores relacionadas con la censura</strong> (Inke, una de las principales plataformas de streaming de vídeo de China, tiene 1200 personas dedicadas a la moderación de contenidos, lo que representa el 60% de su plantilla).</p>\n<!-- BREAK 8 -->\n<p>Pero el artículo cargaba de forma específica contra los algoritmos de recomendación, que estarían <strong>demasiado centrados en tener en cuenta los hábitos e intereses de los usuarios individuales</strong> en lugar de en promover contenido de calidad:</p>\n<!-- BREAK 9 -->\n<blockquote>\n<p>\"Cada línea de código, cada interfaz de usuario, representa una elección, indica una apuesta por determinados valores.</p>\n<p>La tecnología no puede ser una excusa para evadir la responsabilidad mientras clama ser neutral, sino que debe convertirse en una puerta de entrada al pensamiento inspirador, promover la sabiduría y difundir los valores predominantes\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://www.scmp.com/news/china/politics/article/3026784/chinas-internet-regulator-orders-online-ai-algorithms-promote\">South China Morning Post</a></p>\n<!-- BREAK 10 -->\n<p>Imagen | Partido Comunista Chino</p>\n</div>', '2019-09-23 12:08:10', '2019-09-23 12:08:10', 25, 'portada012.jpg', 'Las autoridades chinas quieren que los sistemas de recomendación de contenidos en webs y apps tengan menos en cuenta los gustos individuales de los usuarios, en favor de los valores de la cultura china y las políticas del Partido.'),
(34, 'Los seis mejores cursos online para formarte en inteligencia artificial', '<div class=\"blob js-post-images-container\">\n<p>En un futuro cercano, <strong>la inteligencia artificial será (está siendo) responsable de crear numerosos puestos de trabajo nuevos</strong>, que necesitarán <a href=\"https://iahuawei.xataka.com/como-puedo-aprovechar-hoy-oportunidades-inteligencia-artificial/\">perfiles con formación específica</a> en esta nueva tecnología. Incluso los empleos pre-existentes se verán transformados por ella, de tal modo que contar con conocimientos al respecto será siempre un \'plus\' a la hora de conseguir un trabajo.</p>\n<!-- BREAK 1 -->\n<p>Por eso, es importante aprovechar las <strong>múltiples opciones con que contamos para formarnos sólidamente en todo lo relacionado con la inteligencia artificial</strong>, aprovechando los recursos gratuitos (o, como mínimo, muy asequibles) disponibles en Internet. Nosotros hemos elegido 6 propuestas para que las eches un vistazo:</p>\n<!-- BREAK 2 -->\n<!--more-->\n<h2><a href=\"https://www.elementsofai.com/\">Elements of AI</a></h2>\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Elements\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/61249e/elements/450_1000.png\" data-sf-srcset=\"https://i.blogs.es/61249e/elements/450_1000.png 450w, https://i.blogs.es/61249e/elements/650_1200.png 681w, https://i.blogs.es/61249e/elements/1024_2000.png 1024w, https://i.blogs.es/61249e/elements/1366_2000.png 1366w\"/><noscript><img alt=\"Elements\" src=\"https://i.blogs.es/61249e/elements/450_1000.png\"/></noscript> <span> ElementsOfAI.com </span> </div></div></div>\n<p>Este curso constituye la <a href=\"https://www.xataka.com/robotica-e-ia/finlandia-quiere-que-inteligencia-artificial-ocupe-lugar-nokia-su-apuesta-formar-al-1-poblacion\">punta de lanza del \"Desafío IA\"</a> puesto en marcha por instituciones, universidades y empresas de Finlandia con el objetivo de <strong>reconvertir la economía del país con la vista puesta en la inteligencia artificial</strong>.</p>\n<!-- BREAK 3 -->\n<p>El impulsor original del proyecto, iniciado en 2018, es el propio creador del curso: Teemu Roos, profesor del Departamento de Informática de la Universidad de Helsinki. Aunque originalmente pensado para el público finés, este curso compuesto de 6 lecciones y 25 ejercicios <strong>ya ha llegado a 200.000 estudiantes de 110 países distintos</strong>.</p>\n<!-- BREAK 4 -->\n<p>Es, sin duda, <strong>el curso más recomendable de esta lista para gente sin formación previa en el campo de la IA</strong>, centrándose en enseñarnos qué es posible hacer (y qué no) gracias a la inteligencia artificial, y en cómo afecta eso a nuestras vidas. Todo eso sin necesidad de niveles avanzados de matemática ni de tocar aspectos relacionados con la programación.</p>\n<!-- BREAK 5 -->\n<p>Aunque si eso te sabe a poco, <strong>antes de que finalice el año estará online la segunda parte del curso</strong> (\"Building AI\"), que profundizará más en los contenidos ya disponibles, y para la que sí será recomendable disponer de conocimientos complementarios, como nociones básicas del lenguaje de programación Python.</p>\n<!-- BREAK 6 -->\n<ul>\n<li><strong>Certificado:</strong> Sí (sin validez oficial fuera de Finlandia)</li>\n<li><strong>Precio:</strong> Gratis</li>\n<li><strong>En español:</strong> No (disponible en inglés, finés y sueco)</li>\n</ul>\n\n<h2><a href=\"https://developers.google.com/machine-learning/crash-course/\">Curso intensivo de aprendizaje automático</a></h2>\n<p>Este curso diseñado y ofrecido por Google se define como una \"introducción práctica y rápida al aprendizaje automático\". Planteado como <strong>una herramienta de estudio autónomo</strong> (nadie supervisa ni puntúa nuestra formación), este curso de 15 horas se compone de 25 lecciones complementadas mediante <strong>vídeos con disertaciones de investigadores de Google</strong> y ofrece tanto casos de éxito reales como ejercicios de práctica (más de 40), para los que necesitaremos el <a href=\"https://www.xataka.com/inteligencia-artificial/tensorflow-software-google-lider-machine-learning-presenta-su-nueva-version-2-0-alpha-nuevo-modulo-privacidad\">software TensorFlow</a>. Pretende dar respuesta a preguntas como</p>\n<!-- BREAK 7 -->\n<ul>\n<li>¿En qué se diferencia el aprendizaje automático de la programación tradicional?</li>\n<li>¿Cómo debo representar mis datos para que un programa pueda aprender de ellos?</li>\n<li>¿Cómo creo una red neuronal profunda?</li>\n</ul>\n<p>Este curso sólo es uno de los 13 que Google ofrece en su plataforma de recursos educativos sobre inteligencia artificial \"A<a href=\"https://ai.google/education/\">prende con Google AI</a>\".</p>\n<!-- BREAK 8 -->\n<ul>\n<li><strong>Certificado:</strong> No</li>\n<li><strong>Precio:</strong> Gratis</li>\n<li><strong>Duración:</strong> 15 horas</li>\n<li><strong>En español:</strong> Sí</li>\n</ul>\n<h2><a href=\"https://www.coursera.org/learn/machine-learning?\">Machine Learning (Stanford)</a></h2>\n<p><strong>Posiblemente, el curso de machine learning más reputado hoy en día</strong>. Creado por el popular investigador Andrew Ng, el profesor de la Universidad de Stanford responsable de la creación de proyectos como Google Brain o el Helicóptero Autónomo de Stanford.</p>\n<!-- BREAK 9 -->\n<p>El éxito de este curso la primera vez que se ofreció online en 2011 <strong>animó a Ng a convertirse en cofundador de la plataforma Coursera</strong> (donde se aloja ahora). Ya han pasado casi 2 millones de estudiantes de todo el mundo, a los que se les ha ofrecido una introducción a los conceptos básicos de esta temática: aprendizaje supervisado, aprendizaje no supervisado, minería de datos, reconocimiento de patrones estadísticos, etc.</p>\n<!-- BREAK 10 -->\n<blockquote>\n<p>\"El curso también abordará numerosos estudios de caso y aplicaciones prácticas, de modo que también aprenderás a aplicar algoritmos de aprendizaje en campos como la construcción de robots inteligentes (percepción, control), la comprensión de texto (búsqueda en la Web, anti-spam), la visión artificial, la informática médica, extracción de bases de datos, etc\".</p>\n</blockquote>\n<ul>\n<li><strong>Certificado:</strong> Sí (sin validez oficial)</li>\n<li><strong>Precio:</strong> 71 € con certificado / Gratis sin certificado</li>\n<li><strong>Duración:</strong> 56 horas</li>\n<li><strong>En español:</strong> Sólo subtítulos</li>\n</ul>\n\n<h2><a href=\"https://www.coursera.org/learn/ai-for-everyone\">AI for Everyone</a></h2>\n<p>Otro curso diseñado por Andrew Ng, aunque en este caso <strong>con un enfoque accesible similar al de \'Elements of AI\'</strong>, \'AI for Everyone\' es un curso breve estructurado en 4 bloques (¿Qué es la IA?, Creación de proyectos de IA, IA en la empresa, IA en la sociedad), pensados para cursar uno por semana durante un mes.</p>\n<!-- BREAK 11 -->\n<blockquote>\n<p>\"Si eres un profesional sin conocimientos técnicos, \'AI for Everyone\' te ayudará a entender cómo crear una estrategia de IA sostenible para su negocio. Si eres un ingeniero de machine learning o un científico de datos, este es el curso que deberías pedir a tu jefe que curse si quieres que entienda lo que puedes (y no puedes) hacer\".</p>\n</blockquote>\n<ul>\n<li><strong>Certificado:</strong> Sí (sin validez oficial)</li>\n<li><strong>Precio:</strong> 49 dólares con certificado (gratuito sin él)</li>\n<li><strong>Duración:</strong> 10 horas</li>\n<li><strong>En español:</strong> No (en inglés)</li>\n</ul>\n\n<h2><a href=\"http://course18.fast.ai/ml\">Introduction to Machine Learning for Coders</a></h2>\n<p>Se trata de la versión online del curso impartido por Jeremy Howard en la Universidad de San Francisco, que trata de introducir en el <a href=\"https://www.xataka.com/robotica-e-ia/machine-learning-y-deep-learning-como-entender-las-claves-del-presente-y-futuro-de-la-inteligencia-artificial\">machine learning y el deep learning</a> a <strong>gente con conocimientos previos de programación (concretamente de Python)</strong>, adoptando un enfoque muy práctico. </p>\n<!-- BREAK 12 -->\n<p>¿Te suenan los árboles de decisión o las redes neuronales multicapa? ¿Sabes qué son los datasets o los datos de validación? ¿Quieres aprender a crear una red neuronal desde cero usando PyTorch? Pues <strong>ése el el tipo de cosas que puedes aprender con este curso</strong>.</p>\n<!-- BREAK 13 -->\n<p>Howard fue el ganador del torneo de ciencia de datos de Kaggle en 2010 y 2011, creó el servicio de correo electrónico FastMail (que luego vendió a Opera) y más tarde <strong>fundó el instituto de investigación sobre inteligencia artificial Fast.AI</strong>, en cuya plataforma se alojan éste y otros cursos relacionados. Además, es el docente más joven de la Singularity University.</p>\n<!-- BREAK 14 -->\n<ul>\n<li><strong>Certificado:</strong> No</li>\n<li><strong>Precio:</strong> Gratis</li>\n<li><strong>En español:</strong> No (en inglés)</li>\n</ul>\n<h2><a href=\"https://courses.nvidia.com/courses/course-v1:DLI+C-FX-01+V2/about\">Fundamentals of Deep Learning for Computer Vision</a></h2>\n<p>NVIDIA se ha posicionado en los últimos años como una referencia en la industria de la inteligencia artificial, y no sólo por sus GPUs: su labor investigadora es notable, y también ha hecho sus pinitos en el campo educativo. Un ejemplo de esto último es <strong>el curso que nos ocupa, centrado en la aplicación del aprendizaje profundo al campo de la imagen y el vídeo</strong>, más concretamente a la clasificación de imágenes y detección de objetos.</p>\n<!-- BREAK 15 -->\n<p>Aunque es posible cursar esta formación de modo 100% online, aquellos pocos interesados que vivan en las inmediaciones de Silicon Valley pueden solicitar que se imparta en las oficinas de su propia empresa, o bien <strong>acudir a las de la propia NVIDIA</strong>.</p>\n<!-- BREAK 16 -->\n<p>Este curso, el único sin opción gratuita de la lista, sólo es una formación introductoria dentro de un catálogo más amplio alojado en una plataforma empresarial, en este caso el <a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\">NVIDIA Deep Learning Institute</a>.</p>\n<!-- BREAK 17 -->\n<ul>\n<li><strong>Certificado:</strong> No</li>\n<li><strong>Precio:</strong> 90 dólares</li>\n<li><strong>Duración:</strong> 8 horas</li>\n<li><strong>En español:</strong> No (en inglés)</li>\n</ul>\n</div>', '2019-09-22 16:08:11', '2019-09-22 16:08:11', 43, 'portada013.jpg', 'En Internet existen múltiples opciones (la mayoría gratuitas) para formarnos en todo lo relativo a la IA y el machine learning. Te ofrecemos aquí una pequeña selección de cursos disponibles.');
INSERT INTO `posts` (`id`, `titulo`, `cuerpo`, `created_at`, `updated_at`, `user_id`, `imagen`, `descripcion`) VALUES
(35, 'Esto es lo que sabemos del Huawei Assistant, la alternativa china al Asistente de Google', '<div class=\"blob js-post-images-container\">\n<p>Huawei presentó ayer en Munich sus nuevos smartphones <a href=\"https://www.xataka.com/analisis/huawei-mate-30-mate-30-pro-opiniones-toma-contacto-video-fotos\">Huawei Mate 30 (y 30 Pro)</a> que, a raíz del conflicto entre EE.UU. y China, llegan al mercado con una versión \'libre\' de Android: es decir, la versión conocida como AOSP que se caracteriza por <a href=\"https://www.xataka.com/aplicaciones/que-se-diferencia-android-huawei-mate-30-servicios-google-android-normal-que-ofrecen-otros-telefonos\">carecer de las aplicaciones de Google preinstaladas</a>. Y <strong>eso incluye, claro, al Asistente de Google</strong>.</p>\n<!-- BREAK 1 -->\n<p>Pero los usuarios de los Mate 30 tendrán acceso desde el primer momento a <strong>su propio Huawei Assistant</strong>, una de las apps integradas en los Huawei Mobile Services que sustituyen a las aplicaciones de Google, y que en este caso <strong>suple las funciones no sólo del asistente, sino también de la aplicación de Google y de Discover</strong>. Así, ofrecerá recomendaciones, noticias y notificaciones en tiempo real, organizadas en \'smart cards\'.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<h2>Un asistente que no encontrarás en la Google Play Store</h2>\n<p>Su desventaja frente a Google Assistant o Siri radica en que <strong>no funciona a través de comandos de voz</strong>, de modo que tendremos que deslizar el dedo por la pantalla -1 para activarlo y usar sus cuatro principales funcionalidades.</p>\n<!-- BREAK 3 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <img alt=\"Funciones del Huawei Assistant\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/0899d9/assistant/450_1000.png\" data-sf-srcset=\"https://i.blogs.es/0899d9/assistant/450_1000.png 450w, https://i.blogs.es/0899d9/assistant/650_1200.png 681w, https://i.blogs.es/0899d9/assistant/1024_2000.png 1024w, https://i.blogs.es/0899d9/assistant/1366_2000.png 1366w\"/><noscript><img alt=\"Funciones del Huawei Assistant\" src=\"https://i.blogs.es/0899d9/assistant/450_1000.png\"/></noscript> </div></div>\n<ul>\n<li><strong>Búsqueda:</strong> Además de ser el sistema de búsqueda web por defecto, permitirá buscar información dentro del propio smartphone, como las aplicaciones instaladas, las notas, los correos electrónicos y las entradas del calendario.</li>\n<li><strong>Instant Access:</strong> Esta funcionalidad es una herramienta de accesos directos que nos permite, por ejemplo, acceder a nuestras fotos favoritas o a las listas de reproducción del dispositivo. Huawei promete que la experiencia de usuario mejorará en el futuro, dando acceso a una gama más amplia de funciones.</li>\n<li><strong>SmartCare:</strong> Fundamentalmente, una agenda dotada de inteligencia artificial, que nos irá ofreciendo notificaciones y recordatorios en base al análisis de nuestro uso del smartphone. Nos permitirá gestionar, además, cinco tipos de servicios: viajes, comida, deportes, eventos y servicios en la nube. Eso sí, algunas de las funciones relacionadas con la gestión de reservas se irán añadiendo en actualizaciones futuras.</li>\n<li><strong>Newsfeed:</strong> La función equivalente al Discover, que nos ofrecerá una selección de las noticias más relevantes del momento del momento.</li>\n</ul>\n<p>Según la compañía, Huawei vendrá preinstalado en los nuevos Mate 30 y, en un futuro cercano, <strong>estará disponible para ser instalado en el resto de dispositivos móviles Huawei</strong> a través de su tienda de aplicaciones, AppGallery. Además, el fabricante ha dejado claro que este servicio basado en la nube alojará los datos de los usuarios en suelo europeo, para cumplir así con la normativa de protección de datos de la UE.</p>\n<!-- BREAK 4 --> </div>', '2019-09-21 02:01:36', '2019-09-21 02:01:36', 18, 'portada014.jpg', 'Obligado a prescindir de las apps de Google en sus móviles Android, Huawei ha desarrollado su propio asistente con inteligencia artificial para sus nuevos modelos.'),
(36, 'La IA de Facebook usará vídeos de cámaras corporales policiales para aprender a detectar streamings de tiroteos', '<div class=\"blob js-post-images-container\">\n<p>El pasado mes de marzo, varios atacantes <strong>retransmitieron en directo, a través de Facebook, un asalto con armas de fuego</strong> a la mezquita de Christchurch (Nueva Zelanda), que se saldó con 50 muertos y un número similar de heridos.</p>\n<!-- BREAK 1 -->\n<p>200 personas presenciaron esa retransmisión en directo (otras 4.000 lo hicieron una vez finalizada), y hubo que esperar media hora <strong>hasta que una de ellas se animó a reportar</strong> lo que estaba viendo.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p><strong>Facebook cuenta con moderación de contenidos tanto humana como automatizada</strong>, pero en aquella ocasión eso no bastó para evitar la difusión de las imágenes de la matanza. Ayer, en un comunicado, la compañía señalaba el motivo de ello:</p>\n<!-- BREAK 3 -->\n<blockquote>\n<p>\"El video del ataque en Christchurch no activó nuestros sistemas automatizados de detección debido a que <strong>no disponemos de suficiente contenido de esa clase</strong> (filmaciones en primera persona de eventos violentos) como para entrenar con eficacia nuestra tecnología de <a href=\"https://www.xataka.com/robotica-e-ia/machine-learning-y-deep-learning-como-entender-las-claves-del-presente-y-futuro-de-la-inteligencia-artificial\">aprendizaje automático</a>\".</p>\n</blockquote>\n\n<h2>Cámaras a cambio de vídeos de entrenamiento</h2>\n<p>De modo que la compañía de Marck Zuckerberg ha decidido asociarse con las fuerzas del orden para que éstas le provean de <strong>material de archivo de sus ejercicios de entrenamiento con armas de fuego</strong>, extraído de sus cámaras corporales.</p>\n<!-- BREAK 4 -->\n<p>En este caso, el proveedor será el Servicio de Policía Metropolitana del Reino Unido (aunque están explorando acuerdos similares en EE.UU.), que <strong>incluirá tomas de sus simulaciones de incidentes terroristas y de toma de rehenes</strong>. Facebook no pagará directamente por el acceso a este material pero, según el Financial Times, compensará a la Policía Metropolitana proporcionando al cuerpo cámaras corporales de manera gratuita.</p>\n<!-- BREAK 5 -->\n<p>¿El objetivo? Lograr que, ahora sí, su sistema de IA sea capaz de identificar automáticamente imágenes de un ataque, impidiendo el acceso a la retransmisión y notificándoselo a la policía. Al mismo tiempo, esperan <strong>reducir el número de falsos positivos detectados entre vídeos extraídos de videojuegos o películas</strong> de acción.</p>\n<!-- BREAK 6 -->\n<p>Neil Basu, el máximo responsable de contraterrorismo del Reino Unido, explica que gracias a este \"proyecto innovador\",</p>\n<!-- BREAK 7 -->\n<blockquote>\n<p>\"La tecnología que Facebook está tratando de crear podría ayudar a identificar los ataques con armas de fuego es sus primeras etapas y, potencialmente, ayudar a que las policías de todo el mundo respondan antes a este tipo de incidentes\".</p>\n<p>\"La tecnología que detiene automáticamente la retransmisión en directo de este tipo de ataques también ayudará a prevenir significativamente la exaltación de tales actos, así como la promoción de las ideologías tóxicas que los impulsan\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://www.theverge.com/2019/9/18/20872350/facebook-law-enforcement-bodycam-footage-training-algorithms-christchurch\">The Verge</a> &amp; <a href=\"https://artificialintelligence-news.com/2019/09/18/facebook-ai-shootings-videos-uk-police-firearms-training/\">AI-NEWS</a></p>\n<!-- BREAK 8 --> </div>', '2019-09-20 11:31:31', '2019-09-20 11:31:31', 41, 'portada015.jpg', 'Hace unos meses, Facebook no fue capaz de detectar una emisión en directo de un tiroteo a través de su plataforma. ¿El motivo? Que su IA no disponía de material suficiente de ese tipo como para aprender a detectarlo. Hasta ahora.'),
(37, 'La web de ImageNet Roulette te permite conocer qué ve la inteligencia artificial cuando le muestras tu foto', '<div class=\"blob js-post-images-container\">\n<p><a href=\"https://imagenet-roulette.paglen.com/\">IMAGEnet Roulette</a> es una aplicación web que permite a los usuarios subir un \'selfie\' para que la inteligencia artificial de la herramienta lo analice y nos muestre etiquetas descriptivas de la imagen. Es decir, que <strong>nos describa qué \"ve\" la IA cuando mira nuestra foto</strong>.</p>\n<!-- BREAK 1 -->\n<p>No se diferencia mucho del efecto logrado durante la última caída parcial de Facebook que, ante la imposibilidad de cargar las imágenes, <a href=\"https://www.xataka.com/inteligencia-artificial/ultima-caida-facebook-nos-permitio-ver-como-lee-nuestras-fotos-ia-facebook\">empezó a mostrar a los usuarios de la red social etiquetas</a> que <strong>intentaban describir (con mayor o menor precisión) el contenido de las mismas</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Así, cuando subimos una foto (o nos la hacemos sobre la marcha usando una webcam, o enlazamos una imagen ya subida a Internet), IMAGEnet Roulette se toma unos segundos para estudiarla, y a continuación <strong>proporciona una jerarquía de etiquetas</strong>, desde las más amplias \"alguien, persona, individuo, mortal\" a otras progresivamente más concretas: desde \"capitalista\" hasta \"agente de seguros\", pasando por \"hombre de negocios\".</p>\n<!-- BREAK 3 -->\n<p>Ése ha sido el caso, por ejemplo, de la foto que hemos subido de Pedro Sánchez:</p>\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <img alt=\"Sanchez\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/6d5c29/sanchez/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/6d5c29/sanchez/450_1000.jpg 450w, https://i.blogs.es/6d5c29/sanchez/650_1200.jpg 681w, https://i.blogs.es/6d5c29/sanchez/1024_2000.jpg 1024w, https://i.blogs.es/6d5c29/sanchez/1366_2000.jpg 1366w\"/><noscript><img alt=\"Sanchez\" src=\"https://i.blogs.es/6d5c29/sanchez/450_1000.jpg\"/></noscript> </div></div>\n<p>La segunda foto que hemos subido, correspondiente a la cantante británica Anne-Maria, por contra, ha sido <strong>reconocida de forma algo más precisa</strong> como \"rock star\":</p>\n<!-- BREAK 4 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <img alt=\"Anne Marie\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/4fa497/anne_marie/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/4fa497/anne_marie/450_1000.jpg 450w, https://i.blogs.es/4fa497/anne_marie/650_1200.jpg 681w, https://i.blogs.es/4fa497/anne_marie/1024_2000.jpg 1024w, https://i.blogs.es/4fa497/anne_marie/1366_2000.jpg 1366w\"/><noscript><img alt=\"Anne Marie\" src=\"https://i.blogs.es/4fa497/anne_marie/450_1000.jpg\"/></noscript> </div></div>\n<p>Dejando al margen su nivel de acierto, hemos podido comprobar que unas veces las etiquetas son -como en en los casos anteriores- más o menos concretas, mientras que otras <strong>el sistema se queda en niveles más abstractos</strong> (\"pesimista\", \"transpirador\"...). Pero, ¿de dónde salen todas esas etiquetas?</p>\n<!-- BREAK 5 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"@gloupin\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/fcf5c3/eeqlj-ex4aa083g/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/fcf5c3/eeqlj-ex4aa083g/450_1000.jpg 450w, https://i.blogs.es/fcf5c3/eeqlj-ex4aa083g/650_1200.jpg 681w, https://i.blogs.es/fcf5c3/eeqlj-ex4aa083g/1024_2000.jpg 1024w, https://i.blogs.es/fcf5c3/eeqlj-ex4aa083g/1366_2000.jpg 1366w\"/><noscript><img alt=\"@gloupin\" src=\"https://i.blogs.es/fcf5c3/eeqlj-ex4aa083g/450_1000.jpg\"/></noscript> <span> Este usuario se ha visto etiquetado como \"hermana\" (en referencia al título de las monjas católicas) | Vía <a href=\"https://twitter.com/gloupin/status/1173904110724235265\">@gloupin</a> </span> </div></div></div>\n<h2>Las eternas polémicas en torno a sesgos de la IA (o, mejor dicho, de sus datos)</h2>\n<p>La herramienta, desarrollada por el artista Trevor Paglen y la investigadora Kate Crawford con ocasión de una exposición en la Fondazione Prada de Milán, <strong>recibe su nombre del dataset IMAGEnet, que desde 2009 alberga más de 14 millones de imágenes</strong> etiquetadas y disponibles gratuitamente para cualquier investigador, con 2833 categorías referidas a fotos de personas.</p>\n<!-- BREAK 6 -->\n<p>Y todas ellas se han usado para <strong>entrenar la red neuronal</strong> (usando el framework Caffe) responsable de generar las descripciones que vemos cuando usamos ImageNet Roulette.</p>\n<!-- BREAK 7 -->\n\n<p>Como casi siempre que se aborda el lanzamiento de un software de visión artificial aplicada a humanos y, peor aún, que trabaja con datos previamente etiquetados por humanos, <strong>se ha generado en las redes un debate en torno a los supuestos sesgos raciales</strong> de la IA.</p>\n<!-- BREAK 8 -->\n<p>De hecho, la razón por la que Paglen y Crawford han incluido este software en su exposición se debe al deseo de mostrar lo que ocurre cuando <a href=\"https://www.xataka.com/robotica-e-ia/hay-quien-critica-a-mucha-inteligencia-artificial-como-racista-etnocentrica-problema-esta-datos\">se expone una IA, por bien programada que esté, a datos de mala calidad</a>. De hecho, ellos van más lejos:</p>\n<!-- BREAK 9 -->\n<blockquote>\n<p>\"IMAGEnet contiene una serie de categorías problemáticas, ofensivas y extrañas. [...] Algunos utilizan terminología misógina o racista\".</p>\n</blockquote>\n<p>Denuncian que eso es lo que ocurre cuando se recurre a \"bases de datos que recogen términos usados en Pinceton en los años 80\". y que, si bien el dataset IMAGEnet es un gran logro para la IA, <strong>su sesgo es un problema \"difícil de solucionar\"</strong>.</p>\n<!-- BREAK 10 -->\n\n<p>Sin embargo, <strong>hay que analizar con cuidado qué cabe entender como sesgo</strong>. Una cosa es el conocido caso en que una pareja afroamericana denunció que Google Imágenes les había etiquetado como \"gorilas\" y otra que un periodista del New Statesman denuncie que uno de sus selfies fue clasificado por IMAGEnet Roulette con el \"término racista\" de \"negroide\".</p>\n<!-- BREAK 11 -->\n<p>Tal término, aunque seguramente no sea el que ninguno elegiríamos para referirnos a una persona de raza negra, es relativamente habitual en <a href=\"https://magnet.xataka.com/preguntas-no-tan-frecuentes/pudo-haber-rey-mago-blanco-otro-negro-vinieron-oriente-logica-genetica-detras-mito\">ámbitos antropológicos</a> (igual que se aplica el de \"caucasoide\" a las personas de raza blanca... o \"mongoloide\" a las de raza oriental).</p>\n<!-- BREAK 12 -->\n<p>Más problemático puede resultar que algunos hombres, sin ningún motivo aparente, <strong>se vean etiquetados como \"sospechoso de violación\" o \"delincuente sin antecedentes\"</strong>, según han denunciado algunos usuarios en las redes sociales:</p>\n<!-- BREAK 13 -->\n<p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">????\"ImageNet Roulette is a provocation designed to help us see into the ways that humans are classified in machine learning systems.\" <a href=\"https://t.co/gOQIAPpTXD\">https://t.co/gOQIAPpTXD</a> <a href=\"https://t.co/6usmHtValH\">pic.twitter.com/6usmHtValH</a></p>— Dan Ehrenfeld (@DanEhrenfeld) <a href=\"https://twitter.com/DanEhrenfeld/status/1173681896867467266?ref_src=twsrc%5Etfw\">September 16, 2019</a></blockquote> <script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script></p>\n<!-- BREAK 14 -->\n<p>Vía |</p>\n</div>', '2019-09-19 02:31:11', '2019-09-19 02:31:11', 31, 'portada016.jpg', '¿Cómo te clasificará una inteligencia artificial que haya sido entrenada con la mayor base de imágenes del mundo? Pues ni será infalible... ni estará libre de polémica.'),
(38, 'Científicos noruegos proponen recurrir a los deepfakes para salvaguardar el anonimato de las personas manteniendo su expresividad', '<div class=\"blob js-post-images-container\">\n<p>Cuando se pretende anonimizar una fotografía o un vídeo, <strong>el recurso habitual es distorsionar o pixelar el rostro de la persona que lo protagoniza</strong>. Este recurso es típico, por ejemplo, en programas televisivos de investigación que recogen testimonios de personas que no desean revelar su identidad, pero lo más frecuente es que lo veamos aplicado cuando se captan imágenes de menores junto a sus padres.</p>\n<!-- BREAK 1 -->\n<p>Sin embargo, estas técnicas presentan dos grandes problemas. El primero y fundamental, que <strong>cada vez son menos efectivas ocultando rostros</strong>: hace ya 3 años, investigadores de la Universidad de Austin y el Cornell Tech de Nueva York recurrieron al machine learning para crear un software capaz de identificar rostros (y objetos) aunque éstos estuviesen pixelados.</p>\n<!-- BREAK 2 -->\n<!--more-->\n\n<p>El segundo gran problema es que eliminando de la pantalla las facciones del sujeto <strong>se le arrebata también toda su expresividad</strong>: queda reducido a una masa informe de píxeles que habla sin proporcionarnos referencias para percibir sus emociones.</p>\n<!-- BREAK 3 -->\n<h2>Usando los deepfakes para el bien</h2>\n<p>Sabiendo todo esto, un grupo de investigadores de la Univ. Noruega de Ciencia y Tecnología han puesto en marcha una técnica aún experimental, <a href=\"https://github.com/hukkelas/DeepPrivacy/blob/master/README.md\">llamada DeepPrivacy</a>, basada en extraer información sobre las expresiones faciales de la persona a través de la localización de puntos de referencia (ojos,hombros, nariz, etc).</p>\n<!-- BREAK 4 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Futbol\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/695130/futbol/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/695130/futbol/450_1000.jpg 450w, https://i.blogs.es/695130/futbol/650_1200.jpg 681w, https://i.blogs.es/695130/futbol/1024_2000.jpg 1024w, https://i.blogs.es/695130/futbol/1366_2000.jpg 1366w\"/><noscript><img alt=\"Futbol\" src=\"https://i.blogs.es/695130/futbol/450_1000.jpg\"/></noscript> <span> Leo Messi y Cristiano Ronaldo tras ser \'anonimizados\' usando DeepPrivacy. </span> </div></div></div>\n<p>A continuación, una GAN (<a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-gans-redes-generativas-antagonicas\">red generativa antagónica</a>), previamente entrenada con un dataset de 1,5 millones de imágenes de rostros, <strong>crea un rostro humano completamente nuevo</strong> (es decir, no aplica el rostro de una persona real sobre el cuerpo de otra) y lo aplica sobre el sujeto que aparece en el vídeo manteniendo las expresiones originales.</p>\n<!-- BREAK 5 -->\n<p>Los científicos creadores de esta nueva técnica reconocen que todavía es mejorable, y que <strong>suele fallar cuando los rostros se encuentran en determinados ángulos o están parcialmente ocultados.</strong> Sin embargo, destacan el hecho de haber encontrando un uso socialmente positivo a una tecnología, como es la de los deepfakes, de la que siempre suele destacarse <a href=\"https://www.xataka.com/robotica-e-ia/no-solo-porno-estudio-avisa-como-deep-fakes-pueden-acabar-manipulando-nuestra-opinion\">su vinculación al porno o su utilidad para manipular elecciones</a>.</p>\n<!-- BREAK 6 -->\n<p>Imagen | <a href=\"https://www.flickr.com/photos/vandalog/2961626247\">RJ</a> &amp; DeepPrivacy</p>\n</div>', '2019-09-18 01:17:42', '2019-09-18 01:17:42', 33, 'portada017.jpg', 'Cuando se pretende anonimizar una imagen, el recurso habitual es distorsionar o pixelar el rostro. Pero esa opción presenta problemas que podemos solventar usando deepfakes.'),
(39, '30 años menos gracias a la IA: el último \'deepfake\' viral rejuvenece con gran realismo a David Hasselhoff', '<div class=\"blob js-post-images-container\">\n<p>Los más jóvenes del lugar no lo recordarán, pero hubo un tiempo en que el actor David Hasselhoff (protagonista de series de TV como \'El coche fantástico\' y \'Vigilantes de la playa\') era considerado un \'sex symbol\' por muchos. Pero él, como todos nosotros, <strong>ha acusado el paso de los años</strong>.</p>\n<!-- BREAK 1 -->\n<p>Sin embargo, en las últimas horas el intérprete ha podido presenciar cómo se viralizaba un vídeo en el que, gracias al uso de la tecnología deepfake, <strong>un youtuber lograba devolverle el aspecto de sus años de mayor éxito</strong>... en un vídeo de una reciente entrevista televisiva.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>El autor del vídeo manipulado, VFXChris Ume (que hasta ahora sólo contaba en su canal con dos mejorables deepfakes protagonizados por Ibrahimovic) <strong>logra quitarle años recurriendo a imágenes extraídas de antiguos capítulos de \'Vigilantes de la playa\'</strong>, mediante un proceso que se deja entrever al final del propio vídeo, en el que también se muestra que el proceso de rejuvenecimiento no afecta únicamente al rostro, sino también al cuello, el vello pectoral y las canas.</p>\n<!-- BREAK 3 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/jYY0LAT1YRQ\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>En resumen, una completa sesión de chapa y pintura que engaña perfectamente al <a href=\"https://www.xataka.com/inteligencia-artificial/posible-saber-video-deepfake-solo-abrir-cerrar-ojos-literalmente-quizas-eso-no-sea-suficiente\">ojo desentrenado</a>, y que ilustra a la perfección <strong>el potencial de los deepfakes</strong>. Quién sabe, quizá muchos intérpretes que en su momento fueron dejados de lado por Hollywood terminen agradeciendo a esta tecnología su retorno al cine portando el aspecto de sus mejores años.</p>\n<!-- BREAK 4 -->\n<h2>Similitudes y diferencias entre el joven Mitch Buchannon y el joven Nick Furia</h2>\n<p>No es la primera vez que vemos en pantalla <a href=\"https://www.xataka.com/cine-y-tv/tu-no-te-puedes-rejuvenecer-pero-hollywood-si-asi-se-quitan-las-arrugas-bradd-pitt-o-jonhny-depp\">un logrado rejuvenecimiento digital</a> de alguna estrella del cine o la TV. <strong>Las últimas películas del Universo Marvel, por ejemplo, están llenas de ellos</strong>: el Robert Downey Jr. adolescente de \'Civil War\', el Kurt Russell setentero (que no setentón) de \'Guardianes de la Galaxia 2\'... ¿Y qué decir de Michael Douglas y Michelle Pfeiffer con 30 años menos (y juntos por primera vez en la gran pantalla) en \'Ant-Man y la Avispa\'?</p>\n<!-- BREAK 5 -->\n\n<p>Sin embargo, el rejuvenecimiento de Samuel L. Jackson en \'Capitana Marvel\' ofrece más pistas sobre la complejidad de esta labor. Trent Claus, supervisor de efectos visuales de Lola Visual Effects, la empresa contratada por Marvel para esta labor, explica que aun contando con numeroso material del actor rodado en los 90 (época en que estaba ambientada la película), <strong>gran parte del mismo no sirvió para dotar de su aspecto al joven Nick Furia</strong>:</p>\n<!-- BREAK 6 -->\n<blockquote>\n<p>\"Aunque tengas muchos ángulos, es extraño encontrar la posición adecuada, la luz adecuada y esas cosas [...] Tuvimos que descartar Pulp Fiction casi de inmediato, por el vello facial. Bloqueaba mucho de las referencias que necesitamos, así que no era muy útil\".</p>\n</blockquote>\n<p>Sin embargo, aunque VFXChris Ume sin duda se habrá enfrentado a obstáculos similares para elegir el material para el deepfake que hoy nos ocupa, hay una cosa que le diferencia de los empleados de Lola Visual Effects: su presupuesto. <strong>Esta tecnología democratiza el acceso a los efectos especiales</strong>, permitiendo crearlos a un público mucho más amplio.</p>\n<!-- BREAK 7 --> </div>', '2019-09-17 10:00:32', '2019-09-17 10:00:32', 4, 'portada018.jpg', 'El actor parece recién sacado de \'Vigilantes de la playa\', pero no: todo se debe a la magia de los deepfakes.'),
(40, 'La NASA confía en que la nuevas generaciones de robots espaciales estadounidenses y rusos sean capaces de cooperar entre sí', '<div class=\"blob js-post-images-container\">\n<p>El pasado jueves, la agencia espacial rusa anunció que el primer robot humanoide ruso enviado al espacio, conocido como FIEDOR (Final Experimental Demonstration Object Research), <strong>había finalizado su misión tras sólo tres semanas en órbita y que no volvería al espacio</strong>.</p>\n<!-- BREAK 1 -->\n<p>Los ingenieros rusos ya están <strong>trabajando en \"un reemplazo adecuado\"</strong> para esta máquina de 1,80 metros y 160 kg de peso, un reemplazo que se ajuste \"a las exigencias de trabajo fuera de la Estación Espacial Internacional\", pues este modelo fue originalmente diseñado para la lucha contra incendios:</p>\n<!-- BREAK 2 -->\n<!--more-->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/lHPSN9tfmGA\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<h2>¿Por qué la ISS necesita una nueva generación de robots?</h2>\n<p>Y es que el diseño de FIEDOR no ha terminado de convencer a sus creadores, pues su incapacidad para desplazarse en condiciones de ingravidez <strong>impide usarlo como sustituto de los astronautas en labores arriesgadas</strong> (como las caminatas espaciales), y sus largas piernas han demostrado ser poco útiles en el entorno de la ISS.</p>\n<!-- BREAK 3 -->\n\n<p>Antes de la llegada de FIEDOR, el primer robot humanoide en órbita había sido el Robonaut 2 de la NASA, que permaneció en la ISS desde 2011 hasta principios de 2018, cuando sufrió un fallo en su sistema de alimentación eléctrica y tuvo que descender a la superficie para ser reparado. En su caso, también resultó ser <strong>un fiasco a la hora de realizar las tareas que tenía encomendadas, por resultar \"demasiado grande y torpe\"</strong>.</p>\n<!-- BREAK 4 -->\n<p>Ni su destreza a la hora de manejar objetos, ni su capacidad para trabajar codo con codo con humanos ha resultado ser la esperada. Por ello, <strong>no ha realizado ninguna tarea en la estación durante los últimos 5 años</strong>. Se sabe, eso sí, que Robonaut 2 volverá a la ISS a finales de este año.</p>\n<!-- BREAK 5 -->\n<h2>Colaboración entre robots espaciales</h2>\n<p>Interrogado sobre el uso de robots en la ISS, uno de los portavoces de la NASA, Joshua Finch, reafirmó la importancia de recurrir a ellos en la exploración espacial, sobre todo a partir de futuros viajes a la Luna y Marte.</p>\n<!-- BREAK 6 -->\n\n<blockquote>\n<p>\"Como la disponibilidad de tiempo de los astronautas siempre será limitada y no resulta oportuno encargar a los tripulantes que lleven a cabo algunas tareas a mano, los robots pueden complementar a los humanos, realizando su trabajo de modo autónomo o bajo supervisión desde la Tierra\".</p>\n</blockquote>\n<p>Pero en la ISS, los astronautas en ocasiones tienen que llevar a cabo sus tareas de forma conjunta, colaborando al margen de su nacionalidad. Si futuros robots están preparados para sustituirlos... <strong>¿incluirá eso la colaboración entre distintas clases de robots, fabricados por diferentes potencias espaciales?</strong></p>\n<!-- BREAK 7 -->\n<p>Según Finch, sí: \"Teniendo en cuenta los múltiples éxitos que lograron nuestras tripulaciones, colaborando entre sí en el espacio, <strong>creo que nuestros robots podrán hacer lo mismo</strong>\".</p>\n<!-- BREAK 8 -->\n<p>Vía | <a href=\"https://mundo.sputniknews.com/espacio/201909131088678987-la-nasa-espera-que-robots-de-rusia-y-eeuu-cooperen-con-el-mismo-exito-que-tripulantes/\">Sputnik News</a> &amp; <a href=\"https://www.space.com/russia-launching-humanoid-robot-into-space.html\">Space.com</a></p>\n<!-- BREAK 9 --> </div>', '2019-09-14 20:14:45', '2019-09-14 20:14:45', 29, 'portada019.jpg', 'Ante los fracasos del robot ruso Fiedor y del estadounidense Robonaut-2, la NASA confía en que la nuevas generaciones de robots espaciales de ambas naciones sean capaces de cooperar entre sí'),
(41, 'Una mano protésica combina por primera vez control humano y robótico para mejorar la destreza del usuario', '<div class=\"blob js-post-images-container\">\n<p>Los científicos del el Laboratorio de Algoritmos y Sistemas de Aprendizaje de la Escuela Politécnica Federal de Lausana (EPFL) han estado trabajando en nuevos enfoques que <strong>permitan a pacientes amputados mejorar la precisión de sus manos prostáticas</strong>.</p>\n<!-- BREAK 1 -->\n<p>Ayer publicaron los resultados de sus investigaciones en Nature Machine Intelligence: <strong>una prueba de concepto interdisciplinar, a medio camino entre la robótica y la neuroingeniería</strong>, consistente en recurrir a la inteligencia artificial para combinar el control individual de los dedos y la automatización de los procesos de agarre y manipulación.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>De este modo, gracias a la neuroingeniería, resulta posible descifrar qué movimientos desea realizar la persona amputada, analizando los movimientos musculares del muñón para convertir esta señal es el control individual de la mano protésica, al tiempo la robótica permite que el agarre sea firme, haciendo que <strong>los dedos artificiales sujeten los objetos antes incluso de que el usuario se dé cuenta de que se le estaban escapando</strong>.</p>\n<!-- BREAK 3 -->\n\n<p>Así lo explica Aude Billard, director del laboratorio:</p>\n<blockquote>\n<p>\"Si un objeto comienza a deslizarse en su mano, ésta sólo tiene unos pocos milisegundos para responder. La mano protésica puede responder dentro de los 400 milisegundos. Debido a que está equipada con sensores de presión a lo largo de los dedos, es capaz de reaccionar estabilizando el objeto antes de que el cerebro pueda percibirlo deslizarse\".</p>\n</blockquote>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/L_jhQxMF8R4\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<h2>El algoritmo que hay detrás</h2>\n<p>Todo esto es posible porque, en primer lugar, <strong>se entrena un algoritmo para que sea capaz de decodificar las intenciones de los usuarios y traducirlas en movimientos</strong> de los dedos; dicho entrenamiento consiste en que al usuario amputado se le indiquen una serie de movimientos a realizar, para que la IA capte la actividad muscular a través de los sensores y a prenda a vincular los patrones de la misma con movimientos concretos.</p>\n<!-- BREAK 4 -->\n<p>Sólo una vez que el algoritmo ha entendido los movimientos de los dedos del usuario, esta información se puede utilizar para controlar los dedos individuales de la mano protésica. Lo explica Katie Zhuang, otra de las investigadoras:</p>\n<!-- BREAK 5 -->\n<blockquote>\n<p>\"Debido a que esas señales musculares pueden ser \'ruidosas\', necesitamos que un algoritmo de aprendizaje automático sea capaz de identificar la actividad relevante de esos músculos y que los traduzca en movimientos\".</p>\n</blockquote>\n\n<p>Además, el algoritmo está programado para hacer q<strong>ue la mano apriete automáticamente cuando el usuario intenta agarrar un objeto</strong>. Si bien según los investigadores, todavía deben sortear varios obstáculos antes de que el algoritmo pueda implementarse en una mano protésica disponible en el mercado. Aunque, a largo plazo, en palabras de Silvestro Micera, responsable del Programa Bertarelli en Neurociencia Translacional de la EPFL,</p>\n<!-- BREAK 6 -->\n<blockquote>\n<p>\"nuestro enfoque de \'control compartido\' de las manos robóticas podría contar con varias aplicaciones neuroprotésicas además de las prótesis de mano biónicas, como las interfaces cerebro-máquina, aumentando así el impacto clínico y la usabilidad de estos dispositivos\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://www.sciencedaily.com/releases/2019/09/190911113007.htm\">Science Daily</a></p>\n<p>Imagen | EPFL</p>\n</div>', '2019-09-13 00:05:45', '2019-09-13 00:05:45', 47, 'portada000.jpg', 'La inteligencia artificial de la mano protésica permite tanto \'traducir\' los impulsos nerviosos de las personas amputadas en movimientos como regular el agarre automatizado de los dedos.');
INSERT INTO `posts` (`id`, `titulo`, `cuerpo`, `created_at`, `updated_at`, `user_id`, `imagen`, `descripcion`) VALUES
(42, '7 documentales sobre inteligencia artificial y robótica que puedes ver en Internet', '<div class=\"blob js-post-images-container\">\n<p>En la última década, <strong>los muchos avances conseguidos en el campo de la inteligencia artificial han capturado el interés del público</strong>, tanto del más \'geek\' como el de gente ajena al ámbito tecnológico pero impactada por la dimensión de los cambios que se avecinan.</p>\n<!-- BREAK 1 -->\n<p>Y donde hay demanda del público, suele haber oferta por parte de los productores de contenidos. Y <a href=\"https://www.xataka.com/tag/ia-en-la-cultura\">no sólo el cine de (ciencia) ficción se ha inspirado en la IA</a>: también los creadores de documentales han procurado informar desde la pantalla sobre el tema favorito para los lectores en Xataka Captcha:</p>\n<!-- BREAK 2 -->\n<!--more-->\n<h2>Plug &amp; Pray (2010)</h2>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/ecPEkG2Pclg\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>El director alemán Jens Schnaze <strong>entrevista en este documental a algunas de las principales figuras de la robótica del momento</strong> para interrogarles acerca de la posibilidad (o del deseo) de ir más allá de crear máquinas complejas y empezar a jugar a ser dioses creando \"humanos artificiales\".</p>\n<!-- BREAK 3 -->\n<p><strong>El objetivo de la obra pasa por explorar las consecuencias éticas</strong> (<a href=\"https://www.xataka.com/inteligencia-artificial/inteligencia-artificial-esta-agitando-mundo-que-no-esperabamos-religion\">e, incluso, religiosas</a>) antes que los aspectos técnicos del fenómeno, con visitas incluidas a exposiciones de armas robóticas.</p>\n<!-- BREAK 4 -->\n<p>Schnaze hace desfilar por la pantalla, entre otros, al futurista <strong>Raymond Kurzweil</strong> (<a href=\"https://www.xatakaciencia.com/tecnologia/singularity-university-y-las-predicciones-quiza-demasiado-optimistas-de-kurzweil\">fundador de la Universidad de la Singularidad</a>), al científico italiano <strong>Giogio Metta</strong> (creador del <a href=\"https://www.xataka.com/robotica-e-ia/el-robot-europeo-icub-quiere-aprender-de-nosotros\">robot iCub</a>), al estadounidense <strong>Neil Gershenfeld</strong> (director del Center for Bits and Atoms del MIT) o al <strong>popular ingeniero japonés Hiroshi Ishiguro</strong> (especializado en el d<a href=\"https://www.xataka.com/robotica-e-ia/ibuki-robot-que-simula-aspecto-nino-10-anos-que-bien-podria-ser-protagonista-pelicula-terror\">iseño de robots ultrarrealistas</a>).</p>\n<!-- BREAK 5 -->\n<p>Frente a los diversos grados de tecno-optimismo de la mayoría de los científicos entrevistados, <strong>en el documental se alza la voz de Joseph Weizenbaum</strong>, <a href=\"https://www.xataka.com/historia-tecnologica/asi-era-eliza-el-primer-bot-conversacional-de-la-historia\">creador del primer chatbot (Eliza)</a> y uno de los pioneros de la inteligencia artificial, que murió varios meses antes del estreno del documental:</p>\n<!-- BREAK 6 -->\n<blockquote>\n<p>\"Es desastroso que la mayoría de mis colegas crean que podemos crear un ser humano artificial. Este inmenso sinsentido es producto de un delirio de grandeza. Quizá, si yo hubiera sabido entoces lo que sé ahora, hubiera decidido que no quería dedicarme a este campo\".</p>\n</blockquote>\n<p><strong>Duración:</strong> 90 minutos</p>\n<p><strong>Puedes verlo en</strong>: <a href=\"https://store.playstation.com/en-us/product/UV0046-NPVA55141_CN-0000000000132908\">Playstation Store</a> &amp; <a href=\"https://www.xataka.com/redirect?url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fp%2Fplug-pray%2F8d6kgwzl5jrl%3Factivetab%3Dpivot%253aoverviewtab&amp;category=inteligencia-artificial\">Microsoft Store</a></p>\n<!-- BREAK 7 -->\n<p><em>Ver los primeros cinco minutos</em>:</p>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/QkH88qYsjqM\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n\n<h2>Humans Need Not Apply (2014)</h2>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/7Pq-S557XQU\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Este documental de 15 minutos, financiado mediante crowdfunding, es <strong>una pieza del divulgador (youtuber y podcaster) CGP Grey</strong> y se centra en la integración de las tecnologías de automatización dentro de nuestro sistema productivo, así como en <a href=\"https://www.xataka.com/robotica-e-ia/inteligencia-artificial-reemplazara-40-trabajos-proximos-15-anos-asegura-kai-fu-lee-pionero-ia\">el modo en que esto afectará al mercado de trabajo</a>. Sí, el documental es de 2014, pero cada día que pasa este debate cobra más actualidad.</p>\n<!-- BREAK 8 -->\n<p>Ante el argumento de que <a href=\"https://www.xataka.com/robotica-e-ia/automatizacion-eliminara-75-millones-empleos-para-2025-creara-133-millones-nuevas-funciones-wef\">la IA siempre creará, directa o indirectamente, más empleos</a> para los humanos que pierdan el suyo, el documental recuerda que en su momento <strong>la mecanización nos permitió prescindir de los caballos sin que eso crease nuevas ocupaciones para ellos</strong>.</p>\n<!-- BREAK 9 -->\n<p>Y nos recuerda que <strong>este debate ya no es ciencia-ficción</strong>: el coche totalmente autónomo está (casi) aquí, y 70 millones de personas en todo el mundo trabajan en ese sector.</p>\n<!-- BREAK 10 -->\n<p><strong>Duración:</strong> 15 minutos</p>\n<p><strong>Puedes verlo en:</strong> Youtube, de forma legal y gratuita (sí, lo tienes más arriba).</p>\n<h2>Roboticize me (2015)</h2>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/oLak95-I7DU\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Peter Keleghan, <strong>un actor canadiense especializado en papeles cómicos, ejerce de presentador</strong> de este documental en compañía de <a href=\"https://www.engineeredarts.co.uk/robothespian/\">RoboThespian</a>, un robot humanoide creado con el fin de ser capaz de interactuar con el público.</p>\n<!-- BREAK 11 -->\n<p>A lo largo de esta obra (en realidad, un capítulo de una serie de documentales llamada Doc Zone), se ofrece <strong>una exploración divertida al tiempo que provocadora de la inminente revolución robótica</strong> en la que estamos inmersos.</p>\n<!-- BREAK 12 -->\n<p>\'Roboticize me\' nos muestra a estos compañeros que hemos creado para nuestro uso y disfrute: <strong>sirvientes domésticos, mascotas y hasta potenciales compañeros sexuales</strong>. Y reflexiona sobre cómo, cada vez más, nuestros avances en este campo amenazan con empujar límites éticos y legales.</p>\n<!-- BREAK 13 -->\n<p>El documental <strong>arranca, como no podía ser de otro modo, en Japón</strong>, el país donde los robots cuentan con un papel más preponderante y con una mayor aceptación social, y recorre varios escenarios para mostrarnos diversos ejemplos de esta revolución.</p>\n<!-- BREAK 14 -->\n<p>Así, <strong>presenciamos desde un cabaré robótico en Tokio hasta el laboratorio en la UCLA (California) de Dennis Hong</strong>, el \'Mago de los robots\' que ha creado más de una veintena de androides, desde jugadores de fútbol a robots de rescate. Y se repasa también <a href=\"https://magnet.xataka.com/que-pasa-cuando/la-adorable-y-tragica-historia-de-hitchbot-el-robot-que-viaja-gracias-a-la-gente\">la historia de hichtBOT</a>, el robot autoestopista que hizo historia cruzando \'a dedo\' Canadá, Alemania y Holanda.</p>\n<!-- BREAK 15 -->\n<p>Un aspecto negativo del documental son los diálogos entre Peter Keleghan y RoboThespian, en las que la complejidad de las respuestas de éste último <strong>ofrece una imagen tramposa del nivel actual de desarrollo de la inteligencia artificial</strong>.</p>\n<!-- BREAK 16 -->\n<p><strong>Duración:</strong> 45 minutos.</p>\n<p><strong>Lo puedes ver:</strong> <a href=\"https://www.cbc.ca/doczone/episodes/roboticize-me\">En la web de la CBC</a>... pero sólo si accedes desde Canadá.</p>\n<!-- BREAK 17 -->\n<h2>Lo and Behold: El inicio de Internet (2016)</h2>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/Zc1tZ8JsZvg\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p><a href=\"https://www.espinof.com/proyectos/el-infatigable-werner-herzog\">Werner Herzog es conocido</a> por haber dirigido clásicos del cine de ficción como \'Fitzcarraldo\' o \'Aguirre, la cólera de Dios\', pero <strong>en 2016 también se animó a dirigir un documental sobre tecnología</strong> llamado \'Lo and Behold: Reveries of the Connected World\' (también conocido en Iberoamérica como \'He aquí las ensoñaciones del mundo conectado\').</p>\n<!-- BREAK 18 -->\n<p>Esta obra no se centra únicamente en la inteligencia artificial, sino que sus 10 capítulos repasan varios aspectos de las nuevas tecnologías (positivos y negativos) de las mismas, p<strong>artiendo del instante de la creación de ARPANET (el primer Internet) en 1969</strong>. </p>\n<!-- BREAK 19 -->\n<p>El octavo capítulo se centra en la creación de la inteligencia artificial, y en el trabajo que se está llevando a cabo para crear conciencia artificial. Lawrence Krauss, físico teórico, amigo de Herzog y \'personaje\' presente en varios de los capítulos del documental, recuerda en un momento dado <strong>lo fútil de intentar esto cuando ni siquiera somos capaces de entender nuestra propia conciencia</strong>.</p>\n<!-- BREAK 20 -->\n<p>Cabe destacar que <strong>Elon Musk también se deja caer por este documental</strong>, pero está demasiado ocupado hablando de la conquista espacial como para hacer mención a su otro tema favorito: las amenazas de la IA.</p>\n<!-- BREAK 21 -->\n<p>El documental intenta abarcar muchos temas y <strong>puede \'saber a poco\' a los iniciados en estos temas</strong>, pero ofrece un interesante repaso introductorio para los más legos.</p>\n<!-- BREAK 22 -->\n<p><strong>Duración:</strong> 98 minutos.</p>\n<p><strong>Puedes verlo en:</strong> <a href=\"https://www.netflix.com/es/title/80097363\">Netflix</a></p>\n<h2>Open Source Stories: Road to AI (2017)</h2>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/_sNNSEP-P7A\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<blockquote>\n<p>\"Cómo enseñas a un coche a conducir? Para muchos fabricantes de coches autónomos e investigadores de inteligencia artificial, <a href=\"https://www.xataka.com/automovil/coche-autonomo-union-hace-fuerza-estas-seis-grandes-alianzas-automovilisticas-que-comparten-tecnologia-para-ser-primeras\">la respuesta comienza compartiendo datos</a>\".</p>\n</blockquote>\n<p>\'Road to AI\' no es, por poco, el documental más breve de la lista; se trata de un capítulo de la serie de cortos documentales \'Open Source Stories\', <strong>producida por la compañía de software Red Hat</strong>, una de las más <a href=\"https://www.xataka.com/aplicaciones/open-source-arrasa-vive-epoca-dorada-tambien-hay-algo-postureo\">exitosas del mundo \'open source\'</a> y propietaria de la distribución GNU/Linux del mismo nombre.</p>\n<!-- BREAK 23 -->\n<p>El documental trata de presentar <strong>un repaso al estado actual de la industria del vehículo autónomo</strong>, destacando el papel que el código abierto podría tener en su evolución futura. Por el documental veremos desfilar a François Chollet (creador del framework de deep learning Keras), a Maru Cummings (una de las primeras mujeres que ejerció de piloto de combate en los EE.UU., reconvertida en responsable del Laboratorio de Robótica de la Univ. de Duke) o a Karl Iagnemma (<a href=\"https://www.xataka.com/vehiculos/el-primer-servicio-de-taxis-autonomos-entra-en-periodo-de-pruebas-en-singapur\">fundador de nuTonomy</a>, una startup especializada en conducción autónoma urbana).</p>\n<!-- BREAK 24 -->\n<p><strong>Duración:</strong> 17 minutos.</p>\n<p><strong>Puedes verlo en:</strong> Youtube, de forma legal y gratuita (sí, lo tienes más arriba).</p>\n\n<h2>AlphaGo (2017)</h2>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/8tq1C8spV_g\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p><strong>\'AlphaGo\' es la historia de cinco partidas</strong>: las jugadas en 2016 entre la IA homónima (creada por DeepMind, una compañía vinculada a Google) y el campeón mundial (en 18 ocasiones) del milenario juego oriental Go, el surcoreano Lee Sedol. Por fortuna, todo el torneo fue grabado, y no veremos en pantalla ninguna \'reconstrucción\': sólo lo ocurrido en aquellos días.</p>\n<!-- BREAK 25 -->\n<p><strong>También es la historia de cuatro derrotas</strong>: <a href=\"https://www.xataka.com/robotica-e-ia/alphago-gana-la-ultima-partida-a-lee-sedol-y-cierra-con-un-contundente-4-1-final\">las infligidas por la máquina</a> a un humano que se muestra confiado en que las máquinas nada tienen que hacer frente los humanos en este juego (que en su país es casi un arte). El documental analiza, por ejemplo, cómo vivió el público surcoreano esta imprevista derrota.</p>\n<!-- BREAK 26 -->\n<p>Pero el documental también es una crónica de cómo se llegó hasta ese torneo celebrado durante una semana en el hotel Four Seasons de Seúl: <strong>es, en parte, la historia de cómo el maestro de ajedrez Demis Hassabis creó DeepMind</strong> en su Londres natal y sobre cómo se fijó en el Go como un medio para un fin, como un reto que permitiera aplicar los logros de la IA a campos como el de la medicina.</p>\n<!-- BREAK 27 -->\n<p>Y, aunque <strong>el documental rehuye de sumergirse en detalles técnicos</strong> más allá de unas cuantas pinceladas, también es la historia de cómo las nuevas técnicas de machine learning están logrando victorias imposibles para IAs más antiguas.</p>\n<!-- BREAK 28 -->\n<p><strong>Duración:</strong> 90 minutos.</p>\n<p><strong>Puedes verlo en:</strong> <a href=\"https://www.netflix.com/es/title/80190844\">Netflix</a></p>\n<h2>Do You Trust This Computer? (2018)</h2>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/3CJE6XheubM\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<blockquote>\n<p>\"Nada afectará más al futuro de la humanidad que una superinteligencia artificial. Accede gratis a la nueva película de Chris Paine sobre inteligencia artificial hasta el próximo domingo\".</p>\n</blockquote>\n<p>Este mensaje lo difundió Elon Musk a través de Twitter en abril de 2018, y en referencia al documental \'Do You Trust This Computer?\' (¿Confías en esta computadora?). Y <strong>si fue posible ofrecer gratis la obra fue, sencillamente, porque el propio Musk pagó para ello</strong>.</p>\n<!-- BREAK 29 -->\n<p>Chris Paine y Musk ya se conocían de antes: el primero había dirigido dos documentales sobre coches eléctricos (\'Who Killed The Electric Car?\' y \'Revenge of the Electric Car\') que contaron con la presencia del CEO de Tesla. Por supuesto, también aparece en el que nos ocupa, un documental totalmente <a href=\"https://www.xataka.com/robotica-e-ia/elon-musk-vuelve-inteligencia-artificial-peligro-concentracion-poder-que-humanos-acaben-como-mascotas\">alineado con su propia perspectiva</a> sobre <strong>la inteligencia artificial como un peligro potencial para la especie humana</strong>.</p>\n<!-- BREAK 30 -->\n<p>La película es clara desde su mismo título: quiere que el espectador se haga preguntas sobre la fiabilidad de la inteligencia artificial, sobre la seguridad de nuestros datos, sobre la posibilidad de que la tecnología se descontrole. <strong>Y, por si acaso con eso no basta, el comienzo de la película es aún menos sutil</strong>: la primera frase que oiremos tras empezar el documental será \"La tecnología avanza mucho más rápido que la habilidad de proteger a nuestros ciudadanos\".</p>\n<!-- BREAK 31 -->\n<p>Y justo después de la intro veremos a un usuario clicando en \"Sí, confío\"... <strong>para que a continuación se nos muestre a un Terminator T-800 caminando sobre cráneos humanos</strong>. Nada sutil, no. Y el resto de la puesta en escena va en la línea de lo anterior.</p>\n<!-- BREAK 32 -->\n<p>Uno de los temas que se abordan con detalle en el filme es <strong>el avance experimentado en los últimos años en el campo del análisis de datos</strong> y cuánto ha avanzado la capacidad de aprendizaje de las máquinas, aumentando su capacidad para influir en la vida de gente... hasta el punto de facilitar la realización de manipulaciones electorales.</p>\n<!-- BREAK 33 -->\n<p>Sin embargo, <strong>eso no significa que los mensajes pro-IA hayan sido desterrados del documental</strong> (aunque invariablemente se nos presentan como posicionamientos ingenuos): varios investigadores aparecen hablando de sus actuales aplicaciones para diagnosticar enfermedades con mayor precisión que los humanos, y Eric Horvitz, del Microsoft Research Lab, llega a defender la posibilidad de usar la automatización para convertir los accidentes de tráfico en algo del pasado. </p>\n<!-- BREAK 34 -->\n<p><strong>Duración:</strong> 78 minutos</p>\n<p><strong>Puedes verlo en:</strong> <a href=\"https://www.xataka.com/redirect?url=https%3A%2F%2Fwww.amazon.com%2FDo-You-Trust-This-Computer%2Fdp%2FB07GD9VX5D&amp;category=inteligencia-artificial\">Amazon Prime Video</a></p>\n<!-- BREAK 35 -->\n<p>Imagen | Pixabay</p>\n</div>', '2019-09-08 16:02:33', '2019-09-08 16:02:33', 21, 'portada001.jpg', 'En la última década, los muchos avances conseguidos en el campo de la inteligencia artificial han capturado el interés del público... y los documentalistas han satisfecho su curiosidad. ¿Listo para pasar la tarde viendo documentales?'),
(43, 'Nuevas herramientas para detectar deepfakes, el objetivo en el que Facebook invertirá más de 10 millones de dólares', '<div class=\"blob js-post-images-container\">\n<p>Aunque su <a href=\"https://www.xataka.com/robotica-e-ia/deepfakes-que-te-convierten-dicaprio-movil-8-segundos-asi-zao-app-china-que-esta-arrasando\">faceta lúdica</a> es innegable, hoy en día los deepfakes constituyen también una de las aplicaciones más amenazantes de la inteligencia artificial, por su potencial repercusión sobre la <a href=\"https://www.xataka.com/privacidad/deepnude-polemica-aplicacion-que-desnuda-a-cualquier-mujer-mediante-inteligencia-artificial\">reputación de las personas</a> o sobre <a href=\"https://www.xataka.com/robotica-e-ia/no-solo-porno-estudio-avisa-como-deep-fakes-pueden-acabar-manipulando-nuestra-opinion\">la misma esfera pública</a>. Por eso, <strong>cada vez surgen más iniciativas para luchar contra esta amenaza</strong>... recurriendo a las mismas tecnologías que permitieron crearla. Combatir el fuego con fuego, que suele decirse.</p>\n<!-- BREAK 1 -->\n<p>La mejor forma de limitar los efectos negativos de los deepfakes pasa por detectarlo lo antes posible, antes de que alcances una gran difusión. Pero los sistemas de detección de estas falsificaciones van siempre por detrás de los dedicados a su creación, <strong>constituyendo un constante juego del gato y el ratón</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Mike Schroepfer, CTO de Facebook, afirma que hoy en día ya es relativamente sencillo para un algoritmo detectar un deepfake... <strong>cuando el sistema ya ha tenido ocasión de analizar el vídeo original</strong>, pero si alguien graba un video original y luego lo manipula, la tarea se convierte en algo mucho más complejo.</p>\n<!-- BREAK 3 -->\n\n<p>Por ello, Facebook (con la colaboración de otras compañías e instituciones, como Microsoft o el MIT) ha decidido trabajar para dar un empujón a la evolución de las tecnologías de detección de deepfakes. Y eso se ha traducido en dos decisiones: crear un dataset... y convocar un concurso. <strong>Y la compañía de Zuckerberg piensa dedicar \"más de 10 millones de dólares\" a financiar ambas iniciativas</strong>.</p>\n<!-- BREAK 4 -->\n<h2>Un dataset y un concurso para animar a salir a la caz del deepfake</h2>\n<p>No sería muy útil recurrir al aprendizaje automático para detectar deepfakes si no se contara con un gran almacén de los mismos al que recurrir para entrenar a los algoritmos. Por eso, Facebook ha decidido <strong>tomar la iniciativa de crear no ya sólo el dataset, sino los propios deepfakes que lo conformarán</strong>, contratando a actores que consientan en ceder su imagen para los mismos.</p>\n<!-- BREAK 5 -->\n<blockquote>\n<p>\"Es importante contar con datos disponibles gratuitamente para que la comunidad pueda usarlos, con participantes que consientan claramente, y con pocas restricciones de uso\".</p>\n</blockquote>\n<p>Pero una vez que este dataset esté en marcha, será necesario animar a la comunidad de desarrolladores a que lo use para lograr, cuanto antes, avances en la tarea de detectar deepfakes. <strong>Y ahí es donde entra el concurso, bautizado como  Deepfake Detection Challenge (DFDC)</strong>, que Facebook piensa convocar para premiar económicamente a los mejores desarrollos. </p>\n<!-- BREAK 6 -->\n\n<p>El objetivo del DFDC será <strong>crear herramientas de código abierto</strong> que tanto empresas, como instituciones y medios de comunicación puedan usar para detectar con mayor precisión cuándo un video ha sido manipulado.</p>\n<!-- BREAK 7 -->\n<p>¿De cuánta precisión estamos hablando? El pasado mes de junio, el Information Sciences Institute de la Universidad del Sur de Californio anunció que, gracias a un dataset de 1000 vídeos manipulados, había sido capaz de crear un algoritmo de detección de vídeos deepfake que <strong>mostraba una precisión de hasta el 96%</strong>. Es de esperar que la tasa de éxito de los algoritmos creados gracias al DFDC sea aún mayor.  </p>\n<!-- BREAK 8 -->\n<p>En cualquier caso, desde Facebook dejan claro que la meta de todo esto no es crear un sistema que detecte todo deepfake imaginable, sino tan sólo <strong>encontrar formas de hacer que sea mucho más complejo y costoso crear deepfakes realistas</strong>.</p>\n<!-- BREAK 9 -->\n<p>Vía |</p>\n<p>Imagen | Immortal shots (<a href=\"https://www.pexels.com/photo/image-overturned-magnifying-glass-reading-glass-reading-lens-1972492/\">Pexels</a>)</p>\n<!-- BREAK 10 --> </div>', '2019-09-06 02:51:36', '2019-09-06 02:51:36', 22, 'portada002.jpg', 'Facebook (con el apoyo de otras entidades, como Microsoft o el MIT) convocará un concurso para incentivar a los desarrolladores a que dediquen su atención a esta amenaza.'),
(44, 'Que el lanzamiento de los misiles nucleares tras un ataque enemigo quede en manos de la IA: la propuesta de dos expertos de EE.UU.', '<div class=\"blob js-post-images-container\">\n<p>La existencia de misiles hipersónicos, de misiles furtivos de crucero y de armas dotadas de inteligencia artificial <strong>han reducido aún más el tiempo que las autoridades de una potencia nuclear tendrían para dar luz verde a un contraataque</strong> antes de que sus propios sistemas de control y comunicaciones quedaran inutilizados por el enemigo.</p>\n<!-- BREAK 1 -->\n<p>Ante esta circunstancia, los expertos estadounidenses en disuasión nuclear Adam Lowther y Curtis McGiffin han propuesto en <a href=\"https://warontherocks.com/2019/08/america-needs-a-dead-hand/\">un artículo</a> publicado en una publicación especializada que, en su país, la potestad de dar la orden de contraataque no dependa de ningún ser humano, sino de una inteligencia artificial: <strong>una IA controlando el botón de lanzamiento de los 3.800 misiles nucleares del arsenal estadounidense</strong>.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<h2>¿Qué fue (o es) la \'Mano del hombre muerto\'?</h2>\n<p>En realidad, la propuesta no es tan novedosa, puesto que el gran enemigo de los EE.UU. durante la Guerra Fría, la Unión Soviética, contó con un sistema similar (semiautomatizado y carente de IA en el sentido moderno del término): <a href=\"https://actualidad.rt.com/actualidad/view/6253-%E2%80%9CLa-mano-muerta%E2%80%9D%2C-sistema-que-permit%C3%ADa-castigar-al-agresor-nuclear\">el Sistema Perimtr</a>, <strong>popularmente conocido en Occidente como la Mano del Hombre Muerto</strong>.</p>\n<!-- BREAK 3 -->\n\n<p>Su funcionamiento era sencillo: el sistema se hallaba en constante contacto con diversas instalaciones militares en territorio ruso (incluyendo el propio Kremlin) y si en algún momento llegaran a fallar todos los canales de comunicación (presumiblemente a causa de un masivo ataque nuclear enemigo), Perimtr habilitaría el lanzamiento manual del arsenal soviético, permitiendo que i<strong>ncluso oficiales de bajo rango sin acceso a los códigos de lanzamiento pudieran accionar el contraataque</strong>.</p>\n<!-- BREAK 4 -->\n<p>Oficialmente, la Mano del Hombre Muerto soviética fue desmantelada tras la caída de la URSS, aunque expertos como Peter Smith (autor de \'Doomsday Men\') <strong>afirman que Rusia seguiría protegida aún hoy por un sistema heredero de Perimetr</strong>.</p>\n<!-- BREAK 5 -->\n<p>En cualquier caso, ahora Lowther y McGiffin proponen dotar a Estados Unidos de un mecanismo equivalente, aunque en esta ocasión la IA permitiría que fuera totalmente automatizado:</p>\n<!-- BREAK 6 -->\n<blockquote>\n<p>\"Puede ser necesario desarrollar un sistema basado en inteligencia artificial, con respuestas predeterminadas, que sea capaz de [reaccionar] a tal velocidad que la reducción de los tiempos de ataque no sitúen a los Estados Unidos en una posición insostenible\".</p>\n</blockquote>\n\n<h2>¿Cómo evitar que un error del algoritmo desemboque en un holocausto nuclear?</h2>\n<p>Los autores proponen otras alternativas a esa propuesta, que pasan por <strong>situar sistemas de lanzamiento nuclear fuera de las fronteras estadounidenses para garantizar el mantenimiento de su capacidad de represalia</strong>. Sin embargo, dejan muy clara su apuesta por el uso de inteligencia artificial. Y eso plantea preguntas sobre los algoritmos y los protocolos de seguridad con los que podría contar semejante sosias de Skynet para evitar desatar una guerra nuclear no deseada.</p>\n<!-- BREAK 7 -->\n<p>Hoy en día, los algoritmos de <a href=\"https://www.xataka.com/robotica-e-ia/machine-learning-y-deep-learning-como-entender-las-claves-del-presente-y-futuro-de-la-inteligencia-artificial\">aprendizaje automático</a> <strong>son entrenados mediante el uso de grandes conjuntos de datos que permitirán a la IA aprender a detectar lo que está buscando</strong>. Por ejemplo, los sistemas de conducción autónoma se forman analizando millones de ejemplos de conducción humana... y aún así, <a href=\"https://www.xataka.com/robotica-e-ia/30-millones-ejemplos-conduccion-humana-suficientes-para-ensenar-al-coche-autonomo-a-gestionar-imprevistos\">la cantidad de datos se revela insuficiente dada la complejidad de la tarea.</a> </p>\n<!-- BREAK 8 -->\n<p>De modo que, ¿cómo entrenar un algoritmo que debe ser capaz de detectar que su país ha sufrido un ataque nuclear masivo... <strong>cuando en la historia jamás se ha dado un solo caso de un ataque así</strong>? Michael Horowitz, profesor de la Univ. de Pennsylvania y colaborador del Bulletin of Atomic Scientists (la organización científica responsable de mantener <a href=\"https://www.xatakaciencia.com/otros/el-reloj-del-juicio-final-nos-quedan-cinco-minutos\">el Reloj del Juicio Final</a>), afirma que, dado el estado del desarrollo de la IA, un sistema así implicaría \"algunos riesgos\".</p>\n<!-- BREAK 9 -->\n<p>Siempre se podría buscar, a modo de mecanismo de seguridad, que el sistema demandase la confirmación de algún humano antes de dar por buena la detección de dicho ataque. Al fin y al cabo, <strong>el teniente coronel Stanislav Petrov ya evitó un holocausto nuclear en 1983</strong>, cuando el sistema soviético de detección de misiles nucleares detectó erróneamente cinco misiles dirigiéndose hacia territorio ruso, hecho que él (acertadamente, aunque en contra del protocolo de seguridad oficial) desestimó como una mera \'falsa alarma\'.</p>\n<!-- BREAK 10 -->\n\n<p>Sin embargo, en los tiempos de la inteligencia artificial, <strong>el ser humano tiende a ser cada vez menos escéptico ante las afirmaciones de ésta. Horowitz lo denomina \'sesgo de automatización\'</strong> y pone de ejemplo <a href=\"https://www.researchgate.net/publication/47792928_Complacency_and_Bias_in_Human_Use_of_Automation_An_Attentional_Integration\">un estudio</a> en el que los pilotos afirmaron que no confiarían en un sistema automatizado que les informase de un incendio en el motor a menos que hubiera evidencia que lo corrobora... pero, una vez inmersos en las simulaciones, eso fue exactamente lo que decidieron hacer.</p>\n<!-- BREAK 11 -->\n<blockquote>\n<p>\"Existe en riesgo de que el sesgo de automatización impida al Petrov del futuro utilizar su propio juicio. O, sencillamente, de que en el futuro sencillamente no exista un Petrov\".</p>\n</blockquote>\n<p>Vía | <a href=\"https://thebulletin.org/2019/08/strangelove-redux-us-experts-propose-having-ai-control-nuclear-weapons/?utm_source=Twitter&amp;utm_medium=SocialMedia&amp;utm_campaign=TwitterPost082019&amp;utm_content=DisruptiveTechnology_StrangeloveRedux_08302019\">Bulletin of Atomic Scientists</a></p>\n<!-- BREAK 12 -->\n<p>Imagen | Pixabay</p>\n</div>', '2019-09-05 01:07:11', '2019-09-05 01:07:11', 33, 'portada003.jpg', 'Ante la posibilidad de que un rápido ataque nuclear enemigo impidiera a ningún dirigente ordenar una represalia, dos expertos en disuasión nuclear proponen automatizar esa tarea. Existe un precedente: la \'Mano del Hombre Muerto\' de la URSS.'),
(45, 'Facebook ofrecerá la función de reconocimiento facial a todos los usuarios, pero suprimirá las \'sugerencias de etiquetado\'', '<div class=\"blob js-post-images-container\">\n<p>Facebook ha anunciado hace unas horas que, desde este momento, <strong>su tecnología de reconocimiento facial</strong>, disponible desde 2017 para un pequeño porcentaje de usuarios como futura alternativa de las \'sugerencias de etiquetado\', <strong>finalmente estará disponible para el 100% de los usuarios</strong>.</p>\n<!-- BREAK 1 -->\n<p>Esta tecnología <strong>nos permite saber si otro usuario ha subido alguna foto en la que aparecemos</strong>, o si alguien se ha apropiado de nuestra imagen de perfil. En resumen, proporcionará al usuario un mayor control de su imagen y de sus propios contenidos.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>A los pocos usuarios que ya tenían activada esta función se les facilitará su desactivación y al resto, según vaya siendo activada en cada caso (acompañada de una notificación explicativa) se le entregará <strong>desactivada \'por defecto\'</strong>.</p>\n<!-- BREAK 3 -->\n<p>Como contrapartida, Facebook suprimirá las sugerencias de etiquetado, una función que <strong>recurría al reconocimiento facial únicamente para animar al usuario a etiquetar a sus amigos</strong> si éstos aparecían en las fotos.</p>\n<!-- BREAK 4 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Facebook Reconocimiento Facial\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/48430b/facebook_reconocimiento_facial-min/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/48430b/facebook_reconocimiento_facial-min/450_1000.jpg 450w, https://i.blogs.es/48430b/facebook_reconocimiento_facial-min/650_1200.jpg 681w, https://i.blogs.es/48430b/facebook_reconocimiento_facial-min/1024_2000.jpg 1024w, https://i.blogs.es/48430b/facebook_reconocimiento_facial-min/1366_2000.jpg 1366w\"/><noscript><img alt=\"Facebook Reconocimiento Facial\" src=\"https://i.blogs.es/48430b/facebook_reconocimiento_facial-min/450_1000.jpg\"/></noscript> <span> Nueva pantalla de configuración de la función de reconocimiento facial de Facebook. </span> </div></div></div>\n<h2>Los problemas legales de Facebook con su tecnología de reconocimiento facial</h2>\n<p>La Comisión Federal de Comercio estadounidense ya le había \'llamado la atención\' a Facebook con respecto a esta funcionalidad: <strong>califica de \"engañoso\" denominar \'sugerencia\' a una función activada por defecto</strong>. En el marco de su acuerdo con la compañía tras el escándalo de Cambridge Analytica, la FTC impuso nuevas reglas sobre cómo Facebook debería utilizar en el futuro la tecnología de reconocimiento facial, obligando a dar un \'aviso claro y visible\' de su uso.</p>\n<!-- BREAK 5 -->\n<p>A esto se le suma, además, los problemas legales que le viene generando a la compañía, después de que en 2015 un usuario de Illinois (EE.UU.) les demandara por violar una ley estatal de privacidad de la información biométrica. </p>\n<!-- BREAK 6 -->\n\n<p>Además, la compañía perdió hace sólo unos días una apelación en la que se intentó que se desestimara la demanda... <strong>y  aún podría tener que compensar hasta a 7 millones de personas si pierde</strong>. </p>\n<!-- BREAK 7 -->\n<p>Según lo expuesto por Facebook, la red social irá implementando el nuevo sistema de reconocimiento social a escala global <strong>progresivamente durante \"las próximas semanas\"</strong>.</p>\n<!-- BREAK 8 -->\n<p>Vía | <a href=\"https://mashable.com/article/facebook-facial-recognition-setting/\">Mashable</a></p>\n<p>Imagen | Pixabay</p>\n</div>', '2019-09-04 03:07:21', '2019-09-04 03:07:21', 46, 'portada004.jpg', 'Facebook nos dará la opción de saber quién sube a la red social fotos en las que aparezcamos, pero suprimirá las sugerencias de etiquetado que nos animaban a indicar qué amigos aparecían en nuestras fotos.'),
(46, 'Los defensores de los \'neuroderechos\' advierten del peligro de proyectos, como Neuralink, que busquen conectar cerebros a máquinas', '<div class=\"blob js-post-images-container\">\n<p>Hace dos meses, Elon Musk dio detalles sobre <a href=\"https://www.xataka.com/robotica-e-ia/elon-musk-desvela-planes-neuralink-sensores-para-leer-cerebro-que-se-implantan-cosiendo-hilos-microscopicos\">sus planes para una de sus compañías, Neuralink</a>, con la que busca <strong>lograr conectar áreas del cerebro humano, a través de \'cordones neuronales\', con el exterior</strong>. Concretamente, con máquinas dotadas de inteligencia artificial, que podrían leer nuestros picos neuronales de forma mínimamente invasiva.</p>\n<!-- BREAK 1 -->\n<p>Pero Musk no es el único que apuesta convencido por el desarrollo de interfaces cerebro-máquina (BCI): <strong>también Facebook</strong> tiene sobre la mesa <a href=\"https://www.xataka.com/investigacion/facebook-ofrece-nuevos-detalles-su-misterioso-proyecto-que-buscan-que-nos-comuniquemos-mente\">un proyecto llamado \'Building 8\'</a>, con el que confía en ser capaz de leer los pensamientos de sus usuarios.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>De hecho, afirman haber desarrollado un algoritmo capaz de <strong>decodificar en tiempo real palabras a partir de nuestra actividad cerebral</strong>. Algo poco tranquilizador dado el historial de la compañía <a href=\"https://www.xataka.com/privacidad/dinero-a-cambio-privacidad-facebook-pago-a-jovenes-instalar-vpn-que-espiara-sus-datos\">en asuntos de privacidad</a> durante estos últimos años.</p>\n<!-- BREAK 3 -->\n\n<p>Y <strong>no son las únicas compañías con planes similares</strong>: Kernel, Emotiv, y Neurosky también están trabajando en desarrollar sus propias BCIs y en poder lanzarlas algún día al mercado. En su caso alegan la búsqueda de fines éticos, como ayudar a las personas con parálisis a controlar sus dispositivos (desde sus ordenadores personales a brazos robóticos).</p>\n<!-- BREAK 4 -->\n<h2>Orwell no vio venir a Musk y Zuckerberg</h2>\n<p>En <a href=\"https://magnet.xataka.com/en-diez-minutos/la-gente-esta-tan-obsesionada-por-1984-que-el-libro-se-ha-convertido-otra-vez-en-un-bestseller\">la obra de George Orwell \'1984\'</a> se decía que, en el mundo distópico en que se ambienta la novela, \"nada era del individuo, a excepción de unos cuantos centímetros cúbicos dentro de su cráneo\". Pero incluso esa última frontera de la privacidad podría caer gracias a todos estos proyectos desarrollados últimamente en Silicon Valley.</p>\n<!-- BREAK 5 -->\n<div class=\"article-asset-summary article-asset-small\"><div class=\"asset-content\"><div class=\"article-asset-summary article-asset-small\"><div class=\"asset-content\"><p class=\"sumario_izquierda\">“Los datos del cerebro son el último refugio de nuestra privacidad. Cuando eso se acaba, se acaba todo. Y una vez que esos datos empiecen a recopilarse a un gran escala, será muy difícil de revertir el proceso\". (Marcelo Ienca)</p></div></div></div></div>\n<p>Esto está motivando a algunos científicos e intelectuales a denunciar que <strong>nuestras actuales leyes no están ni por asomo preparadas para las amenazas a las que estas tecnologías empiezan a abrir ahora las puertas</strong>. Estarían en juego, afirman, aspectos tan básicos de nuestras vidas que ni podemos pensar en ellos en términos de \'derechos\' legales.</p>\n<!-- BREAK 6 -->\n<p>Por ello, proponen <strong>llevar los derechos humanos también dentro de nuestros cráneos creando lo que ellos llaman \'neuroderechos\'</strong>. Dos académicos residentes en Suiza, el argentino Roberto Adorno (especialista en bioética) y el italiano Marcello Ienca (especialista en neuroética), llevan ya unos años defendiendo la necesidad de proteger los pensamientos y las memorias almacenados en los cerebros de los ciudadanos de toda clase de robo o \'hackeo\'.</p>\n<!-- BREAK 7 -->\n<p>Y han resumido esa necesidad de protección en cuatro nuevos derechos humanos:</p>\n<h2>Derecho a la libertad cognitiva</h2>\n<p>Ienca y Adorno defienden que los humanos tenemos el derecho a usar estas nuevas neurotecnologías, pero también a recibir <strong>protección contra el uso no consentido de éstas sobre nosotros</strong>.</p>\n<!-- BREAK 8 -->\n\n<h2>Privacidad mental</h2>\n<p>Si las webs y aplicaciones recopilan hoy en día tal cantidad de datos sobre sus usuarios que ha sido necesario someterlas a leyes de protección de los datos personales para evitar repercusiones negativas sobre estos, será necesario <strong>situar las nuevas BCIs bajo un status legal aún más estricto</strong>, por su capacidad para acceder a muchos más datos de naturaleza mucho más privada.</p>\n<!-- BREAK 9 -->\n<h2>Integridad mental</h2>\n<p>Definen así a la protección de la mente contra posibles daños. Argumentan que, si los ordenadores pueden ser hackeados, <strong>los nuevos dispositivos que Musk y Zuckerberg tratan de implantar en los cerebros de los usuarios pueden sufrir igual suerte</strong>, por lo que debemos estar protegidos antes accesos desautorizados o manipulación de señales neuronales que puedan derivar en daños psicológicos o físicos.</p>\n<!-- BREAK 10 -->\n<h2>Continuidad psicológica</h2>\n<p>Por último, Ienca y Adorno hablan del derecho a <strong>preservar la identidad personal y la coherencia del comportamiento individual</strong> frente a alteraciones realizadas sin permiso por terceras partes (por ejemplo, la modificación, adición o supresión de memorias \"esenciales para que un humano se reconozca a sí mismo\").</p>\n<!-- BREAK 11 -->\n<p>Vía | <a href=\"https://www.vox.com/2019/8/30/20835137/facebook-zuckerberg-elon-musk-brain-mind-reading-neuroethics\">Vox.com</a></p>\n<!-- BREAK 12 -->\n<p>Imagen | Pixabay</p>\n</div>', '2019-09-04 00:17:54', '2019-09-04 00:17:54', 49, 'portada005.jpg', 'Los nuevos proyectos de Zuckerberg y Elon Musk pueden llevar los hackeos y filtraciones de seguridad hasta nuestros mismos cerebros. Y algunos reivindican que eso obliga a legislar nuevas clases de derechos.');
INSERT INTO `posts` (`id`, `titulo`, `cuerpo`, `created_at`, `updated_at`, `user_id`, `imagen`, `descripcion`) VALUES
(47, 'Del porno cutre a películas alternativas modificadas por los fans: así han evolucionado los \'deepfakes\' en menos de dos años', '<div class=\"blob js-post-images-container\">\n<p>En otoño de 2017, un usuario de Reddit llamado <strong>\'deepfakes\' (\'falsificaciones profundas\')</strong> compartía varios vídeos que había creado insertando el rostro de personajes conocidos en cuerpos de otras personas. Rápidamente, <strong>el nombre del creador terminó siendo también el de la creación</strong>, y hoy en día esa clase de material multimedia (se ha extendido <a href=\"https://www.xataka.com/inteligencia-artificial/crecen-ataques-deepfakes-audio-suplantacion-ceos-este-sistema-hizo-perder-millones-a-tres-grandes-companias\">también al audio</a>) se conoce como deepfake.</p>\n<!-- BREAK 1 -->\n<p>Lo que tenían de particular es que no eran meros montajes (incluso en formato vídeo, éstos existían desde mucho antes), sino que <strong>en ellos el uso de la inteligencia artificial permitía adaptar las facciones de una persona al movimiento y expresiones de otra</strong>, de tal modo que el realismo del material daba como resultado algo nunca antes visto.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Aunque había varios que presentaban a Nicolas Cage dentro de películas en las que jamás había participado, los vídeos que atrajeron la atención de los medios y usuarios, una vez su fama se extendió en diciembre de 2017, fueron los protagonizados por actrices. <strong>Todos ellos, y de ahí gran parte de su repercusión, de contenido pornográfico</strong>.</p>\n<!-- BREAK 3 -->\n<div class=\"base-asset-media\"><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"200\" id=\"audio_32043162\" scrolling=\"no\" src=\"https://www.ivoox.com/player_ej_32043162_4_1.html?c1=ff6600\" style=\"border:1px solid #EEE; box-sizing:border-box; width:100%;\"></iframe></div>\n<p>Lo más \'populares\' de entre los creados por el usuario \'deepfake\' <strong>portaban los rostros de Gal Gadot y Daisy Ridley</strong>, pero también los había de Emma Watson, Katy Perry, Taylor Swift y Scarlett Johansson. Ésta última diría meses más tarde que la lucha contra esta clase de contenidos <a href=\"https://www.xataka.com/otros-dispositivos/scarlett-johansson-se-rinde-deep-fakes-pornograficos-basicamente-causa-perdida\">era ya una batalla perdida</a>.</p>\n<!-- BREAK 4 -->\n<h2>Esos primeros deepfakes</h2>\n<p>Aunque, ciertamente, a muchos de los primeros consumidores de <a href=\"https://www.xataka.com/robotica-e-ia/muchos-estan-prohibiendo-videos-deepfake-hay-compania-que-ha-empezado-comercializar-ellos\">esos primeros deepfakes porno</a> (acostumbrados a montajes fotográficos ciertamente mejorables realizados con Photoshop) sólo les importaba poder ver rostros famosos en escenas porno en movimiento, <strong>lo cierto es a que muchas de aquellas primeras escenas aún se las \'veían las costuras\'</strong>.</p>\n<!-- BREAK 5 -->\n<p>Así, <strong>los bordes del rostro no siempre terminaban de encajar bien y a veces \'temblaban\'</strong>, las cejas y la boca solían adquirir una apariencia extraña, aunque sólo fuera durante unos pocos segundos. Aquí un pequeño ejemplo:</p>\n<!-- BREAK 6 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Porno Deepfake\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/5f572f/porno_deepfake/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/5f572f/porno_deepfake/450_1000.jpg 450w, https://i.blogs.es/5f572f/porno_deepfake/650_1200.jpg 681w, https://i.blogs.es/5f572f/porno_deepfake/1024_2000.jpg 1024w, https://i.blogs.es/5f572f/porno_deepfake/1366_2000.jpg 1366w\"/><noscript><img alt=\"Porno Deepfake\" src=\"https://i.blogs.es/5f572f/porno_deepfake/450_1000.jpg\"/></noscript> <span> Fotogramas extraídos de vídeos deepfake para adultos \'protagonizados\' por las actrices Maisie Williams, Scarlett Johansson y Cobie Smulders. </span> </div></div></div>\n<p>Otro ejemplo más, esta vez en movimiento y comparando el material original con el \'deepfakeado\':</p>\n<div class=\"article-asset-embed-giphy article-asset-normal article-asset-center\">\n<div class=\"article-asset-video\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\" id=\"_giphy_L2kYeholL9sOSGfi74\">\n<iframe allowfullscreen=\"\" class=\"giphy-embed\" src=\"https://giphy.com/embed/L2kYeholL9sOSGfi74\" style=\"position:absolute\"></iframe>\n</div>\n</div>\n</div>\n</div>\n<p>Pero no todo va a ser porno, tranquilos. En los primeros meses de 2018, <strong>muchos internautas copiaron las técnicas basadas en IA de \'Deepfake\'</strong> (basadas en el uso del software <a href=\"https://www.xataka.com/inteligencia-artificial/tensorflow-software-google-lider-machine-learning-presenta-su-nueva-version-2-0-alpha-nuevo-modulo-privacidad\">Tensorflow</a>), y hubo un pequeño \'boom\' de creación de deepfakes usando también como material películas para todos los públicos.</p>\n<!-- BREAK 7 -->\n<p>El programador Sven Charleer creó, ya en enero de 2018, <a href=\"http://svencharleer.com/2018/02/02/family-fun-with-deepfakes-or-how-i-got-my-wife-onto-the-tonight-show/\">varios deepfakes</a> poniendo la cara de su esposa sobre el cuerpo de la actriz Anne Hathaway. No está mal hecho, pero <strong>el rostro producía todavía una sensación muy poco natural</strong>, típico de las creaciones de aquellas fechas:</p>\n<!-- BREAK 8 -->\n<div class=\"article-asset-embed-giphy article-asset-normal article-asset-center\">\n<div class=\"article-asset-video\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\" id=\"_giphy_UufYflZzUhPzO6j9Si\">\n<iframe allowfullscreen=\"\" class=\"giphy-embed\" src=\"https://giphy.com/embed/UufYflZzUhPzO6j9Si\" style=\"position:absolute\"></iframe>\n</div>\n</div>\n</div>\n</div>\n<p>Nicolas Cage. Un clásico de los fotomontajes antes, y de los deepfakes ahora. Pero en este vídeo (también de enero de 2018) <strong>el problema era el mismo: algo chirría con el rostro</strong>...</p>\n<!-- BREAK 9 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/vca_2IQHaR8\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p><em>BONUS:</em></p>\n<div class=\"article-asset-embed-giphy article-asset-normal article-asset-center\">\n<div class=\"article-asset-video\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\" id=\"_giphy_lly7mQZ1pOlmX2TeJV\">\n<iframe allowfullscreen=\"\" class=\"giphy-embed\" src=\"https://giphy.com/embed/lly7mQZ1pOlmX2TeJV\" style=\"position:absolute\"></iframe>\n</div>\n</div>\n</div>\n</div>\n<h2>Deepfakes en 2018-2019: más y mejores</h2>\n<p>Pero <strong>no tardaron en llegar vídeos mucho más impresionantes a nivel técnico</strong>, y que ayudaron a abrir el debate sobre <a href=\"https://www.xataka.com/robotica-e-ia/deepfakes-tendremos-problema-verdad-videos-serviran-como-pruebas\">qué clase de futuro le esperaba al vídeo como material probatorio</a> una vez que alcanzara cierto nivel de sofisticación. En una fecha tan temprana como abril de 2018, Buzzfeed publicaba este vídeo, con el que lograban <strong>aplicar el movimiento de labios de un actor a un vídeo institucional de Barack Obama</strong>, haciéndole decir cosas como \"Killmonger tenía razón\":</p>\n<!-- BREAK 10 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/cQ54GDm1eL0\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Pero no nos engañemos: por ahora (crucemos los dedos) los creadores de deepfakes <strong>prefieren seguir echándose unas risas remezclando a intérpretes de Hollywood antes que manipular políticos</strong> para hacer caer gobiernos. Ya en enero de  2019, un deepfake que <a href=\"https://www.xataka.com/robotica-e-ia/video-que-pone-rostro-steve-buscemi-cuerpo-jennifer-lawrence-recordatorio-peligro-deepfakes\">impresionó al público</a> fue éste de Jennifer Lawrence hablando ante los medios con el rostro de Steve Buscemi:</p>\n<!-- BREAK 11 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/r1jng79a5xc\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Parece que esto de poner caras masculinas sobre cuerpos femeninos es un subgénero bastante exitoso, porque sólo un mes después subían a Youtube este vídeo en el que <strong>Ellen DeGeneres y Amy Schumer asumían, respectivamente, el rostro del CEO de Tesla, Elon Musk, y del youtuber Pewdiepie</strong>:</p>\n<!-- BREAK 12 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/x9mVFrVe1kE\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Por esas mismas fechas, <strong>un usuario recurrió a esta tecnología para dejar en evidencia a los productores de \'La Liga de la Justicia\'</strong> por la chapuza de CGI realizada para borrar el bigote de Henry Cavill. Su mensaje es claro: cualquier PC del montón hubiera sido c<a href=\"https://www.xataka.com/robotica-e-ia/este-video-asegura-que-un-ordenador-de-500-dolares-elimina-el-bigote-de-superman-mejor-que-los-costosos-efectos-de-hollywood\">apaz de generar un resultado más digno</a>: </p>\n<!-- BREAK 13 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/2PZ3W1W20bk\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p><strong>Pero ha sido este mismo verano cuando la cosa parece haberse salido de control</strong>: llevamos unos meses en los que cada vez vemos deepfakes ingeniosos y \'currados\' en lo técnico, y en los que <strong>los artistas de este peculiar género empiezan a labrarse un nombre</strong> y a conseguir miles de seguidores.</p>\n<!-- BREAK 14 -->\n<p>Este vídeo, de agosto de 2019, y <strong>elaborado por el popular youtuber DrFakenstein</strong>, presenta a Ron Swanson (uno de los personajes de \'Parks &amp; Recreation\') como omnipresente okupa de la intro de \'Padres forzosos\'. Son imágenes, avisamos, ciertamente turbadoras:</p>\n<!-- BREAK 15 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/aUphMqs1vFw\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Otro youtuber, Ctrl Shift Face (<a href=\"https://www.xataka.com/inteligencia-artificial/ctrl-shift-face-autor-deepfakes-al-que-debemos-montaje-jim-carrey-protagonizando-resplandor\">del que ya os hablamos hace poco</a>) es el responsable de varios vídeos en los que el comediante de \'Saturday Night Live\' <strong>Bill Hader se va convirtiendo en varios actores famosos a medida que imita sus voces</strong>. En el más reciente de ellos, que os ponemos a continuación, Hader asume el rostro de Tom Cruise... pero también puede verle como <a href=\"https://www.youtube.com/watch?v=bPhUhypV27w\">Arnold Schwarzenegger</a> o <a href=\"https://www.youtube.com/watch?v=kjI-JaRWG7s\">Al Pacino</a>:</p>\n<!-- BREAK 16 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/VWrhRBb-1Ig\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Otra de las creaciones populares de Ctrl Shift Face es <a href=\"https://www.youtube.com/watch?v=AQvCmQFScMA\">su Terminator interpretado por Stallone</a> (que podéis ver en la imagen que encabeza el artículo), pero no podemos dejar de recordar <a href=\"https://www.youtube.com/playlist?list=PLmzEuib9Ve_JXVY4_8mxEe1ehB_CTflhS\">los 4 vídeos</a> en los que hace que <strong>Jim Carrey sustituya a Jack Nicholson como protagonista de \'El resplandor\'</strong>. Juzgad vosotros mismos el resultado del primero de ellos, de julio de 2019:</p>\n<!-- BREAK 17 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/HG_NZpkttXE\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>La siguiente es una de las últimas incorporaciones a esta epatante lista de deepfakes. A finales de agosto llegaba a Youtube este vídeo de <strong>Alison Brie con la cara de Jim Carrey</strong>:</p>\n<!-- BREAK 18 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/b5AWhh6MYCg\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>¿Qué futuro le espera, entonces, a esta tecnología? Hace 6 años, una Robin Wright que se interpretaba a sí misma en esa delirante joya de la ciencia ficción que es \'<a href=\"https://www.xataka.com/cine-y-tv/17-peliculas-de-ciencia-ficcion-para-visitar-si-te-gusto-la-llegada\">El Congreso</a>\' nos mostraba un futuro en el que los <strong>actores acabados vendían su imagen a las productoras para que ésta fuera escaneada y alterada sin su conocimiento</strong>, produciendo miles de nuevas películas a partir de ese momento. Aún estamos lejos de esos, pero el camino parece llevarnos, al menos, a que eso <strong>sea técnicamente posible</strong>.</p>\n<!-- BREAK 19 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/461d32/el_congreso/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/461d32/el_congreso/450_1000.jpg 450w, https://i.blogs.es/461d32/el_congreso/650_1200.jpg 681w, https://i.blogs.es/461d32/el_congreso/1024_2000.jpg 1024w, https://i.blogs.es/461d32/el_congreso/1366_2000.jpg 1366w\"/><noscript><img alt=\"\" src=\"https://i.blogs.es/461d32/el_congreso/450_1000.jpg\"/></noscript> <span> Fotograma del tráiler de \"El Congreso\". </span> </div></div></div>\n<p>Imagen | Crtl Shift Face (extraído de Youtube)</p>\n</div>', '2019-09-01 16:01:33', '2019-09-01 16:01:33', 11, 'portada006.jpg', 'Los deepfakes, videomontajes generados mediante inteligencia artificial, nacieron en un oscuro foro de Reddit en otoño de 2017. Menos de 48 después, repasamos todo el camino que han recorrido hasta un realismo casi total.'),
(48, 'Científicos del MIT crean robots con forma de hilo para facilitar el acceso a los vasos sanguíneos del cerebro', '<div class=\"blob js-post-images-container\">\n<p>El MIT ha creado <strong>un nuevo tipo de robot que no lo parece en absoluto</strong>: tiene forma de hilo y constituye una herramienta para las últimas técnicas de cirugía endovascular (es decir, la que opera dentro de los vasos sanguíneos).</p>\n<!-- BREAK 1 -->\n<p>Su particular forma está diseñada, precisamente, para facilitar el tratamiento de problemas en los vasos cerebrales vinculados a aneurismas o accidentes cerebrovasculares, <strong>una zona del cuerpo extremadamente delicada</strong> y fácil de dañar durante una intervención.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/INSyV4dgqu8\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Xuanhe Zhao, profesor asociado de ingeniería mecánica del MIT, explica que \"si el accidente cerebrovascular agudo pudiera ser tratado dentro de los primeros 90 minutos, <strong>las tasas de supervivencia de los pacientes podrían aumentar significativamente</strong>\", al evitar daños permanentes en el cerebro.</p>\n<!-- BREAK 3 -->\n<p>Esto es especialmente relevante <strong>en España, donde los accidentes cerebrovasculares (ictus) constituyen la segunda causa de muerte</strong> (la primera entre las mujeres), según <a href=\"https://fundaciondelcorazon.com/prensa/notas-de-prensa/2498-ictus-es-segunda-causa-muerte-en-nuestro-pais-y-primera-en-mujeres.html\">datos de la Fundación del Corazón</a>.</p>\n<!-- BREAK 4 -->\n<h2>Un dispositivo guiado por un imán y dotado de memoria de forma</h2>\n<p>El objetivo de los investigadores fue el de diseñar un dispositivo capaz de revertir la obstrucción de los vasos sanguíneos dentro de esta \'hora de oro\'. Así, este \'hilo-robot\' extraordinariamente fino, <strong>permite intervenciones mucho menos invasivas</strong> que las que se venían realizando hasta ahora.</p>\n<!-- BREAK 5 -->\n\n<p>Además, facilita el trabajo a los cirujanos (que ya no serán los responsables de guiar el cable manualmente) y resulta más saludable para los pacientes (que <strong>no tendrán que estar expuestos a la radioactividad de los rayos X</strong> durante toda la intervención).</p>\n<!-- BREAK 6 -->\n<p>El núcleo del robot (<strong>compuesto de una aleación plegable de níquel-titanio con memoria de forma</strong>) se ha recubierto de una capa de partículas magnéticas (que permiten guiarlo a distancia con el uso de un imán gigante), recubiertas a su vez de un hidrogel que permite al robot deslizarse por los vasos sanguíneos <strong>sin provocar fricciones</strong>.</p>\n<!-- BREAK 7 -->\n<p>Los investigadores del MIT están probando ya a alterar este prototipo, <strong>cambiando el níquel-titanio por fibra óptica</strong> que permita proyectar el haz láser con el que eliminar las obstrucciones de los vasos cerebrales.</p>\n<!-- BREAK 8 -->\n<p>Vía | <a href=\"https://news.mit.edu/2019/robot-brain-blood-vessels-0828\">MIT News</a></p>\n<p>Imagen | MIT</p>\n</div>', '2019-08-30 21:43:21', '2019-08-30 21:43:21', 4, 'portada007.jpg', 'Los investigadores han logrado crear un robot extremadamente fino que, guiado por un imán y dotado de memoria de forma, permite realizar complicadas cirugías en los vasos cerebrales con los que hacer frente a aneurismas e ictus.'),
(49, 'OpenAI defiende la publicación \'incremental\' de su generador de textos fake y lanza una versión equivalente al 50% de la completa', '<div class=\"blob js-post-images-container\">\n<p>El pasado mes de febrero, OpenAI (un laboratorio sin ánimo de lucro fundado por Elon Musk) alcanzaba los titulares de la prensa generalista <a href=\"https://www.xataka.com/inteligencia-artificial/gpt-2-que-sabemos-que-no-generador-textos-ia-que-openai-dice-haber-censurado-ser-demasiado-peligroso\">gracias a la creación de GPT-2</a>, una inteligencia artificial capaz de generar noticias falsas en formato texto, <strong>dotada de tal nivel de realismo que se negaron a difundirla</strong>, como es habitual, para evitar que una herramienta tan \"peligrosa\" cayera en malas manos y pudiera ser usada con fines de \"desinformación o propaganda\".</p>\n<!-- BREAK 1 -->\n<p>En ese momento, la comunidad de investigadores de inteligencia artificial se dividió en torno a la decisión adoptada por OpenAI: si bien algunos celebraron las precauciones tomadas, <strong>otros lo consideraron un truco publicitario y sensacionalista</strong>. Ese fue el caso, por ejemplo, de Anima Anandkumar, directora de investigación de Nvidia, que se enzarzó en un debate tuitero con uno de los directivos de OpenAI:</p>\n<!-- BREAK 2 -->\n<!--more-->\n<blockquote>\n<p>\"Es lamentable que exageréis, propaguéis el miedo y desbaratéis tanto la reproducibilidad de resultados como el esfuerzo científico. [...] Estáis exagerando como nunca antes se había hecho. ¿Qué chorrada es ésa de \'malicioso\'? No le hacéis ningún favor a la ciencia usando esa palabra\".</p>\n</blockquote>\n\n<h2>OpenAI ha cambiado su estrategia... pero sus críticos creen que no lo suficiente</h2>\n<p>Ahora, seis meses después, OpenAI ha publicado un documento (elaborado junto a otras instituciones de investigación) que examina el impacto que su decisión ha tenido hasta ahora, a<strong>compañado de una versión incompleta del GPT-2, de la mitad de tamaño que la completa</strong> (aún inédita).</p>\n<!-- BREAK 3 -->\n<p>En febrero habían publicado una versión de demostración, de <strong>apenas el 8% del tamaño de la versión completa</strong>. Más tarde, ya en mayo, OpenAI decidió <strong>revisar su postura de no publicación en favor de una \"liberación por etapas\"</strong> del modelo de GPT-2.</p>\n<!-- BREAK 4 -->\n<p>Según el documento, estas decisiones han permitido que las instituciones asociadas a OpenAI pudieran cuantificar mejor algunas de las amenazas vinculadas a GPT-2 que antes sólo constituían meras especulaciones: un estudio realizado por investigadores de la Universidad de Cornell, por ejemplo, descubrió que <strong>los lectores identificaban como legítimos en un porcentaje muy similar tanto los textos de GPT-2 como los extraídos del New York Times</strong>.</p>\n<!-- BREAK 5 -->\n\n<p>El documento, sin embargo, viene a reconocer que los <a href=\"https://www.xataka.com/inteligencia-artificial/estudiante-logra-replicar-ia-generadora-fake-news-que-openai-se-nego-a-publicar-amenaza-publicarla-1-julio\">esfuerzos exitosos para replicar GPT-2</a> <strong>convierten en \"discutible\" lo acertado de la retención del código</strong>. Pese a la existencia de esas réplicas, lo cierto es que <strong>no se han detectado intentos de usos maliciosos de la misma</strong>, sino múltiples aplicaciones derivadas útiles dedicadas a campos como el autocompletado de código, la ayuda gramatical o el desarrollo de sistemas de preguntas y respuestas para asistencia médica.</p>\n<!-- BREAK 6 -->\n<p>Otras voces, sin embargo, se han seguido alzando para criticar la estrategia de OpenAI: Vanya Cohen, una estudiante recién graduada que desarrolló hace poco una de las réplicas de GPT-2, argumenta que <strong>retrasar la publicación del modelo completo no ha contribuido más que a retrasar la investigación de contramedidas</strong> que permitan <a href=\"https://www.xataka.com/inteligencia-artificial/posible-detectar-noticias-falsas-creadas-inteligencia-artificial-que-openai-no-quiso-publica\">detectar los textos falsificados</a>.</p>\n<!-- BREAK 7 -->\n<p>Vía | <a href=\"https://www.technologyreview.com/s/614237/openai-released-its-fake-news-ai-gpt-2/\">MIT Technology Review</a></p>\n<!-- BREAK 8 -->\n<p>Imagen | Logan Ingalls (<a href=\"https://www.flickr.com/photos/plutor/847695350\">Flickr</a>)</p>\n</div>', '2019-08-30 14:31:17', '2019-08-30 14:31:17', 32, 'portada008.jpg', 'En febrero, OpenAI no quiso publicar más que una minúscula versión demo de su generador de textos \'fake\', GPT-2, por su \"peligrosidad\". Luego decidió liberar cada cierto tiempo versiones cada vez más completas de su IA. Y ya va por el 50%.'),
(50, 'Investigadores de Facebook utilizan Minecraft para entrenar un nuevo asistente digital dotado de inteligencia artificial', '<div class=\"blob js-post-images-container\">\n<p>Los investigadores de Facebook Research desean crear un asistente digital avanzado que, al contrario de lo habitual en el campo de la inteligencia artificial, no ofrezca un rendimiento sobrehumano circunscrito a una única tarea, <strong>sino uno que sea competente gestionando un amplio abanico de las mismas</strong>. Y no sólo eso, sino que sea capaz de aprender nuevas tareas a medida que interactúa con los seres humanos.</p>\n<!-- BREAK 1 -->\n<p>Y la manera en que piensan lograrlo es integrando este asistente en <a href=\"https://www.xataka.com/analisis/minecraft-el-objeto-cultural-minecraft-el-videojuego\">el popular videojuego Minecraft</a> (<strong>el más vendido de todos los tiempos, con más de 170 millones de copias</strong>, y más de 90 millones de usuarios mensuales). Este juego, de tipo \'sandbox\', permite explorar libremente un amplio mundo virtual y explotar sus recursos naturales, crear herramientas y construir toda clase de estructuras usando bloques.</p>\n<!-- BREAK 2 -->\n<!--more-->\n\n<p>Según explican en el \'paper\' académico <a href=\"https://arxiv.org/abs/1907.09273\">publicado en arXiv</a>, los investigadores eligieron Minecraft porque <strong>ofrece un número casi ilimitado de posibles tareas, pero lo hace dentro de un mundo con reglas simples</strong> y, hasta cierto punto, predecibles.</p>\n<!-- BREAK 3 -->\n<p>Tanto, que los investigadores de inteligencia artificial ya están empezando a <strong>usar este juego como plataforma para entrenar y probar otros de sistemas de IA</strong> distintos, igual que hace 4 años <a href=\"https://www.xataka.com/robotica-e-ia/minecraft-es-el-entrenamiento-perfecto-para-que-estos-robots-aprendan-a-resolver-problemas\">se usaba para entrenar robots</a>.</p>\n<!-- BREAK 4 -->\n<blockquote>\n<p>Las condiciones del entorno permiten que la ejecución de una tarea sea sencilla una vez que ésta se especifica, permitiendo que [los movimientos a realizar] e incluso la exploración y combate pueden ser razonablemente \'programados\'.</p>\n<p>\"Los jugadores de Minecraft son creativos, y la diversidad de objetos construidos por ellos en el juego es asombrosa [...] Incluyen monumentos, esculturas, templos, montañas rusas y paisajes urbanos completos.</p>\n</blockquote>\n<h2>El asistente digital</h2>\n<p>El objetivo perseguido por Arthur Szlam y sus compañeros de Facebook Research es que <strong>podamos pedirle al asistente, mediante órdenes verbales, que construya las estructuras que necesitemos</strong> en cada momento. Ése es otro de los obstáculos a los que tendrá que hacer frente el asistente: las complejidades del lenguaje coloquial, y la deficiente capacidad de los humanos para explicar en qué clase de estructuras concretas está pensando en un momento dado.</p>\n<!-- BREAK 5 -->\n\n<p>Una frase aparentemente sencilla como \"Construye una torre de 20 bloques de altura y sitúa un smiley gigante en la cima\" <strong>requiere en realidad de un grado de conocimientos notable</strong>, de una comprensión de la estructura de la frase, de un sonido claro que permite identificar bien las palabras, etc.</p>\n<!-- BREAK 6 -->\n<p>Y luego queda lo más difícil: <strong>el asistente debe ser capaz de comprender el concepto \"torre\", saber cómo construirla</strong> y entender a qué se refiere su interlocutor con las expresiones \"bloques de altura\", \"smiley\" o \"cima de la torre\"; y no olvidemos identificar a cuál de ellas se vincula cada una de las cifras mencionadas (\'20\' en este caso). Todo eso es lo que Szlam busca conseguir con este \'entrenamiento\' en Minecraft.</p>\n<!-- BREAK 7 -->\n<p>Vía | <a href=\"https://www.technologyreview.com/s/614182/facebook-is-creating-an-ai-assistant-for-minecraft/\">MIT Technology Review</a></p>\n<!-- BREAK 8 -->\n<p>Imagen | <a href=\"https://commons.wikimedia.org/wiki/File:Papercraft_Minecraft_(9579580151).jpg\">interestedbystandr</a> (Wikimedia)</p>\n<!-- BREAK 9 --> </div>', '2019-08-30 12:01:13', '2019-08-30 12:01:13', 6, 'portada009.jpg', 'Un proyecto de Facebook Research ha creado un asistente digital que entrena sus inteligencia artificial resolviendo una amplia gama de tareas dentro de \'Minecraft\'.'),
(51, 'Los mejores momentos del debate entre Jack Ma y Elon Musk sobre inteligencia artificial en la WAIC 2019', '<div class=\"blob js-post-images-container\">\n<p>Dos de los más destacados emprendedores tecnológicos del mundo, el sudafricano Elon Musk (CEO de Tesla) y el chino Jack Ma (CEO de Alibaba) han coincidido esta mañana, en el marco de la WAIC (World Artificial Intelligence Conference) que se inauguraba hoy en Shanghai, para <strong>debatir sobre el futuro de la inteligencia artificial</strong>.</p>\n<!-- BREAK 1 -->\n<p>El debate, que duró 45 minutos (incluyendo las preguntas del público), <strong>reunió a ejecutivos de cerca de 300 compañías</strong> (IBM, Microsoft, Intel y Qualcomm entre ellas) y a investigadores científicos de todo el mundo.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Esta tecnología ha tenido un papel central en la estrategia de negocio de ambos, pero cada uno de ellos <strong>llegaba a este evento con una visión bien distinta</strong> sobre qué podemos esperar de su evolución a corto y largo plazo.</p>\n<!-- BREAK 3 -->\n<p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">This clip sums up the <a href=\"https://twitter.com/hashtag/WAIC2019?src=hash&amp;ref_src=twsrc%5Etfw\">#WAIC2019</a> debate.<br/><br/>Jack: \"Computer may be clever, but human being are much smarter\". <br/><br/>Elon: \"Yeah, definitely not\"???? <a href=\"https://twitter.com/elonmusk?ref_src=twsrc%5Etfw\">@elonmusk</a> <a href=\"https://t.co/SHLjfv2ZvR\">pic.twitter.com/SHLjfv2ZvR</a></p>— Sofiaan Fraval (@Sofiaan) <a href=\"https://twitter.com/Sofiaan/status/1166946310320058369?ref_src=twsrc%5Etfw\">August 29, 2019</a></blockquote> <script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script></p>\n<!-- BREAK 4 -->\n<h2>En el lado tecno-optimista del ring: Jack Ma</h2>\n<p><a href=\"https://www.xataka.com/historia-tecnologica/jack-ma-de-profesor-de-ingles-a-magnate-de-internet-sin-pasar-por-silicon-valley\">Jack Ma, fundador de Alibaba</a>, la compañía de comercio electrónico líder en Asia, capaz de hablarle de tú a tú a Amazon, siempre s<strong>e ha caracterizado por un planteamiento fundamentalmente optimista respecto a la IA</strong>.</p>\n<!-- BREAK 5 -->\n<p>Ma la ve como u<strong>n agente de cambio para la sociedad digital, tan positivo como inevitable</strong>: \"La mayoría de las proyecciones sobre la inteligencia artificial están equivocadas: la gente que está familiarizada con ella no le tiene miedo\".</p>\n<!-- BREAK 6 -->\n<blockquote>\n<p>\"Gracias a la IA, la gente tendrá más tiempo para disfrutar de sus vida. Olvidaos de las largas jornadas de trabajo: podríamos terminar teniendo semanas laborales de 12 horas. No me preocupa demasiado el impacto de la IA sobre los empleos... en el futuro no necesitaremos una gran cantidad de puestos de trabajo\".</p>\n</blockquote>\n<p>\"Creo que la IA puede ayudarnos a entender mejor al resto de seres humanos. Creo que no es una amenaza\". Musk no pudo menos que responder a eso: \"Hombre, no sé, <strong>eso ha sonado a célebres últimas palabras</strong>\".</p>\n<!-- BREAK 7 -->\n\n<h2>... en el lado pesimista del ring. Elon Musk</h2>\n<p>Musk, por el contrario, ha estado a la cabeza de los que vaticinan que los seres humanos podemos terminar siendo dominados por la IA (ha llegado a hablar de que <a href=\"https://www.xataka.com/robotica-e-ia/elon-musk-vuelve-inteligencia-artificial-peligro-concentracion-poder-que-humanos-acaben-como-mascotas\">seremos \"sus mascotas\"</a>), y define a esta tecnología como nuestra \"mayor amenaza existencial\"</p>\n<!-- BREAK 8 -->\n<blockquote>\n<p>\"Los seres humanos pueden llegar a ser demasiado lentos, y una milésima de segundo es una eternidad para un ordenador hoy en día. Los ordenadores son ya más inteligentes que nosotros en muchos aspectos\".</p>\n</blockquote>\n<p>Entonces <strong>le llegó a Jack Ma su turno de responder con ironía</strong>: \"Puede que sean más inteligentes, pero nosotros somos más listos: fuimos nosotros quienes inventamos los ordenadores, pero aún no he visto a un ordenador inventar un ser humano\". E incluso <strong>optó por ponerse poético</strong>: \"Los ordenadores sólo tienen chips, los humanos tenemos corazones\".</p>\n<!-- BREAK 9 -->\n<p>Pero <strong>esta retórica no pareció convencer a Musk</strong>, quien no dio su brazo a torcer y vaticinó que la IA terminaría haciendo algo muy similar: <strong>sustituirnos a nosotros como autores de su código de programación</strong>.</p>\n<!-- BREAK 10 -->\n\n<h2>\"¿Dónde están los aliens?\"</h2>\n<p>Cuando Musk y Ma se salieron del debate estricto sobre la IA para tocar otros temas que iban saliendo en la conversación, aparecieron <strong>los primeros acuerdos</strong>. El primero giró en torno al colapso demográfico. Musk defendió, entre asentimientos de su interlocutor que</p>\n<!-- BREAK 11 -->\n<blockquote>\n<p>\"La mayoría de la gente cree que tenemos demasiadas personas en el planeta. Se trata de una visión anticuada: suponiendo que la IA siga siendo benévola, el mayor problema que enfrentará el mundo de aquí a 20 años será el colapso demográfico. No el \'boom\', sino el colapso\".</p>\n</blockquote>\n<p>Pero <strong>los derroteros por los que Musk llevó luego la conversación no convencieron a Ma</strong>. No sólo se preguntó varias veces <strong>por qué aún no habíamos encontrado aliens</strong>, ante la extrañeza de Jack Ma y el resto del auditorio (\"No parece que ambos estén en la misma longitud de onda\", <a href=\"https://weibointl.api.weibo.cn/share/89585639.html?weibo_id=4410621708145218\">comentaba</a> un usuario en Weibo), sino que aprovechó para afirmar que necesitábamos más gente... para mandarla al espacio. </p>\n<!-- BREAK 12 -->\n<blockquote>\n<p>\"No tenemos mucho tiempo. Esta es la primera vez en los 4.500 millones de años de historia de nuestro planeta que vamos a ser capaces de extender la vida terrestre fuera del mismo. Debemos asegurar su futuro, para que la luz de la conciencia no se extinga\".</p>\n</blockquote>\n<p>Llegados a este punto, <strong>Jack Ma optó por cuestionar los esfuerzos que Musk está llevando a cabo para llevarnos a Marte</strong> con su compañía SpaceX: \"Necesitamos un héroe como tú, pero necesitamos aún más héroes dispuestos a mejorar las cosas aquí en la Tierra\".</p>\n<!-- BREAK 13 -->\n\n<h2>La guerra comercial no beneficia a nadie</h2>\n<p>Otro aspecto en el que no discreparon los ponentes fue en lo relativo a la situación de enfrentamiento entre sus respectivos países. A principios de esta semana, Ma se había pronunciado con respecto a <a href=\"https://www.xataka.com/robotica-e-ia/eeuu-vs-china-guerra-no-solo-comercial-tambien-tecnologica-despeja-x-1x15\">la guerra comercial y tecnológica entre su país y los EE.UU.</a>. Según él, incluso en este contexto, <strong>ambos países debían seguir trabajando codo con codo para lograr que todo el mundo pueda beneficiarse de los avances tecnológicos</strong>.</p>\n<!-- BREAK 14 -->\n<p>Y volvió a sostener ese punto de vista durante el debate: \"En esta Era \'Smart\', es casi imposible que alguien pueda actuar por su cuenta. Sólo si China y los EE.UU. trabajan juntos en este campo podremos entrar en la era digital juntos\".</p>\n<!-- BREAK 15 -->\n<p><strong>La guerra comercial tampoco beneficia a Tesla</strong>, y ha habido medios que han interpretado la presencia de Musk en el evento (financiado y respaldado por el gobierno chino) como un mensaje a Washington al respecto.</p>\n<!-- BREAK 16 -->\n<p>Pero, al fin y al cabo, <strong>China es el segundo mayor mercado de Tesla después de EE.UU.</strong> (habiendo experimentado durante el primer semestre un aumento interanual del 40% en sus ingresos en dicho mercado). Y, además, la compañía de coches eléctricos se encuentra ahora mismo <strong>construyendo su primera fábrica en el extranjero en la zona de libre comercio de Shangai</strong>.</p>\n<!-- BREAK 17 -->\n<p>Vía | <a href=\"https://www.abacusnews.com/big-guns/jack-ma-and-elon-musk-give-conflicting-visions-ais-future/article/3024903\">Abacus News</a></p>\n<!-- BREAK 18 -->\n<p>Imagen | Extraída de Youtube (CGTN)</p>\n</div>', '2019-08-29 23:45:11', '2019-08-29 23:45:11', 29, 'portada0010.jpg', 'Los fundadores de Tesla y Alibaba, con posicionamientos contrapuestos sobre el futuro de la IA, han estado debatiendo hoy al respecto en Shanghai: \"—Creo que la IA no es una amenaza. —Eso ha sonado a célebres últimas palabras\".'),
(52, 'Los usuarios del programa piloto de taxis autónomos de Waymo en Silicon Valley aún valoran negativamente la mitad de sus viajes', '<div class=\"blob js-post-images-container\">\n<p>El pasado mes de diciembre, <strong>Waymo</strong> (la compañía desarrolladora de vehículos autónomos vinculada a Google) <a href=\"https://www.xataka.com/vehiculos/taxis-autonomos-waymo-one-inician-su-andadura-plantean-revolucion-transporte\">puso en marcha el primer servicio de taxis autónomos</a> del mundo bajo el nombre de Waymo One. <strong>Su propósito era el \"competir directamente con Uber y Lyft\"</strong>, compañías líderes del transporte de pasajeros bajo demanda en EE.UU., que ya se encuentran desarrollando sus propias soluciones basadas en coches autónomos.</p>\n<!-- BREAK 1 -->\n<p>Pese a eso, Waymo quiso empezar a desplegar su servicio con cautela, concentrándose primero en una única ciudad: Phoenix (y sus alrededores). <strong>En los últimos meses han empezado a operar también en la zona de Silicon Valley</strong>, aunque en este caso sólo acepta como pasajeros a sus propios empleados, a los que se anima a ser sinceros en sus valoraciones.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p><strong>Los viajes se realizan en su mayor parte con conductores de seguridad</strong>, que rara vez tienen que poner las manos en el volante, y en algún caso se han llegado a ofrecer viajes <a href=\"https://www.xataka.com/automovil/el-coche-autonomo-de-google-waymo-se-vuelve-completamente-autonomo-y-por-primera-vez-sale-a-la-calle-sin-conductor\">100% sin conductor</a>, si bien dentro de áreas geográficas muy delimitadas.</p>\n<!-- BREAK 3 -->\n\n<h2>Datos de 10.500 trayectos sugieren un problema de Waymo con cierto tipos de diseño urbano</h2>\n<p>Ahora, el diario <a href=\"https://www.theinformation.com/articles/waymos-backseat-drivers-confidential-data-reveals-self-driving-taxi-hurdles\">The Information</a> <strong>ha tenido acceso a las reseñas anonimizadas de 10.500 trayectos realizados entre julio y agosto</strong> en ambas localizaciones (6.100 de ellos en Phoenix), unos datos que ofrecen algunas pistas sobre la evolución de la compañía que, teóricamente, cuenta con la tecnología de conducción autónoma más avanzada a día de hoy.</p>\n<!-- BREAK 4 -->\n<p>Y se ha encontrado con que, si bien el 30% de esas reseñas contenía algún tipo de crítica, el porcentaje <strong>se situaba en el 47% cuando se analizan únicamente los datos de Silicon Valley</strong>.</p>\n<!-- BREAK 5 -->\n<p>En los comentarios, los viajeros <strong>se quejan, principalmente, de las \"rutas tortuosas\", del \"frenado excesivo\" y de la \"conducción inestable\" de los taxis autónomos</strong>. De hecho, uno de los explícitos mensajes recogidos de forma literal por The Information reza así: \"¡Este viaje fue una mierda! Incómodo y verdaderamente aterrador\".</p>\n<!-- BREAK 6 -->\n<p>Y eso, escrito por un empleado de la propia compañía. Otro empleado se queja de los constantes y repentinos frenados \"llegaron a provocarle mareos\".</p>\n<!-- BREAK 7 -->\n\n<p>Por último, <strong>hasta un 7% de las reseñas se quejan de la excesiva distancia con que los taxis les dejaban con respecto a su destino real</strong>, de entre 15 y 900 metros, obligándolos a caminar o a llamar a otro taxi para finalizar el trayecto.</p>\n<!-- BREAK 8 -->\n<p>Es interesante analizar a qué puede deberse la diferencia de valoraciones entre Phoenix y Silicon Valley, pues da una pista de los retos a los que se enfrentan esta clase de vehículos a corto plazo. Podría deberse, es cierto, al distinto perfil de los viajeros, al ser los de la segunda área expertos de la industria específicamente motivados para hacer reseñas exhaustivas. Pero <strong>hay una segunda explicación posible: el urbanismo</strong>.</p>\n<!-- BREAK 9 -->\n<p>Y es que los municipios de Silicon Valley (San Francisco, Mountain View, Sunnyvale, Los Altos y Palo Alto) presentan un diseño urbano más denso, con vías más estrechas y mayor presencia de peatones y ciclistas que en Phoenix. Es decir, un desafío mayor para la tecnología de navegación automática de Waymo... y <strong>la clase de diseño urbano más frecuente fuera de los Estados Unidos</strong>, por lo que resolver este reto supondrá un requisito antes de emprender cualquier intento de expansión internacional ed Waymo One.</p>\n<!-- BREAK 10 -->\n<p>Vía | <a href=\"https://futurism.com/the-byte/riders-waymo-self-driving-cars-complaining\">Futurity</a></p>\n<!-- BREAK 11 -->\n<p>Imagen | Waymo</p>\n</div>', '2019-08-29 01:29:03', '2019-08-29 01:29:03', 11, 'portada0011.jpg', 'Viajes inolvidables en el mal sentido de la palabra, por \"frenados contantes y excesivos\" y por \"conducción inestable\": esa es la experiencia del 47% de los usuarios del servicio de taxis autónomos de Waymo en San Francisco y alrededores.');
INSERT INTO `posts` (`id`, `titulo`, `cuerpo`, `created_at`, `updated_at`, `user_id`, `imagen`, `descripcion`) VALUES
(53, 'Así evitan que el rostro de un muerto o una mano cortada se usen para superar sistemas de identificación biométrica', '<div class=\"blob js-post-images-container\">\n<p>La semana pasada <a href=\"https://gizmodo.com/murder-suspect-caught-after-ai-detected-victims-face-wh-1837507103\">se hizo público</a> que la policía había detenido a un hombre del sureste de China como sospechoso del asesinato de su novia: tras discutir sobre dinero, él la habría estrangulado y habría utilizado el móvil de ella para enviar mensajes a sus familiares avisándoles de que estaría unos días de viaje. Pero el asesino hizo algo más: intentó usar la app Money Station para solicitar un préstamo a nombre de la fallecida... <strong>utilizando su cadáver como forma de identificarse en su cuenta, al requerir reconocimiento facial</strong>. </p>\n<!-- BREAK 1 -->\n<!--more-->\n<p>Y ése fue su gran error, porque la app cuenta con un sistema de seguridad que requiere que el usuario parpadee y hable durante el proceso de identificación. <strong>Al fallar el login, por ausencia de movimiento ocular en la supuesta usuaria</strong>, los empleados de la compañía de préstamos verificaron manualmente qué había pasado y se encontraron con que el rostro de la imagen presentaba magulladuras y marcas rojas en el cuello, y que la voz que sonaba era de hombre. Todo esto se puso en conocimiento de la policía china, que detuvo al presunto asesino a comienzos de este mes.</p>\n<!-- BREAK 2 -->\n\n<h2>El gran agujero de seguridad de los sistemas biométricos</h2>\n<p>A medida que los sistemas de identificación biométrica van aumentando su presencia en nuestro día a día, se hace cada vez más necesario contar con esta clase de tecnología dedicada a detectar los casos en los que <strong>el rostro u huella dactilar que trata de identificarse podría podrían no ser los de alguien que actúa voluntariamente</strong> (por estar fallecido, dormido o inconsciente): esto son los que <strong>se conocen como sistemas de detección de la \'vivacidad\'</strong> (\'liveness\' en inglés).</p>\n<!-- BREAK 3 -->\n<p>La popularización de cualquier tecnología aumenta inevitablemente la cantidad de intentos de manipularla. Y esa manipulación puede ser <strong>tan sencilla como el niño que desbloquea el móvil de su padre pasando su dedo por la pantalla mientras éste se echa la siesta</strong>, o tal compleja como recurrir a la <a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-inteligencia-artificial-antagonica-como-puede-manipular-a-otras-ias\">inteligencia artificial antagónica</a> para convencer a un sistema de identificación facial de que eres alguien que no eres. O también existe la opción, claro, de <a href=\"https://www.xataka.com/seguridad/este-modelo-3d-hiperrealista-ha-conseguido-enganar-a-face-id-reconocimiento-facial-iphone\">utilizar máscaras ultrarrealistas</a>.</p>\n<!-- BREAK 4 -->\n<h2>Comprobaciones de vivacidad en el reconocimiento facial</h2>\n<div class=\"article-asset-image article-asset-small\"><div class=\"asset-content\"> <img alt=\"Liveness Detection\" class=\"sf-lazy izquierda_sinmarco\" data-sf-src=\"https://i.blogs.es/3849ec/liveness-detection-pad-presentation-attacks/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/3849ec/liveness-detection-pad-presentation-attacks/450_1000.jpg 450w, https://i.blogs.es/3849ec/liveness-detection-pad-presentation-attacks/650_1200.jpg 681w, https://i.blogs.es/3849ec/liveness-detection-pad-presentation-attacks/1024_2000.jpg 1024w, https://i.blogs.es/3849ec/liveness-detection-pad-presentation-attacks/1366_2000.jpg 1366w\"/><noscript><img alt=\"Liveness Detection\" src=\"https://i.blogs.es/3849ec/liveness-detection-pad-presentation-attacks/450_1000.jpg\"/></noscript> </div></div>\n<p>En 2012, Google realizó su primer intento de introducir el desbloqueo facial en Android: fue rápidamente sorteado con sólo poner ante la cámara una simple foto, por lo que la compañía decidió <strong>establecer por primera vez un sistema de \"detección de la vivacidad\" que obligaba a los usuarios a parpadear</strong>. De nuevo, sólo hubo que cambiar una foto fija por una animación para lograr el efecto deseado y desbloquear el móvil.</p>\n<!-- BREAK 5 -->\n<p>De modo que, sólo un año más tarde, Google presentaba su <a href=\"https://nakedsecurity.sophos.com/2013/06/12/google-files-patent-to-let-you-unlock-your-phone-by-grimacing-at-it/\">primera patente</a> de una tecnología de \"detección de la vivacidad\", que <strong>exigía aleatoriamente al usuario la realización de muecas y gestos</strong> que podían ir de \"fruncir el ceño\" a \"sonreír\", \"sacar la lengua\" o \"mover las cejas\". A partir de entonces, cientos de compañías <strong>han implementado sus propios sistemas de detección de vivacidad, sobre todo en el ámbito de la banca</strong>.</p>\n<!-- BREAK 6 -->\n<p>Sin embargo, han tenido que añadir nuevas comprobaciones, puesto que las basadas en la realización de gestos pueden ser eludidas recurriendo -por ejemplo- a sistemas de inteligencia artificial capaces de proyectar el gesto deseado, por lo que cada vez es más habitual recurrir a comprobaciones de la cartografía 3D del rostro, <strong>capaces de asegurar que lo que la IA biométrica está \'viendo\' no es una simple proyección o una foto sostenida ante la cámara</strong>, sino que efectivamente hay una persona ante la cámara.</p>\n<!-- BREAK 7 -->\n\n<p><strong>Pero estos sistemas siguen teniendo vulnerabilidades inesperadas</strong>. El mes pasado, durante la conferencia de ciberseguridad Black Hat celebrada en la ciudad de Las Vegas (EE.UU.), el investigador de Tencent Bin Ma realizó una conferencia  en las que repasaba las vulnerabilidades de los vigentes sistemas de autenticación biométrica.</p>\n<!-- BREAK 8 -->\n<p>Ma y sus compañeros descubrieron que <strong>el sistema de detección de vivacidad del Face ID de Apple ignora información relevante de los rostros cuando detecta que éstos portan gafas</strong>, lo que le permite ser manipulando usando sencillamente unas gafas con cinta negra y un pequeño punto blanco sobre cada lente, como se puede ver en este tuit:</p>\n<!-- BREAK 9 -->\n<p><blockquote class=\"twitter-tweet\"><p dir=\"ltr\" lang=\"en\">Security researchers demonstrate how to bypass Face ID with glasses and tape <a href=\"https://t.co/sr5Mtt81E7\">https://t.co/sr5Mtt81E7</a> by <a href=\"https://twitter.com/ChanceHMiller?ref_src=twsrc%5Etfw\">@ChanceHMiller</a> <a href=\"https://t.co/PMxmF4BcTg\">pic.twitter.com/PMxmF4BcTg</a></p>— 9to5Mac.com (@9to5mac) <a href=\"https://twitter.com/9to5mac/status/1159624781760798721?ref_src=twsrc%5Etfw\">August 9, 2019</a></blockquote> <script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script></p>\n<!-- BREAK 10 -->\n<h2>Comprobaciones de vivacidad en la identificación de huellas dactilares</h2>\n<p>Para evitar accesos no autorizados, los escáneres incluyen <strong>sensores de temperatura corporal</strong> que sólo funcionan con el dedo desnudo. Tanto el uso de cadáveres como el de manos falsas de plástico quedarían descartados. También suelen incluir detectores de presión, aunque eso está destinado más bien a evitar desbloqueos involuntarios de móviles por parte del usuario.</p>\n<!-- BREAK 11 -->\n\n<p>Además, como una versión mejorada de este sistema, los lectores de huellas dactilares más modernos, como el Touch ID de Apple, <strong>reaccionan ante la electricidad generada por un cuerpo vivo</strong> y producida bajo la epidermis, una actividad que desaparece pocos minutos después de fallecer, lo que impide usar el dedo de una persona muerta para desbloquear su terminal móvil.</p>\n<!-- BREAK 12 -->\n<h2>Comprobaciones de vivacidad en sistemas basados en el reconocimiento ocular</h2>\n<p><strong>Las manos cortadas o de plástico no son menos frecuentes en las tramas de ficción que sus equivalentes oculares</strong>, y los científicos también han pensado en medidas para evitar que la inteligencia artificial dé por bueno, por ejemplo, el escaneo de un globo ocular situado fuera de su correspondiente cuenca.</p>\n<!-- BREAK 13 -->\n\n<p>Además de la ya mencionada medida de esperar a que el ojo parpadee, a la hora de identificar un iris o retina las medidas de seguridad incluyen por ejemplo:</p>\n<!-- BREAK 14 -->\n<ul>\n<li>La realización de cambios en la iluminación ambiental para <strong>medir la dilatación de las pupilas</strong>.</li>\n<li><strong>Detectar movimientos naturales del ojo</strong>, bien en estado de reposo (como los nistagmos), bien obligando a realizar alguna tarea (como leer un texto).</li>\n<li>Uso del análisis espectral para d<strong>istinguir el tejido particular del ojo de otra clase de material</strong> (como lentillas). </li>\n</ul>\n<p>Imagen | <a href=\"https://commons.wikimedia.org/wiki/File:Set_of_50_artificial_glass_eyes,_all_shapes_and_sizes,_by_E._Wellcome_L0058938.jpg\">Welcome Images</a></p>\n<!-- BREAK 15 --> </div>', '2019-08-28 15:00:44', '2019-08-28 15:00:44', 48, 'portada0012.jpg', 'Parece la típica trama de una película de espías: matar a alguien con el fin de usar sus huellas, su ojo o su rostro para desbloquear un sistema de seguridad biométrica. Y sin embargo, las medidas destinadas a impedir esto no son de ficción'),
(54, 'El mundo al revés: esta inteligencia artificial está tratando de enseñar empatía a los humanos', '<div class=\"blob js-post-images-container\">\n<p>En la comedia británica de 2012 \"Exótico Hotel Marigold\", el personaje interpretado por Judi Dench (Evelyn) terminaba, una anciana viuda británica, terminaba recalando en la India y lograba un empleo <strong>dando charlas a los teleoperadores locales para enseñarles el mejor modo de tratar a sus interlocutores</strong>, de tratarles de un modo más \"humano\".</p>\n<!-- BREAK 1 -->\n<p>Sin embargo, un informe de 2017 elaborado por la consultora McKinsey &amp; Co afirmaba que <strong>una tercera parte de las actividades del 60% de los empleos eran potencialmente automatizables</strong> gracias al avance de la inteligencia artificial y la robótica. Y la ola expansiva de la automatización de mercado laboral parece haber alcanzado también a la clase de empleo que Evelyn ejercía en aquel call center hindú.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<div class=\"article-asset-video article-asset-normal\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"366\" src=\"//www.youtube.com/embed/M-bIBy_JxlI\" width=\"650\"></iframe>\n</div>\n</div>\n</div>\n<p><em>¿Creéis que una IA podrá llegar a realizar algún día este trabajo?</em></p>\n<p>La tienda de medicamentos online Humana Pharmacy ha implementado un sistema de inteligencia artificial en sus centros de atención telefónica para que vaya ofreciendo asesoramiento sobre la marcha a sus 1.700 teleoperadores mientras éstos hablan con sus clientes. ¿El objetivo? <strong>Que sea la máquina quien enseñe a los humanos a ser más amables y empáticos con sus congéneres</strong>; es decir, a ser más humanos.</p>\n<!-- BREAK 3 -->\n<p>El uso de este coach artificial no será obligatorio para los empleados, aunque sí se bonificará. Y, aunque sus creadores afirman que no se trata de una herramienta de gestión del rendimiento, sí ofrecerá a sus supervisores información sobre sus respectivos desempeños.</p>\n<!-- BREAK 4 -->\n\n<p>El algoritmo, desarrollado por la empresa inteligencia artificial Cogito (y que lo ofrecerá ahora al resto de sus clientes), <strong>avisa a los teleoperadores cuando han permanecido demasiado tiempo en silencio, o cuando considera que empiezan a hablar demasiado rápido</strong>.</p>\n<!-- BREAK 5 -->\n<p>Un inesperado cambio de roles, dado que solemos ser los humanos los que tenemos que intervenir para lograr que el trato de las IAs en su interacción con otros humanos pueda ser... eso, más humana.</p>\n<!-- BREAK 6 -->\n<p>Pero John Feast, CEO de la compañía, marca un límite a esta invasión de la IA en el sector de los call centers: los coach como Evelyn están en parte obsoletos, sí, pero <strong>no cree que los propios teleoperadores puedan ser sustituidos tan fácilmente</strong>:</p>\n<!-- BREAK 7 -->\n<blockquote>\n<p>\"Los seres humanos siempre van a querer hablar con otros seres humanos. Porque sólo otros seres humanos nos entienden de verdad\".</p>\n</blockquote>\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"El software desarrollado por Cogito\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/5454a9/cogito/450_1000.png\" data-sf-srcset=\"https://i.blogs.es/5454a9/cogito/450_1000.png 450w, https://i.blogs.es/5454a9/cogito/650_1200.png 681w, https://i.blogs.es/5454a9/cogito/1024_2000.png 1024w, https://i.blogs.es/5454a9/cogito/1366_2000.png 1366w\"/><noscript><img alt=\"El software desarrollado por Cogito\" src=\"https://i.blogs.es/5454a9/cogito/450_1000.png\"/></noscript> <span> El software desarrollado por Cogito </span> </div></div></div>\n<h2>La clave de la empatía: la teoría de la mente</h2>\n<p>Y es que la creación de este nuevo algoritmo no significa que de pronto <a href=\"https://iahuawei.xataka.com/asi-como-inteligencia-artificial-capaz-entender-nuestras-necesidades/\">la IA sea capaz de comprender las emociones humanas</a> de forma equiparable a como lo hacemos nosotros: <strong>sólo que es capaz de detectar una serie de patrones en nuestra voz o la del cliente</strong>, independientes de la cultura o el lenguaje de los mismos, que están estadísticamente vinculados (ahora sí, según otros humanos) a la falta de empatía.</p>\n<!-- BREAK 8 -->\n\n<p>¿Qué necesitaría la IA para comprender nuestra conducta y nuestras emociones? <strong>De la capacidad de percibir que los otros agentes (los humanos, en este caso), poseen un estado mental interno igual que el de uno mismo</strong>, y a la vez diferente de él, lo que llevaría a imaginar los estados mentales ajenos, a ponerse en el lugar del otro.</p>\n<!-- BREAK 9 -->\n<p>A esta compleja habilidad de la que nosotros disponemos de forma innata la llamamos \"teoría de la mente\" <strong>y en ella fundamos nuestra capacidad de interacción social</strong> (los humanos con una teoría de la mente deficitaria, como los autistas, tienen graves impedimentos a la hora de interactuar y mostrar empatía).</p>\n<!-- BREAK 10 -->\n<p>Varios investigadores publicaron hace año y medio un \'paper\' académico (<a href=\"https://arxiv.org/abs/1802.07740\">\"Machine Theory of Mind\"</a>) que sostenía que dicha habilidad era aplicable a máquinas... pero el algoritmo de Cogito no se encuentra aún en esa etapa, y <strong>es complicado que vayamos a ver a corto plazo una IA dotada de una teoría de la mente funcional</strong>.</p>\n<!-- BREAK 11 -->\n<p>Vía | <a href=\"https://eu.usatoday.com/story/tech/2019/08/23/ai-training-human-employees-to-have-more-empathy-work/2070002001/\">USA Today</a></p>\n<!-- BREAK 12 -->\n<p>Imagen | <a href=\"https://unsplash.com/photos/0E_vhMVqL9g\">Unsplash</a></p>\n</div>', '2019-08-27 23:54:32', '2019-08-27 23:54:32', 43, 'portada0013.jpg', '\"Hablas demasiado rápido\" o \"has permanecido demasiado tiempo en silencio\" son algunas de las instrucciones que este algoritmo proporciona a los empleados de call centers para procurar que su atención sea, irónicamente, más humana.'),
(55, 'Esta es la razón por la que las grandes compañías de conducción autónoma están publicando los datos recopilados por sus vehículos', '<div class=\"blob js-post-images-container\">\n<p>Desarrollar <strong>la tecnología necesaria para lograr que un vehículo autónomo funcione</strong> de forma adecuada es una tarea compleja. Tanto, que el gran líder de la industria (Waymo, una compañía subsidiaria de Google) ha decidido <strong>compartir con su competencia los datos que han ido recopilando sus vehículos</strong>.</p>\n<!-- BREAK 1 -->\n<p>¿La razón? Su convencimiento de que <strong>\"cuantos más grandes cerebros podamos juntar para resolver los problemas, aunque no procedan de nuestra compañía, mejor\"</strong>. Así resume Drago Anguelov, científico principal de Waymo, la decisión de compartir esa valiosa información para lograr resolver los problemas con los que se ha enfrentado su compañía, y acelerar así el desarrollo de esta tecnología.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Sin embargo, pese a que <strong>hasta hace muy poco las compañías guardaban con sumo celo estos datos</strong>, la reticencia para compartirlos empieza a ser algo del pasado: incluso antes de que Waymo diera este paso, otros rivales como <a href=\"https://www.nuscenes.org/\">APTIV</a> o <a href=\"https://www.geospatialworld.net/blogs/lyft-datasets-for-autonomous-vehicles/\">Lyft</a> ya habían tomado la iniciativa de publicar los datos recopilados por sus vehículos.</p>\n<!-- BREAK 3 -->\n\n<p>Otra compañía, Argo AI, anunció la inminente publicación de <a href=\"http://www.argoverse.org/\">su dataset</a> en la misma conferencia en que lo hizo Waymo; en su caso incluye únicamente 113 escenas grabadas en dos ciudades, pero también es <strong>la única que ha incluido datos de sus mapas de alta definición</strong>.</p>\n<!-- BREAK 4 -->\n<p>Lo relevante es que, en todos estos casos, la motivación alegada por la compañías es la misma: <strong>el convencimiento de que todos saldrán ganando</strong>. Eso sí: según el responsable de producto de Waymo, Vijaysai Patnaik, </p>\n<!-- BREAK 5 -->\n<blockquote>\n<p>\"El hecho de que tanto Waymo como otras compañías estén liberando sus datos no va tanto de decir \'Eh, este problema es muy complicado, creo que deberíamos poner en común nuestros datos\' como de afrontar que debemos dar un empujón a la comunidad de investigadores**, para quienes resulta muy difícil acceder a fuentes de datos de esta clase</p>\n</blockquote>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/rnM0n-Vci8Q\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<h2>La importancia de los datos (y de compartir)</h2>\n<p>Pero ¿qué tienen de especial esos datos? ¿por era tan importante tanto protegerlos antes como compartirlos ahora? Veamos: en un día normal, <strong>un coche autónomo de pruebas puede recopilar una enorme cantidad de datos</strong> en bruto (<strong>más de 4 Tb</strong>, según explica en Forbes Sam Abuelsamid, analista de Navigant Research).</p>\n<!-- BREAK 6 -->\n<p>Pero no todos esos terabytes resultan útiles: un ingeniero, que acompaña al director de seguridad en los trayectos, <strong>va anotando las situaciones más relevantes en que se ve implicado el vehículo</strong> (normalmente encuentros con señales de tráfico, ciclistas, peatones y animales).</p>\n<!-- BREAK 7 -->\n<p>Y son estas situaciones las que se terminan analizando y etiquetando a mano... esta última, <strong>una tarea para la que pueden hacer falta cientos o miles de humanos que analicen cuidadosamente las escenas</strong> para comprender todos los elementos relevantes de las mismas.</p>\n<!-- BREAK 8 -->\n\n<p>Una vez que todo ese trabajo ha sido realizado, es cuando los investigadores tienen la oportunidad de <strong>desarrollar algoritmos</strong> capaces de mejorar la predicción del comportamiento del resto de usuarios de las calles y carreteras. Según Anguelov:</p>\n<!-- BREAK 9 -->\n<blockquote>\n<p>\"[En Waymo] sentimos (no sólo nosotros, bastantes más compañías también ha llegado a la misma conclusión) que actualmente el campo se está viendo obstaculizado por la ausencia de los datos adecuados. Decidimos ayudar a que los investigadores académicos se hicieran las preguntas correctas y, para ello, necesitan trabajar con los datos correctos\".</p>\n</blockquote>\n<p>El objetivo último claro, es <strong>obtener el Santo Grial de la conducción autónoma: el nivel 5</strong> de <a href=\"https://www.xataka.com/automovil/de-0-a-5-cuales-son-los-diferentes-niveles-de-conduccion-autonoma\">la norma SAE J3016</a>, es decir, el máximo nivel de automatización posible. Hasta ahora, la industria sólo ha sido capaz de ofrecer coches autónomos de nivel 2 (sí, incluso <a href=\"https://www.xataka.com/inteligencia-artificial/que-que-tesla-otros-llaman-piloto-automatico-no-realmente-coche-autonomo\">el famoso \'Auto Pilot\' de Tesla está así de limitado</a>) por lo que dará la bienvenida a cualquier avance <a href=\"https://www.xataka.com/automovil/esperamos-tener-un-coche-autonomo-de-nivel-3-a-partir-de-2021-hablamos-con-dirk-wisselmann-experto-de-bmw-en-conduccion-autonoma\">hacia el siguiente peldaño</a> que sea capaz de ofrecer a unos consumidores deseosos de avances en este campo.</p>\n<!-- BREAK 10 -->\n<div class=\"article-asset-embed-giphy article-asset-normal article-asset-center\">\n<div class=\"article-asset-video\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\" id=\"_giphy_jTTZcamhptCTwPcU4Z\">\n<iframe allowfullscreen=\"\" class=\"giphy-embed\" src=\"https://giphy.com/embed/jTTZcamhptCTwPcU4Z\" style=\"position:absolute\"></iframe>\n</div>\n</div>\n</div>\n</div>\n<p>En el caso del dataset de vídeos publicados por Waymo (<a href=\"https://waymo.com/open/\">disponible en su web</a>) se trata, en palabras de la compañía, en \"<strong>uno de las mayores, más ricos y más diversos datasets</strong> sobre conducción autónoma que jamás se han hecho públicos con fines de investigación\".</p>\n<!-- BREAK 11 -->\n<p>Las imágenes (<strong>1000 escenas de conducción, cada una de 20 segundos, capturadas en 25 ciudades de los Estados Unidos</strong>), procedentes de las cámaras de alta resolución del los coches, vienen acompañadas también de datos captados con sensores LiDAR (siendo Waymo la primera compañía que ha publicado esta información).</p>\n<!-- BREAK 12 -->\n<p>Sumando todo esto, Waymo afirma que ese millar de escenas cubre \"<strong>una amplia variedad de entornos</strong>, desde centros urbanos densos a paisajes suburbanos, así como datos recogidos durante el día y la noche, durante el amanecer y el crepúsculo, bajo el sol y bajo la lluvia\".</p>\n<!-- BREAK 13 -->\n<p>Anguelov afirma que la intención de Waymo es <strong>ir publicando aún más datos</strong>: \"Esto es sólo el primer corte\", a la espera de recibir el feedback de la comunidad.</p>\n<!-- BREAK 14 -->\n<p>Vía | <a href=\"https://www.axios.com/newsletters/axios-autonomous-vehicles-cd2d7a71-d6cc-4265-9333-b33addee6a5e.html\">Axios</a> &amp; <a href=\"https://www.inverse.com/article/58624-musk-reads-nuke-mars-elon-musk-declares\">Inverse</a></p>\n<!-- BREAK 15 -->\n<p>Imagen | Pixabay</p>\n</div>', '2019-08-24 00:48:08', '2019-08-24 00:48:08', 43, 'portada0014.jpg', 'En los últimos meses varias compañías de la conducción autónoma han publicado datos de sus vehículos, hasta hace poco celosamente guardados. Ahora, la última compañía en subirse al carro de esta tendencia ha sido el líder del sector: Waymo.'),
(56, 'Huawei presenta un chip de IA con el que plantar cara a Nvidia y Google, y muestra pocas esperanzas en que EE.UU. levante su veto', '<div class=\"blob js-post-images-container\">\n<p>Huawei Technologies ha presentado esta mañana, en la conferencia de prensa realizada en su sede central en Shenzhen, dos propuestas con las que afirma haber completado su gama de soluciones para el mercado de la inteligencia artificial: su nuevo chip de IA de gama alta, <strong>el Ascend 910, y la plataforma de desarrollo MindSpore</strong>, con los que la marca china entra a competir directamente contra las GPUs de Nvidia y contra el framework TensorFlow de Google, respectivamente..</p>\n<!-- BREAK 1 -->\n<p>Dicha cartera está conformada a día de hoy por los <strong>chips de la serie Ascend</strong>, por la <strong>librería y kit de herramientas CANN</strong> (Compute Architecture for Neural Networks), por el nuevo <strong>framework de desarrollo MindSpore</strong> y por <strong>la plataforma de implantación de modelos de IA ModelArts</strong>, todos ellos productos que cubren una amplia variedad de entornos: Internet de las Cosas, nubes públicas y privadas, dispositivos de consumo, etc.</p>\n<!-- BREAK 2 -->\n<!--more-->\n\n<h2>Ascend 910, \"el chip más potente del mundo\"</h2>\n<p>Huawei ya presentó el año pasado, cuando aún se encontraba en desarrollo, algunos datos sobre las especificaciones técnicas del Ascend 910. Ahora afirma que el chip ha \"superado nuestras expectativas iniciales\": pese a su potencia de cálculo han logrado mantener <strong>un consumo máximo real en 310W, por debajo incluso de los 350w que se marcaron como objetivo</strong> durante la fase de desarrollo.</p>\n<!-- BREAK 3 -->\n<p>El Ascend 910 ofrece un rendimiento de hasta 256 <a href=\"https://www.xataka.com/basics/que-son-los-teraflops-y-que-miden-exactamente\">teraFLOPS</a>, <a href=\"https://medium.com/syncedreview/huawei-leaps-into-ai-announces-powerful-chips-and-ml-framework-f9aa6ec87bcb\">mientras</a> que la GPU más potente de Nvidia Tesla V100 ofrece hasta 125 teraFLOPS (con un consumo de energía máximo algo menor: 300W), y la TPU 2.0 de Google puede alcanzar los 180 teraFLOPS.</p>\n<!-- BREAK 4 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Eric Xu\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/3c4228/eric-xu/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/3c4228/eric-xu/450_1000.jpg 450w, https://i.blogs.es/3c4228/eric-xu/650_1200.jpg 681w, https://i.blogs.es/3c4228/eric-xu/1024_2000.jpg 1024w, https://i.blogs.es/3c4228/eric-xu/1366_2000.jpg 1366w\"/><noscript><img alt=\"Eric Xu\" src=\"https://i.blogs.es/3c4228/eric-xu/450_1000.jpg\"/></noscript> <span> Eric Xu, presidente rotatorio de Huawei, presentando Ascend 910. </span> </div></div></div>\n<p>El Ascend 910 no estará destinado a su uso por parte de usuarios domésticos en PCs o dispositivos móviles (si bien la existencia de una gama de smartphones <a href=\"https://www.xataka.com/tag/huawei-ascend-p6\">Huawei Ascend P</a> puede inducir a la confusión), <strong>sino en centros de datos destinados al entrenamiento de modelos de IA</strong> con fines científicos.</p>\n<!-- BREAK 5 -->\n<p>Una labor en la que, si hemos de creer a Huawei, destacará sobre su competencia: en una sesión de entrenamiento típica basada en ResNet-50, la combinación de Ascend 910 y MindSpore demostró ser hasta el doble de rápido que otras tarjetas que utilizan <a href=\"https://www.xataka.com/inteligencia-artificial/tensorflow-software-google-lider-machine-learning-presenta-su-nueva-version-2-0-alpha-nuevo-modulo-privacidad\">el framework TensorFlow</a>.</p>\n<!-- BREAK 6 -->\n<h2>MindSpore, el framework</h2>\n<p>En 2018, Huawei anunció que se había marcado <strong>tres objetivos a la hora de desarrollar su framework de IA</strong>: que facilitara el desarrollo (ahorrando tiempo y costes), que fuera eficiente (la mayor potencia con el menor consumo posibles) y que fuera capaz de adaptarse a diversos escenarios (desde el \'<a href=\"https://www.xataka.com/n/la-ia-se-baja-de-la-nube-para-meterse-en-el-movil-un-nuevo-presente-y-promesas-de-futuro\">edge computing</a>\' a la computación en la nube).</p>\n<!-- BREAK 7 -->\n<p>Según ha dado a entender Xu, MindSpore (que pasará a ser open source a partir de la primera mitad del año que viene) cumple con los objetivos que la compañía se marcó el año pasado. Y además, ha tenido en cuenta la preocupación existente sobre la protección de la privacidad, y <strong>afirma haber garantizado con MindSpore la seguridad de los datos privados</strong>, gracias al hecho de que no se ocupa de los datos en sí, sino de gradientes y modelos procesados que ya carecen de información identificable.</p>\n<!-- BREAK 8 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/3cvLXCzChhc\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Su concepto de diseño, bautizado como \"<em>AI Algorithm As Code</em>\", permite entrenar modelos más rápidamente y desarrollar aplicaciones avanzadas de AI con facilidad: por ejemplo, en una red neuronal típica de procesamiento de lenguaje natural (PNL), <strong>MindSpore requiere un 20% menos de líneas de código que sus rivales</strong>, y ayuda a los desarrolladores a aumentar su eficiencia en al menos un 50%.</p>\n<!-- BREAK 9 -->\n<p>La compañía ha explicado a los medios que este nuevo framework no sólo está pensado para trabajar en colaboración con el nuevo Ascend 910, sino que <strong>será compatible con CPUs y GPUs de otros proveedores</strong>. También el nuevo chip Kirin 990, que Huawei presentará en la inminente IFA 2019, será compatible con MindSpore.</p>\n<!-- BREAK 10 -->\n<p>Actualmente el campo de los frameworks de machine learning, que facilitan a los desarrolladores la creación de sistemas de IA, <strong>está liderado por TensorFlow, de Google</strong>. <a href=\"https://www.genbeta.com/desarrollo/ecosistema-tensorflow-para-programadores-principiantes-expertos-machine-learning-cursos-lenguajes-edge-computing\">Es el estándar de facto</a>, la herramienta que acapara la mayor cantidad de búsquedas, de artículos académicos y de cursos online; a mayor distancia se encuentran otras alternativas como Keras, PyTorch (de Facebook) o Caffee.</p>\n<!-- BREAK 11 -->\n<p>Es difícil predecir <strong>cómo quedará el mercado de estas herramientas una vez se consolide MindSpore</strong>, pero Huawei ha querido dar a entender a la comunidad de usuarios que apuesta fuerte por su nueva herramienta.</p>\n<!-- BREAK 12 -->\n\n<h2>Huawei sigue mirando de reojo al Departamento de Comercio de los EE.UU.</h2>\n<p>Hay que recordar que la compañía china disfruta ahora mismo (<a href=\"https://www.xataka.com/empresas-y-economia/estados-unidos-plantea-alargar-tregua-a-huawei-otros-90-dias-cook-debate-trump-guerra-comercial\">y hasta el 19 de noviembre</a>) de un aplazamiento del <a href=\"https://www.xataka.com/moviles/que-se-sabe-que-no-veto-google-android-a-moviles-huawei\">veto que aprobó contra ella</a> el Departamento de Comercio de los EE.UU. para <strong>impedirle adquirir tecnología estadounidense</strong>. </p>\n<!-- BREAK 13 -->\n<p>Sin embargo, muchos analistas consideran que dicho aplazamiento, más que ser un gesto hacia China y/o Huawei, busca dar margen a los proveedores estadounidenses <strong>para prepararse ante un futuro sin Huawei</strong>.</p>\n<!-- BREAK 14 -->\n<p>El propio Xu confirmó hoy que no tiene las esperanzas puestas en un levantamiento definitivo del veto: \"Ya nos hemos acostumbrado a trabajar con las restricciones de la Lista de Entidades\" y <strong>están dispuestos a \"trabajar y vivir bajo esta situación durante mucho tiempo\"</strong>.</p>\n<!-- BREAK 15 --> </div>', '2019-08-23 19:01:22', '2019-08-23 19:01:22', 16, 'portada0015.jpg', 'Huawei ha presentado hoy dos propuestas con las que afirma haber completado su gama de soluciones para el mercado de la IA: su nuevo chip de IA de gama alta, el Ascend 910, y la plataforma de desarrollo MindSpore.'),
(57, 'Un algoritmo que \'lee\' el movimiento de las manos abre la puerta a que los smartphones puedan traducir el lenguaje de signos', '<div class=\"blob js-post-images-container\">\n<p>Varias compañías, como Kintrans o Signall, han intentado en los últimos años desarrollar software capaz de interpretar el lenguaje de signos, permitiendo así a sus usuarios comunicarse fácilmente con cualquier persona. <strong>Pero lo cierto es que no han tenido gran éxito hasta ahora</strong>.</p>\n<!-- BREAK 1 -->\n<p>Sin embargo, un nuevo algoritmo desarrollado por los Google AI Labs, podría ser la clave para <strong>materializar al fin un sistema usable de traducción simultánea entre personas sordas y oyentes</strong>. Y bastaría, para ello, con usar un smartphone.</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Este nuevo algoritmo es capaz de seguir el movimiento de las manos del usuario una vez que ha tenido la ocasión de mapearlas enfoncándolas con la cámara de su móvil. \"Nuestra solución utiliza <a href=\"https://www.xataka.com/robotica-e-ia/machine-learning-y-deep-learning-como-entender-las-claves-del-presente-y-futuro-de-la-inteligencia-artificial\">aprendizaje automático</a> para calcular los 21 puntos clave en 3D de una mano dentro de un fotograma de vídeo\".</p>\n<!-- BREAK 3 -->\n\n<h2>La \'lectura de manos\' es más problemática de lo que parece</h2>\n<p>Pero <strong>este aspecto del algoritmo no ha sido fácil de desarrollar</strong>: los diferentes tamaños de cada mano, la velocidad de sus movimientos y el hecho de que, durante la conversación, unos dedos puedan obstruir la visión de otros... todos estos aspectos han constituido interesantes retos para los investigadores.</p>\n<!-- BREAK 4 -->\n<p>Para reducir los requerimientos de hardware del algoritmo, apostaron por reducir la cantidad de datos que el algoritmo necesitaba analizar, para que así el tiempo de respuesta fuera menor. <strong>Así, abandonaron la idea de detectar la posición y el tamaño de toda la mano como conjunto</strong>, y empezaron por detectar la palma (la parte más distintiva y regular de la misma).</p>\n<!-- BREAK 5 -->\n<p>A continuación, el sistema detecta los cinco elementos que brotan de la misma (los dedos). Para lograr todo esto, <strong>hicieron que la IA analizara 30.000 imágenes</strong> (cada una con diferentes poses y condiciones de iluminación) y aprendiera de todas ellas, aunque eso requirió que un equipo de humanos <strong>etiquetara manualmente los 21 puntos clave</strong> en cada una de esas imágenes.</p>\n<!-- BREAK 6 -->\n<div class=\"article-asset-embed-giphy article-asset-normal article-asset-center\">\n<div class=\"article-asset-video\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\" id=\"_giphy_LSpVx6zK9EfBLGx3t0\">\n<iframe allowfullscreen=\"\" class=\"giphy-embed\" src=\"https://giphy.com/embed/LSpVx6zK9EfBLGx3t0\" style=\"position:absolute\"></iframe>\n</div>\n</div>\n</div>\n</div>\n<p>Ahora, es capaz de reconocer el tamaño y ángulos de las palmas de las manos,** asignándoles coordenadas en base a la posición estimada de los dedos y nudillos**. El algoritmo también permite calcular la profundidad de los elementos de la imagen, indicada en la siguiente imagen mediante una en escala de grises:</p>\n<!-- BREAK 7 -->\n<div class=\"article-asset-embed-giphy article-asset-normal article-asset-center\">\n<div class=\"article-asset-video\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\" id=\"_giphy_H7BL6A3KPiuRgvrWzq\">\n<iframe allowfullscreen=\"\" class=\"giphy-embed\" src=\"https://giphy.com/embed/H7BL6A3KPiuRgvrWzq\" style=\"position:absolute\"></iframe>\n</div>\n</div>\n</div>\n</div>\n<p>En su blog corporativo, los investigadores de Google AI Labs explican que la novedad de su propuesta reside en que <strong>rompe con el enfoque hasta ahora vigente, basado en potentes entornos de escritorio</strong>, logrando un rendimiento en tiempo real pese a funcionar en teléfonos móviles.</p>\n<!-- BREAK 8 -->\n<p>Desde Google afirman que seguirán trabajando en mejorar su precisión y anuncian la <a href=\"https://github.com/google/mediapipe/blob/master/mediapipe/docs/hand_tracking_mobile_gpu.md\">disponibilidad del código fuente</a> del algoritmo <strong>para otros investigadores que quieran consultarlo</strong>.</p>\n<!-- BREAK 9 -->\n<p>Vía | <a href=\"https://techcrunch.com/2019/08/19/this-hand-tracking-algorithm-could-lead-to-sign-language-recognition/?tpcc=ECFB2019\">TechCrunch</a></p>\n<!-- BREAK 10 -->\n<p>Imagen | Google AI Labs</p>\n</div>', '2019-08-21 23:45:42', '2019-08-21 23:45:42', 49, 'portada0016.jpg', 'Una traductor para usuarios de lengua de signos preciso, en tiempo real y sin grandes requerimientos de hardware: eso es lo que ofrece el último software desarrollado por Google AI Labs.'),
(58, 'FSGAN, el algoritmo que permite crear deepfakes en formato vídeo más fácilmente y en tiempo real', '<div class=\"blob js-post-images-container\">\n<p>La <strong>generación de deepfakes</strong> es una de las aplicaciones de la inteligencia artificial que <a href=\"https://www.xataka.com/robotica-e-ia/deepfakes-tendremos-problema-verdad-videos-serviran-como-pruebas\">más ha dado de qué hablar</a> en este último año. Y además, <strong>evoluciona a ojos vista</strong>, permitiendo a una base de usuarios cada vez más amplia realizar deepfakes cada vez más realistas de manera cada vez más sencilla. Pero lo de <a href=\"https://nirkin.com/fsgan/\">FSGAN</a> es un paso enorme en ese sentido.</p>\n<!-- BREAK 1 -->\n<p>Este nuevo software (cuyo nombre significa \'Face Swapping GAN\' o \'<a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-gans-redes-generativas-antagonicas\">Red generativa antagónica</a> cambia-caras\') nos permite recurrir a <strong>dos técnicas diferentes de creación de deepfakes</strong> en formato vídeo: la de <strong>\'face swapping\'</strong>, con la que superponemos el rostro de una persona sobre el cuerpo de otra, y la de <strong>\'face reenactment\'</strong>, que nos permite recrear la efigie de una persona aplicándole los movimientos de otra persona (incluyendo los de los labios).</p>\n<!-- BREAK 2 -->\n<!--more-->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"FSGAN\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/5d27c5/fsgan_/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/5d27c5/fsgan_/450_1000.jpg 450w, https://i.blogs.es/5d27c5/fsgan_/650_1200.jpg 681w, https://i.blogs.es/5d27c5/fsgan_/1024_2000.jpg 1024w, https://i.blogs.es/5d27c5/fsgan_/1366_2000.jpg 1366w\"/><noscript><img alt=\"FSGAN\" src=\"https://i.blogs.es/5d27c5/fsgan_/450_1000.jpg\"/></noscript> <span> Ejemplos del uso de ambas técnicas de creación de deepfakes. </span> </div></div></div>\n<h2>Falsificar más rápido y más fácil</h2>\n<p>Pero lo más relevante de FSGAN es que permite hacer todo esto <strong>sin necesidad de poner el algoritmo en cuestión a \'entrenar\' durante horas (o incluso días)</strong>, y recurriendo a costoso hardware especializado, hasta que éste sea capaz de \'aprender\' el aspecto y movimiento de un rostro humano.</p>\n<!-- BREAK 3 -->\n<p>Y el ahorro de tiempo no es la única gran novedad, sino hasta qué punto se simplifica a partir de ahora la generación de deepfakes: básicamente, <strong>cualquier persona con unos conocimientos básicos de esta tecnología podrá producir nuevas falsificaciones</strong>, porque el \'trabajo pesado\' pasa a estar completamente automatizado.</p>\n<!-- BREAK 4 -->\n\n<p>Desarrollado por científicos de la Universidad Bar-Ilan de Israel, según explican en el <a href=\"https://arxiv.org/pdf/1908.05932.pdf\">\'paper\' disponible</a> desde el pasado viernes en ArXiv, FSGAN mapea el rostro o las expresiones faciales de una persona \'fuente\' (<strong>extraída de un vídeo o -incluso, aunque con peores resultados- de un mero fotograma</strong>) para aplicarlos sobre un segundo individuo \'objetivo\'.</p>\n<!-- BREAK 5 -->\n<p>El vídeo de demostración publicado por los investigadores permite observar que <strong>los resultados de esta técnica simplificada se acercan al nivel de perfección mostrado en el pasado por otras más complejas</strong> de usar, consiguiendo salir airoso de retos como el de lidiar con micrófonos superpuestos al rostro \'objetivo\' o el conjuntar rostros y cuerpos de distinta pigmentación de piel.</p>\n<!-- BREAK 6 -->\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allowfullscreen=\"\" height=\"563\" src=\"//www.youtube.com/embed/BsITEVX6hkE\" width=\"1000\"></iframe>\n</div>\n</div>\n</div>\n<p>Sin embargo, es cierto que por ahora FSGAN <strong>flojea bastante en otros casos</strong>, como a la hora de aplicar el \'face reenactment\' a partir de un fotograma individual, mostrando fondos de imagen temblorosos que permiten detectar el deepfake a la legua.</p>\n<!-- BREAK 7 -->\n<h2>FSGAN no seguirá la senda de GPT-2</h2>\n<p>Pero, ¿por qué sus creadores han anunciado que <strong>compartirán de manera \"inminente\" el código de este software</strong> con el mundo? ¿No podrían aplicar la política por la que optó Open AI cuando se negó a permitir el acceso a <a href=\"https://www.xataka.com/inteligencia-artificial/gpt-2-que-sabemos-que-no-generador-textos-ia-que-openai-dice-haber-censurado-ser-demasiado-peligroso\">su generador de \'fake news\' GPT-2</a> porque <strong>era \"demasiado peligroso en las manos equivocadas\"</strong>?</p>\n<!-- BREAK 8 -->\n<p>Lo cierto es que ellos no están de acuerdo: afirman que <strong>lo compartirán porque impedirlo \"no detendrá el desarrollo de tecnología similar\"</strong>, sino que sencillamente ocultaría al público y a los responsables políticos las repercusiones del mal uso de esta clase de algoritmos, e impediría el desarrollo de alguna clase de contramedidas.</p>\n<!-- BREAK 9 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"FSGAN\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/d58706/system_05/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/d58706/system_05/450_1000.jpg 450w, https://i.blogs.es/d58706/system_05/650_1200.jpg 681w, https://i.blogs.es/d58706/system_05/1024_2000.jpg 1024w, https://i.blogs.es/d58706/system_05/1366_2000.jpg 1366w\"/><noscript><img alt=\"FSGAN\" src=\"https://i.blogs.es/d58706/system_05/450_1000.jpg\"/></noscript> <span> Esquema de funcionamiento del algoritmo, elaborado por los autores de la investigación. </span> </div></div></div>\n<p>Vía | <a href=\"https://futurism.com/the-byte/system-easy-create-deepfakes\">Futurity</a></p>\n</div>', '2019-08-21 11:00:36', '2019-08-21 11:00:36', 36, 'portada0017.jpg', 'Crear vídeos manipulados requiere de menos conocimientos técnicos y de menos tiempo de procesamiento que nunca gracias al algoritmo FSGAN.');
INSERT INTO `posts` (`id`, `titulo`, `cuerpo`, `created_at`, `updated_at`, `user_id`, `imagen`, `descripcion`) VALUES
(59, 'La inteligencia artificial también está en la carretera: así funciona el reconocimiento automático de matrículas (o ANPR)', '<div class=\"blob js-post-images-container\">\n<p>Circulando por las carreteras españolas <strong>es posible que alguna vez hayas reparado en unas cámaras</strong> que, situadas junto a algunos paneles informativos, pasos elevados o semáforos, se dedican a vigilar todos aquellos coches que pasan por debajo de las mismas.</p>\n<!-- BREAK 1 -->\n<p>Cada vez que llega a tus manos una multa por exceso de velocidad, es gracias a la principal funcionalidad de estas cámaras: <strong>el reconocimiento automático de matrículas</strong>, conocido también por sus acrónimos en inglés <strong>ANPR</strong> (Automatic Number Plate Recognition) y <strong>ALPR</strong> (Automatic License Plate Recognition).</p>\n<!-- BREAK 2 -->\n<!--more-->\n<p>Las cámaras ANPR dedicadas en exclusiva a la lectura de matrículas son capaces de registrar instantáneas perfectamente visibles de los vehículos en marcha (incluso de aquellos que circulan a altas velocidades) gracias a su notable <a href=\"https://www.xatakafoto.com/tutoriales/los-secretos-de-la-velocidad-de-obturacion\">velocidad de obturación</a>: es habitual encontrar dispositivos de este tipo con una velocidad de obturación de 1/10.000 (es decir, que <strong>requieren únicamente de una diezmilésima de segundo para capturar la imagen</strong>).</p>\n<!-- BREAK 3 -->\n\n<p>Esto permite <strong>utilizarlas tanto para saber qué coches han pasado por un determinado punto geográfico</strong> (aunque también existen cámaras ANPR móviles, preparadas para ser acopladas a coches de policía). La captura de imágenes durante la noche y en situaciones meteorológicas adversas (lluvia, niebla, presencia de humo, etc) se solventa mediante el uso de cámaras con reflector infrarrojo.</p>\n<!-- BREAK 4 -->\n<p>Esta tecnología recurre al <a href=\"https://www.xataka.com/robotica-e-ia/machine-learning-y-deep-learning-como-entender-las-claves-del-presente-y-futuro-de-la-inteligencia-artificial\">aprendizaje automático</a> para <strong>diferenciar las matrículas de otros elementos visibles en la imagen</strong> (como señales de tráfico o publicidad rotulada en los coches) y a la tecnología OCR (de reconocimiento óptico de caracteres) para identificar las letras y los números de la matrícula en cuestión.</p>\n<!-- BREAK 5 -->\n<p>El siguiente paso, una vez reconocidos los caracteres de la matrícula, corresponde al <strong>cotejo de la misma en una base de datos para identificar el vehículo en cuestión</strong> y la información disponible sobre el mismo (modelo, listado de propietarios, etc).</p>\n<!-- BREAK 6 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Proceso ANPR\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/4cf701/proceso_alpr/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/4cf701/proceso_alpr/450_1000.jpg 450w, https://i.blogs.es/4cf701/proceso_alpr/650_1200.jpg 681w, https://i.blogs.es/4cf701/proceso_alpr/1024_2000.jpg 1024w, https://i.blogs.es/4cf701/proceso_alpr/1366_2000.jpg 1366w\"/><noscript><img alt=\"Proceso ANPR\" src=\"https://i.blogs.es/4cf701/proceso_alpr/450_1000.jpg\"/></noscript> <span> Pasos que debe realizar la IA para reconocer una matrícula (Fuente: YND) </span> </div></div></div>\n<p>La detección de matrículas se puede realizar con <strong>cámaras específicas para esta tecnología</strong> (que realizan la tarea de reconocimiento <a href=\"https://www.xataka.com/internet-of-things/edge-computing-que-es-y-por-que-hay-gente-que-piensa-que-es-el-futuro\">dentro del propio dispositivo)</a>, pero también es frecuente ejecutar software ANPR para analizar el vídeo provisto por un circuito cerrado de televisión.</p>\n<!-- BREAK 7 -->\n<p>Aunque más recientemente, con la disponibilidad de <a href=\"http://www.openalpr.com/\">software de código abierto</a> preparado para esta tarea, es posible utilizar, en la práctica, todo tipo de cámaras, desde <a href=\"https://www.xataka.com/inteligencia-artificial/consultor-ciberseguridad-modifica-su-coche-tesla-para-transformarlo-sistema-vigilancia-rodante\">las de un coche autónomo</a> hasta cualquier webcam.</p>\n<!-- BREAK 8 -->\n<p>Los cuerpos policiales de todo el mundo usan esta tecnología tanto para fines de gestión del tráfico, detectando a infractores de normas viales (en España, la DGT ha dotado a su red de cámaras ANPR con una función extra: <a href=\"http://revista.dgt.es/es/reportajes/2017/04ABRIL/0411camaras-que-vigilan-el-uso-del-cinturon.shtml\">la detección de cinturones de seguridad</a>) como para tareas de vigilancia y persecución de criminales.</p>\n<!-- BREAK 9 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Detección de cinturones de seguridad\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/f47f72/3-figure4-1/450_1000.png\" data-sf-srcset=\"https://i.blogs.es/f47f72/3-figure4-1/450_1000.png 450w, https://i.blogs.es/f47f72/3-figure4-1/650_1200.png 681w, https://i.blogs.es/f47f72/3-figure4-1/1024_2000.png 1024w, https://i.blogs.es/f47f72/3-figure4-1/1366_2000.png 1366w\"/><noscript><img alt=\"Detección de cinturones de seguridad\" src=\"https://i.blogs.es/f47f72/3-figure4-1/450_1000.png\"/></noscript> <span> Los mismos fotogramas que permiten reconocer las matrículas pueden ser sometidos a otros algoritmos de machine learning para detectar la presencia de otros elementos, como los cinturones de seguridad (Vía Semanticscholar.org). </span> </div></div></div>\n<p>Las empresas de gestión de carreteras con peaje, por su parte, <a href=\"https://www.transportealdia.es/sistema-peaje-uniforme-europeo-eets/\">la usan</a> para controlar el pago de los mismos; similares usos le dan algunas estaciones de servicio. Igualmente, se utiliza ya para automatizar el acceso a aparcamientos de acceso restringido. Pero <strong>no dejan de aparecer constantemente nuevos usos para el ANPR</strong>.</p>\n<!-- BREAK 10 -->\n<h2>Una historia que comenzó hace 43 años</h2>\n<p>La primera tecnología enfocada al reconocimiento de matrículas (aunque entonces aún no estaba automatizada) <strong>fue desarrolla por de la División de Mejora Científica de la Policía británica en 1976</strong>. A ello contribuyó tanto el aumento de la potencia de computación como la mejora de las cámaras del momento.</p>\n<!-- BREAK 11 -->\n\n<p>Pero no empezó a popularizarse hasta los 90, cuando el software se hizo mucho más fácil de manejar y el hardware mucho más barato. La recogida de datos ANPR para su uso futuro (es decir, con el objetivo de resolver crímenes aún no identificados) tuvo que esperar una década más. <strong>Fue en el año 2005 cuando esta tecnología permitió, por primera vez, identificar y detener a tres sospechosos de asesinato</strong>.</p>\n<!-- BREAK 12 -->\n<p>Y ¿cuál es el futuro de esta tecnología a corto plazo? Los <a href=\"https://www.marketwatch.com/press-release/automatic-number-plate-recognition-anpr-system-market-is-anticipated-to-exceed-us-350-billion-by-2024-2019-05-15\">últimos estudios</a>, publicados hace tan sólo unos días, muestran que las soluciones ANPR son un campo en crecimiento cuyo valor aumentará un 9,3% anual de aquí a 2024, hasta situarse en un total de 4.700 millones de dólares.</p>\n<!-- BREAK 13 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Deteccion De Matriculas\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/2a3249/deteccion-de-matriculas/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/2a3249/deteccion-de-matriculas/450_1000.jpg 450w, https://i.blogs.es/2a3249/deteccion-de-matriculas/650_1200.jpg 681w, https://i.blogs.es/2a3249/deteccion-de-matriculas/1024_2000.jpg 1024w, https://i.blogs.es/2a3249/deteccion-de-matriculas/1366_2000.jpg 1366w\"/><noscript><img alt=\"Deteccion De Matriculas\" src=\"https://i.blogs.es/2a3249/deteccion-de-matriculas/450_1000.jpg\"/></noscript> <span> (Imagen de Amir shahram Hematian, 2010) </span> </div></div></div>\n<h2>¿Otra amenaza más para nuestra privacidad?</h2>\n<p>Pero la popularización de esta tecnología es una moneda con dos caras: muchos sostienen que la existencia de una infraestructura amplia de dispositivos ANPR <strong>debería suscitar el mismo debate sobre el papel de la privacidad que el provocado por la adopción masiva del reconocimiento facial</strong> puesto que, al fin y al cabo, sigue permitiendo conocer la localización y las rutinas de los ciudadanos.</p>\n<!-- BREAK 14 -->\n<p>Y al igual que la difusión del reconocimiento facial ha motivado a los activistas para buscar toda clase de creativas <a href=\"https://www.xataka.com/inteligencia-artificial/estos-metodos-que-intentan-sortear-reconocimiento-facial-defensores-privacidad\">técnicas para sortearlo o manipularlo</a>, el reconocimiento de matrículas empieza a suscitar la misma clase de reacciones.</p>\n<!-- BREAK 15 -->\n<p>La semana pasada, sin ir más lejos, la \'hacktivista\' y diseñadora de moda Kate Rose <a href=\"https://www.technologyreview.com/f/614175/a-new-clothing-line-confuses-automated-license-plate-readers/\">presentó su propia línea de ropa para tal fin</a>. No lo hizo, claro, en ninguna gala de moda, sino en la convención de seguridad DefCon de Las Vegas. Porque su objetivo es <strong>confundir a las cámaras ANPR recurriendo a prendas basadas en diseños de matrículas</strong>, como el siguiente:</p>\n<!-- BREAK 16 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Adversarial Fashion\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/03b0b4/adversarial_fashion/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/03b0b4/adversarial_fashion/450_1000.jpg 450w, https://i.blogs.es/03b0b4/adversarial_fashion/650_1200.jpg 681w, https://i.blogs.es/03b0b4/adversarial_fashion/1024_2000.jpg 1024w, https://i.blogs.es/03b0b4/adversarial_fashion/1366_2000.jpg 1366w\"/><noscript><img alt=\"Adversarial Fashion\" src=\"https://i.blogs.es/03b0b4/adversarial_fashion/450_1000.jpg\"/></noscript> <span> (Vía: Adversarial Fashion) </span> </div></div></div>\n<p>Estos ejemplos de <a href=\"https://www.xataka.com/inteligencia-artificial/conceptos-inteligencia-artificial-que-inteligencia-artificial-antagonica-como-puede-manipular-a-otras-ias\">IA antagónica</a> <strong>se aprovechan de las deficiencias de la IA de ANPR</strong> y de su amplia campo visual para lograr que los sistemas de reconocimiento asuman que sus portadores son coches.</p>\n<!-- BREAK 17 -->\n<p>Por otro lado, la Electronic Frontier Foundation lleva, además, denunciando el peligro que representa para nuestra privacidad el uso negligente de esta tecnología: <a href=\"https://www.eff.org/deeplinks/2015/10/license-plate-readers-exposed-how-public-safety-agencies-responded-massive\">un estudio publicado en 2015</a> por esta organización estadounidense encontró decenas de <strong>dispositivos ANPR expuestos en Internet a causa de configuraciones deficientes de la seguridad</strong>.</p>\n<!-- BREAK 18 -->\n<p>Tres años más tarde, y con poco esfuerzo, <a href=\"https://techcrunch.com/2019/01/22/police-alpr-license-plate-readers-accessible-internet/\">el medio online Techcrunch</a> encontró más de 150, entre ellos cámaras usadas por las policías estatales de California o Washington.</p>\n<!-- BREAK 19 -->\n<p>Imagen | PxHere</p>\n</div>', '2019-08-20 21:00:24', '2019-08-20 21:00:24', 50, 'portada0018.jpg', 'Cada vez más presentes en nuestras carreteras, las cámaras ANPR son capaces de identificar matrículas con la misma eficacia con que otros sistemas de vigilancia más polémicos identifican rostros.'),
(60, 'Conceptos de inteligencia artificial: qué es la inteligencia artificial antagónica (y cómo puede manipular a otras IAs)', '<div class=\"blob js-post-images-container\">\n<p>La IA está cada vez más presente en nuestro día a día: a medida que aumenta su ámbito de uso, tanto nosotros como las grandes empresas o los gobiernos pasamos a depender más de esta tecnología. Y, a medida que esto ocurre, nos vemos obligados a valorar no sólo su funcionalidad, sino también su seguridad: <strong>¿Qué probabilidad existe de que la IA falle o, peor aún, de que sea vulnerable a un ataque?</strong></p>\n<!-- BREAK 1 -->\n<p>La inteligencia artificial es, en el sentido amplio de la palabra, un arma; y, como todas las armas, puede ser usada para defendernos, pero también podemos ser atacados por ella. Así que reformulemos la anterior pregunta: <strong>¿cómo de vulnerable es la inteligencia artificial a un ataque realizado recurriendo a la propia inteligencia artificial?</strong></p>\n<!-- BREAK 2 -->\n<!--more-->\n\n<h2>Cuando la IA se engaña a sí misma</h2>\n<p>Ya en 2013, algunos empleados de Google publicaron <a href=\"https://arxiv.org/pdf/1312.6199.pdf\">un artículo</a> bautizado como \"Las propiedades intrigantes de las redes neuronales\", en el que planteaban cómo ésta tecnología podía ser manipulada mediante \'adversarial attacks\' (ataques antagónicos), un término que se ha generalizado como modo de referirnos a las nuevas técnicas desarrolladas para <strong>manipular sistemas basados en machine learning a través de la introducción experimental de datos en un algoritmo</strong>.</p>\n<!-- BREAK 3 -->\n<p>Así, si hacemos que un algoritmo de visión artificial procese una gran cantidad de imágenes, sería posible realizar lo que llamamos <a href=\"https://www.xataka.com/historia-tecnologica/asi-es-como-la-ingenieria-inverso-cambio-la-historia-de-la-informatica-para-siempre\">ingeniería inversa</a>, para conocer al detalle su funcionamiento y garantizar que seremos capaces de manipularlo; bien haciendo que dicho algoritmo no sea capaz de ver algo... <strong>o bien convenciéndolo de que está viendo algo que no existe</strong>.</p>\n<!-- BREAK 4 -->\n<p>Antes de seguir profundizando, <strong>remitámonos a un ejemplo real y concreto</strong>: en 2017, cuatro investigadores de la compañía LabSix pusieron a prueba el clasificador de imágenes Inception-v3, desarrollado por Google bajo código libre. Para ello, recurrieron a la impresión 3D para crear varias tortugas falsas totalmente realistas (indistinguibles de una verdadera para el ojo humano).</p>\n<!-- BREAK 5 -->\n<div class=\"article-asset-embed-giphy article-asset-normal article-asset-center\">\n<div class=\"article-asset-video\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\" id=\"_giphy_f9AmKMTZCO1ptwSQXE\">\n<iframe allowfullscreen=\"\" class=\"giphy-embed\" src=\"https://giphy.com/embed/f9AmKMTZCO1ptwSQXE\" style=\"position:absolute\"></iframe>\n</div>\n</div>\n</div>\n</div>\n<p><strong>Lo lógico hubiera sido que Inception-v3 las hubiera identificado como tortugas, pero el algoritmo de Google sólo veía... rifles</strong>. ¿Cómo era esto posible? Fácil: el equipo de investigadores había llevado a cabo lo que se conoce como \'ataque antagónico\'.</p>\n<!-- BREAK 6 -->\n<p>Mediante ingeniería inversa, habían identificado qué patrones concretos vinculaba el algoritmo a cada animal u objeto que era capaz de identificar, y una vez identificado el del rifle lo aplicaron al diseño del caparazón de las tortugas. <strong>Este patrón creado \'ex profeso\' para manipular a una IA es lo que llamamos una \'muestra antagónica\'</strong>.</p>\n<!-- BREAK 7 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Artificial Intelligence Adversarial Example Panda\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/743cf0/artificial-intelligence-adversarial-example-panda/450_1000.png\" data-sf-srcset=\"https://i.blogs.es/743cf0/artificial-intelligence-adversarial-example-panda/450_1000.png 450w, https://i.blogs.es/743cf0/artificial-intelligence-adversarial-example-panda/650_1200.png 681w, https://i.blogs.es/743cf0/artificial-intelligence-adversarial-example-panda/1024_2000.png 1024w, https://i.blogs.es/743cf0/artificial-intelligence-adversarial-example-panda/1366_2000.png 1366w\"/><noscript><img alt=\"Artificial Intelligence Adversarial Example Panda\" src=\"https://i.blogs.es/743cf0/artificial-intelligence-adversarial-example-panda/450_1000.png\"/></noscript> <span> Un poco de \'ruido\' correctamente aplicado y ¡voilá! Un oso panda \'se convierte\' en un gibón. (Vía Arxiv.org) </span> </div></div></div>\n<p>De esta forma, <strong>el ojo humano seguía viendo una tortuga normal y corriente, pero una IA no</strong>. No es algo tan extraño: nosotros, <a href=\"https://www.xataka.com/inteligencia-artificial/vista-humana-versatil-que-artificial-porque-nosotros-vemos-formas-maquinas-solo-reconocen-texturas\">cuando vemos, percibimos formas, pero las máquinas sólo reconocen texturas</a>; eso a veces ha permitido que la IA vea cosas que nosotros no podemos apreciar en modo alguno, pero también la ha convertido en vulnerable a algunos ataques.</p>\n<!-- BREAK 8 -->\n<p><a href=\"https://www.labsix.org/physical-objects-that-fool-neural-nets/\">Los resultados de este experimento</a>, llevado a cabo por Andrew Ilyas y sus compañeros de LabSix, se hicieron públicos durante la Conferencia Internacional sobre Aprendizaje Automático de 2018, recibiendo una amplia cobertura por parte de los medios de comunicación.</p>\n<!-- BREAK 9 -->\n<p>\"Incluso si no crees que ningún atacante va a manipular tu señal de \'stop\'\", explicaba Ilyas en referencia a los posibles efectos de los ataques antagónicos sobre el funcionamiento de los sistemas de conducción autónoma, <strong>\"la mera posibilidad de que pueda ocurrir es preocupante\"</strong>.</p>\n<!-- BREAK 10 -->\n\n<p>Pocos llevan más tiempo que Pin-Yu Chen, investigador de IBM, trabajando en este problema: él formaba parte del equipo que, en 2017 (antes del mediático caso de las tortugas-rifle) dio la voz de alarma avisando de <strong>lo fácil que era manipular la visión artificial de un coche autónomo</strong>: él y sus compañeros demostraron cuán fácil era volver \'invisible\' una señal de \'Stop\' con tan sólo añadirles algunas pequeñas pegatinas blancas y negras, como las que se ven aquí:</p>\n<!-- BREAK 11 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Autonomous Car Road Signs\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/2e6392/autonomous-car-road-signs/450_1000.jpg\" data-sf-srcset=\"https://i.blogs.es/2e6392/autonomous-car-road-signs/450_1000.jpg 450w, https://i.blogs.es/2e6392/autonomous-car-road-signs/650_1200.jpg 681w, https://i.blogs.es/2e6392/autonomous-car-road-signs/1024_2000.jpg 1024w, https://i.blogs.es/2e6392/autonomous-car-road-signs/1366_2000.jpg 1366w\"/><noscript><img alt=\"Autonomous Car Road Signs\" src=\"https://i.blogs.es/2e6392/autonomous-car-road-signs/450_1000.jpg\"/></noscript> <span> La señal de \'Stop\' de arriba fue indetectable para la visión artificial de los coches autónomos usados en el experimento. La de abajo fue \'confundida\' con un señalizador de límite de velocidad. (Vía: Arxiv.org) </span> </div></div></div>\n<p>¿Más posibles usos para esta clase de tecnología? Pues, por ejemplo, <a href=\"https://www.xataka.com/inteligencia-artificial/estos-metodos-que-intentan-sortear-reconocimiento-facial-defensores-privacidad\">boicotear sistemas de reconocimiento facial</a>. Pero... <strong>¿y si en lugar de simplemente evitar ser reconocido, te hace pasar por otra persona, como Milla Jovovich?</strong> Científicos de la Universidad Carnegie Mellon lo consiguieron portando unas vistosas (ridículas) gafas que alteran la percepción de tus rasgos por parte de la IA de turno:</p>\n<!-- BREAK 12 -->\n<div class=\"article-asset-image article-asset-normal\"><div class=\"asset-content\"> <div class=\"caption-img \"> <img alt=\"Carnegie Mellon University\" class=\"sf-lazy centro_sinmarco\" data-sf-src=\"https://i.blogs.es/734c3d/these-glasses-trick-facial-recognition-software-into-thinking-youre-carnegie-mellon-university-facial-recognition/450_1000.png\" data-sf-srcset=\"https://i.blogs.es/734c3d/these-glasses-trick-facial-recognition-software-into-thinking-youre-carnegie-mellon-university-facial-recognition/450_1000.png 450w, https://i.blogs.es/734c3d/these-glasses-trick-facial-recognition-software-into-thinking-youre-carnegie-mellon-university-facial-recognition/650_1200.png 681w, https://i.blogs.es/734c3d/these-glasses-trick-facial-recognition-software-into-thinking-youre-carnegie-mellon-university-facial-recognition/1024_2000.png 1024w, https://i.blogs.es/734c3d/these-glasses-trick-facial-recognition-software-into-thinking-youre-carnegie-mellon-university-facial-recognition/1366_2000.png 1366w\"/><noscript><img alt=\"Carnegie Mellon University\" src=\"https://i.blogs.es/734c3d/these-glasses-trick-facial-recognition-software-into-thinking-youre-carnegie-mellon-university-facial-recognition/450_1000.png\"/></noscript> <span> (Vía Carnegie Mellon University) </span> </div></div></div>\n<p>Pero aunque la manipulación de la visión artificial pueda ser el uso más obvio de esta clase de ataques, no deberíamos quedarnos ni mucho menos con la idea de que el resto de \'sentidos\' de la IA son inmunes los ataques antagónicos: <strong>exactamente el mismo procedimiento usado antes permite ocultar en vídeos mensajes de audio</strong> dirigidos, por ejemplo, a asistentes digitales, pero imperceptibles para el oído humano, de tal manera que un tercero pueda dar órdenes a nuestro Amazon Echo, por ejemplo, sin nuestro conocimiento.</p>\n<!-- BREAK 13 -->\n<p><em>Veamos un ejemplo:</em></p>\n<div class=\"article-asset-video article-asset-large\">\n<div class=\"asset-content\">\n<div class=\"base-asset-video\">\n<iframe allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/t5-vMJDFr8E\" width=\"560\"> </iframe>\n</div>\n</div>\n</div>\n<h2>Malas noticias: la precisión y la vulnerabilidad ante los ataques antagónicos van de la mano</h2>\n<p>Irónicamente, concentrarse en dotar de mayor precisión a los sistemas de reconocimiento (de imagen, de audio, de lo que sea) basados en IA <strong>es lo que está convirtiéndolos en vulnerables a los ataque antagónicos</strong>. <a href=\"https://arxiv.org/abs/1808.01688\">Según Pin-Yu Chen</a>, la precisión los convierte en \"frágiles\", por lo que la clave radica en buscar un equilibrio entre precisión y lo que llama \'robustez\', medidas de protección contra estos ataques.</p>\n<!-- BREAK 14 -->\n<p>La <a href=\"https://arxiv.org/abs/1905.02175\">confirmación</a> de esto llegó precisamente de la mano de Andrew Ilyas y su equipo (recordemos: los responsables del experimento de la tortuga): entrenaron una IA capaz de identificar gatos en base a características \"consistentes\" (reconocibles para los seres humanos) y \"no consistentes\" (aquellas que nuestra vista pasa por alto), y descubrieron que los clasificadores visuales eran capaces de identificar gatos recurriendo a ambos tipos de rasgos, pero que <strong>obtenían un mayor grado de precisión cuanto más tenían en cuenta las \"no consistentes\"</strong>.</p>\n<!-- BREAK 15 --> </div>', '2019-08-18 19:31:52', '2019-08-18 19:31:52', 16, 'portada0019.jpg', 'Los sistemas de reconocimiento de imagen o audio mediante inteligencia artificial son manipulables... por otras IAs. Y así, pueden lograr que un clasificador de imágenes vea un fusil donde sólo hay una tortuga. ¿Cómo es esto posible?');

-- --------------------------------------------------------

--
-- Estructura de tabla para la tabla `users`
--

CREATE TABLE `users` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `name` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `email` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `email_verified_at` timestamp NULL DEFAULT NULL,
  `password` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL,
  `remember_token` varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL,
  `nick` varchar(191) COLLATE utf8mb4_unicode_ci NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- Volcado de datos para la tabla `users`
--

INSERT INTO `users` (`id`, `name`, `email`, `email_verified_at`, `password`, `remember_token`, `created_at`, `updated_at`, `nick`) VALUES
(1, 'edgardo saravelli diaz', 'ziniestro_dj@hotmail.com', NULL, '$2b$10$eob3SkJTsyIxJwH2CQYr/eoiMgF4WfjcKeqeYIWf8bA7yh8NwTZPO', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'edgardoSD'),
(2, 'ana carolina zorrilla mattos', 'carozm217@hotmail.com', NULL, '$2b$10$bGQVhl9oX8Q.q8yQ8bEP0OBtAuYhgYRVoXBJ4iyLQTt6aM1XNjRN.', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'anaZM'),
(3, 'alessandra sanchez capristan', 'ale180213@gmail.com', NULL, '$2b$10$1bKhj0bcK06Dxn8gypjmQeQKNNEKxroEBS/G/zFjzIF7Q1zBsX9Kq', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'alessandraSC'),
(4, 'elias lizardo prado flores', 'elipra95@hotmail.com', NULL, '$2b$10$R0h5wRdMRAbZNA/g4Y85KuXfPWULbMkiQdkoLKqRKupS.pON3nInu', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'eliasPF'),
(5, 'renato andre francisco rabines leiva', 'andrerl.1990@gmail.com', NULL, '$2b$10$QVQDMZrDLivuxtMenorutujsji8rlz5yqemSjkZ6IWYXAg2gFnTOi', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'renatoRL'),
(6, 'claudia lucia reyes pinedo', 'claudia.reyespinedo@gmail.com', NULL, '$2b$10$/5Wkk1ub2csSIzx/DwLxeO7G7X4UM3Wu49JY3FVfXAQiXZ0FJ.Hn6', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'claudiaRP'),
(7, 'maria leticia guerra inca', 'guerrainca_18@outlook.es', NULL, '$2b$10$/ta0n.UD6rRAc.Burti1g.kNa.rkrrkCZDhCg9Ftuhk5juuD7XULO', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'mariaGI'),
(8, 'paulo cava espinoza', 'leo_2995@hotmail.com', NULL, '$2b$10$KPpStKeX3UrB.sw47jHo/OqSHLhXY7axMT7fWUq0gQqjE7e/xtzCK', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'pauloCE'),
(9, 'sergio enrique mendez segura', 'sergiomndz9@gmail.com', NULL, '$2b$10$q6G77EznhFkqsLlsCOrmVOzArsltHlS50z652Av/talXz7Vx9data', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'sergioMS'),
(10, 'ana cecilia torres ramirez', 'anace_tr@hotmail.com', NULL, '$2b$10$jv5ioPzElA.FKLzd3EOUheX2b/0oDkVK/IRXlUC3FNERV.Xuxd3Iu', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'anaTR'),
(11, 'fiorela alexandra ruiz oliver', 'fioruizoliver@gmail.com', NULL, '$2b$10$/L8Md9aQWh5dWL3ktUAtW.RiEqCgHgit4LZNKFaIOBlMX7l9C3omq', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'fiorelaRO'),
(12, 'josie nathaly calle valdiviezo', 'josie_nath_46@hotmail.com', NULL, '$2b$10$C5tZvxpYG90wQSZ48/EJU.9zcZy22tf7b/Y5em7cVPHoWUVRjnjpW', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'josieCV'),
(13, 'andrea blanco vizcarra', 'andreablanca25@hotmail.com', NULL, '$2b$10$UEIf/RC.ROyOR7DMGJULNOKPNh5aorcwlz1.OXC9hZgE6eYLVqJIq', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'andreaBV'),
(14, 'yomayra susana chigne castro', 'sagitario_chc12@hotmail.com', NULL, '$2b$10$eayere3Mxbw93r7yFcrylOtZ1TIKbjM/Elg4pz1GN/6/.vctQZgWi', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'yomayraCC'),
(15, 'arturo david quinte castro', 'arturo@hotmail.com', NULL, '$2b$10$a2tVnENrH9NIHTE9McKYcucNZIxWmaoPeDNt8zrN/ICmOe9/RRkb6', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'arturoQC'),
(16, 'gomer jonathan deza jacobs', 'jonathandezaja@hotmail.com', NULL, '$2b$10$QMSDsvl8.J3B4QFjvr9VbOQMeZAt18ZUKmZciNHtAOSK6lQDOFQjG', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'gomerDJ'),
(17, 'hans abdel valderrama zarate', 'hans@hotmail.com', NULL, '$2b$10$wweGUnNuCE2Wiw2LH35Qpuje0gZtGJTExBQeIl4ki8hulqCkYZFfm', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'hansVZ'),
(18, 'andreu steven rodriguez tiznado', 'andriusteves@hotmail.com', NULL, '$2b$10$cGlX75rvj9wAyQWkduGyxOdcFR/EC11PvvfWxJRGSslDcUdbHqWlO', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'andreuRT'),
(19, 'victor raul roldan malaspina', 'victor.rm_10@hotmail.com', NULL, '$2b$10$wfApk6f2Bdi4Z.QvAb3qKe/BtabWLrMsRxuByLcP7U78FNNs6uXSq', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'victorRM'),
(20, 'josue christian aycho armas', 'adriano_libra_10@hotmail.com', NULL, '$2b$10$GGYf6Ap3vHzigMIoNMVbReC1PW4RXnVxScVR9gHileIv3MQSwGRBS', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'josueAA'),
(21, 'christian antonio sanchez plasencia', 'marcosanchez1904@hotmail.com', NULL, '$2b$10$T1BAV4ZN35v6i/6BTJegDebJfvQEeyLGn6bJ2ugbaj2mwwSeZ6tDq', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'christianSP'),
(22, 'eddy bryan garay alvarez', 'eddybryan04@hotmail.com', NULL, '$2b$10$WHjXtK09jNuxuNmY2iKzP.KJ8bW/BwQ3or6o9QyL3mLUWE8HVzfKe', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'eddyGA'),
(23, 'gerson luis gonzales perez', 'gonp_12@hotmail.com', NULL, '$2b$10$TiZJJCX.FBtKdDB7k./7de6rgF6lDTerwTukdwgI4tXx3XieZrw56', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'gersonGP'),
(24, 'ricardo antonio cabanillas martinez', 'ricardo_25_39@hotmail.com', NULL, '$2b$10$b9FNNbvRZuWblMZAXg.qcesOZt6u93I42KqtPdP80wnrlehhNK5Vq', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'ricardoCM'),
(25, 'luciana fiorella villanueva vasquez', 'lucianavillanueva95@hotmail.com', NULL, '$2b$10$Z4h/9j/7.rUsRd3EIfLEVOjuIxTFUHfERpQmX6eUnaF0lYKgGPhmy', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'lucianaVV'),
(26, 'katia lilibeth paredes minchola', 'katy_95_18@hotmail.com', NULL, '$2b$10$v85oNaLouqvTTsSQO1Ns/O8IWp68h5UF7G3TSWaSWvpS2XGEzJ0LW', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'katiaPM'),
(27, 'hernan arturo chauca siccha', 'hernan_chauca_10@hotmail.com', NULL, '$2b$10$/Nc2BMgfcvONf6bQrfxvnOq8ankzW/BqFSLU62DBKA50TkPK0XQVK', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'hernanCS'),
(28, 'cristian ferrer mendoza', 'cristianferrer437@gmail.com', NULL, '$2b$10$RxDFcT9je9OTfZ..QEhDfeZBvInv2H0DkjSBJehHD43i5XzNoIMUu', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'cristianFM'),
(29, 'melissa velasquez rebaza', 'love_neli_96@hotmail.com', NULL, '$2b$10$x8VIu8KghezGd3XjbE9fCuD60U56o6WasT1sUFxPSdQyhFdiCGek.', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'melissaVR'),
(30, 'eymi raquel navarro otiniano', 'eymi@hotmail.com', NULL, '$2b$10$yNdbRacMv6OOLFg0BVXj.eGgbbDlxvYc3KrMrHZBwbAYQ18nlHO1C', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'eymiNO'),
(31, 'olenka janeth cruz angulo', 'olenkacruzangulo@hotmail.com', NULL, '$2b$10$MPmfx8vvyPy61i7RPMLaaOEO45OIBeeuxMhVZ3Fz7owFF1GITIzV6', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'olenkaCA'),
(32, 'karina alexandra silva lopez', 'karry_silop@hotmail.com', NULL, '$2b$10$fEGKedZNTtYSID0JX2ALwOsZC1/hKgywQwhOynuwYIIJYKfxYjqN2', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'karinaSL'),
(33, 'leticia mariana moncada mesias', 'lemar1105@hotmail.com', NULL, '$2b$10$GvmyfSN4zBb4q7wXPieS5uBsiLRHSgOMuuLlxavqeB0UH2yHjX82O', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'leticiaMM'),
(34, 'rosa jimena martinez paredes', 'roji_96@hotmail.com', NULL, '$2b$10$If10u9lL8RvBGBLxDKkM1uiOVAk66cebulJjm2BKJ0xeJfztnpWqK', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'rosaMP'),
(35, 'joana isabel ortecho castillo', 'joana@hotmail.com', NULL, '$2b$10$128AmTgc7L2hVofMtiRo3OKnvRDZhwu/CSmMIrIETejvdbE4j0Pf.', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'joanaOC'),
(36, 'karen nohelly cayhuaray diaz', 'dreak_95@hotmail.com', NULL, '$2b$10$bvOrjkU9.x2Sgm3suspCfOTRm/sP2XZh5tKe4mqfG7c.OpoL661HG', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'karenCD'),
(37, 'luis fernando caballero garcia', 'luisfernando396@gmail.com', NULL, '$2b$10$OUXiZWWrZxldRj6mnAe8Oef4XKuzhF8tdwFBcPEerkSjsgPiPYXIq', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'luisCG'),
(38, 'karen estefanie linares mostacero', 'karen.kelm@hotmail.com', NULL, '$2b$10$lvFnXTaRv9eD5wN6k6bYN.fCfScdhg75ZnVU3.ei.XG1Zkx6WP3ma', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'karenLM'),
(39, 'ingrid nicole garcia pereda', 'juanateresa@hotmail.es', NULL, '$2b$10$KagP3f8HEAnJzFHy8XnRUOTVYiPjRGcSvzPFe0rXW5mxDwxkgTY2W', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'ingridGP'),
(40, 'favio junior cueva vasquez', 'faviocueva7@gmail.com', NULL, '$2b$10$JeD7nayMwpEThw.InnZHJu/DqKWGxcsfZNMJnJCnkbTaoYWrE7xo6', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'favioCV'),
(41, 'lorena ibeth rodriguez sanchez', 'lore.7696.lrs@hotmail.com', NULL, '$2b$10$gf1j.Yz28qI5TmkFmkuxxOMaL5iBn2bv28hNFkqTGUYvgMr1XyzAm', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'lorenaRS'),
(42, 'jetsabell cristina gutierrez vallejos', 'jgutierrezvallejos5@gmail.com', NULL, '$2b$10$NVeCpiwUYDSipIodb1Yhx.h6Kw7LzV/gBSJlaNmjD1m.DvdOTItB.', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'jetsabellGV'),
(43, 'gabriela antonet ayay suarez', 'gabrielayays@hotmail.com', NULL, '$2b$10$R3kyI6cN7lJiohRIm6zgIu5FmBh5c9tNN8VrDReHVvy.E64zpnSV2', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'gabrielaAS'),
(44, 'yelsin erik fernandez llajamango', 'yeiryck.fm_95@hotmail.com', NULL, '$2b$10$Q9BS3d5mfk.6dxiZvj3xJeQlqSK1N1tFB5uZB8lpQy0/yZYP/neP6', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'yelsinFL'),
(45, 'christian ibaãez moreno', 'christianaim@hotmail.com', NULL, '$2b$10$kUgUbUte7ldro7MkedE8EeuSNwc2P9.XL3hU0QJii9JCZomIcvgPq', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'christianIM'),
(46, 'hector joel navarro mantilla', 'leojrotceheigna@gmail.com', NULL, '$2b$10$vpHqN4TC7qRU/PbY0BrO1uO6E1QSzsIDPORHjP4bm.y4JJzC4b0d.', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'hectorNM'),
(47, 'viviana fabiola gonzales cabanillas', 'vivi_142_106@hotmail.com', NULL, '$2b$10$LoHFuEXQZiVS8msnnTSiNOrUIZ4TgSFsPgULXGqcLSIxYDFO2MK.q', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'vivianaGC'),
(48, 'pamela elizabeth quispe avila', 'pamelitaz_c14@hotmail.com', NULL, '$2b$10$e./7dsk8l5rZka44wDoEKOsNQtHCV92Zw7ad5kzxY9wxsGHsuI.Wi', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'pamelaQA'),
(49, 'brenda paola del rosario yancaya echavarria', 'miily06@hotmail.com', NULL, '$2b$10$QjyF0EPJlq0KxWbBF1Tg5uKmB5SuyBxMjucW6Es6nNEgZIi8WE6nG', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'brendaYE'),
(50, 'lucia angelica ybaãez evangelista', 'leo_13_yba@hotmail.com', NULL, '$2b$10$.iGuL1cdGk8DMm46hvM0ouvLofWslgE/n6vWLJ3V6lTYDUbGFr.iS', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'luciaYE'),
(51, 'esther del pilar saona baltodano', 'sabalespi@hotmail.com', NULL, '$2b$10$HtWa.SuLXzmAha0oOZNKnul24YvlSnrqnzLmHWo3TnXhqApWmUCGu', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'estherSB'),
(52, 'jose carlos miranda valdivia', 'josecmv3@gmail.com', NULL, '$2b$10$Hl59y1.vVEKgnHLfM78Qxe048xJj1X7aQhKl32bDh9FjywJs/DWaW', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'joseMV'),
(53, 'maria laura leon lozano', 'maria_laurall@hotmail.com', NULL, '$2b$10$W49PNTXyEw2dyrt1xJPv0umzSwTPia/Enx6bQzk0erguD7vfSN4aW', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'mariaLL'),
(54, 'andrea de fatima benites miãano', 'andrea_96_02_17@hotmail.com', NULL, '$2b$10$ZkEZiROxoZy2uA2X5QQsneveOACL2PQXsj0uIhjfKPVpmgY51KNkO', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'andreaBM'),
(55, 'jorge eduardo fernandez sanchez', 'terroblade_695@hotmail.com', NULL, '$2b$10$YJrVAkbEOLg72ARfHoOov.0qHWYIzBntkfBm/gYRNXOVGa5nNskHC', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'jorgeFS'),
(56, 'germancito antony navez lucho', 'navez.lucho@gmail.com', NULL, '$2b$10$QNEq4EuY7PS0cMIClbWOF.Q7CO1VLCENPNs/dRtwxwMAoj0kjctv2', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'germancitoNL'),
(57, 'aron day ichi velasquez chigne', 'cielo_enero25@hotmail.com', NULL, '$2b$10$kQ7rAzt5mWNEKLiBMC7//eaOg6d.OobE7U1z9Lw01pEt/O/n.YLue', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'aronVC'),
(58, 'darlin maricielo vasquez chigne', 'ruthchignepretel@hotmail.com', NULL, '$2b$10$8mV17L3YUhmNe0Si71uO3OjXUMbihBg5WDPS8LXeO9vvW2kfUXN1W', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'darlinVC'),
(59, 'diego martin mendez huaman', 'diego_h96@hotmail.com', NULL, '$2b$10$TzYDlyh1NcxrqoHZLeSaO.O6oUG15d0IzEY3iXRQWrl/vzanCISCi', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'diegoMH'),
(60, 'karina llanos avalos', 'kone_261@outlook.es', NULL, '$2b$10$0gWhBFEMEhgHjhwzvsMu8eXN6zjGeqIDBsnwUSqYC54jw9TsKMdsm', NULL, '2019-10-27 00:54:27', '2019-10-27 00:54:27', 'karinaLA');

--
-- Índices para tablas volcadas
--

--
-- Indices de la tabla `failed_jobs`
--
ALTER TABLE `failed_jobs`
  ADD PRIMARY KEY (`id`);

--
-- Indices de la tabla `follows`
--
ALTER TABLE `follows`
  ADD PRIMARY KEY (`id`);

--
-- Indices de la tabla `migrations`
--
ALTER TABLE `migrations`
  ADD PRIMARY KEY (`id`);

--
-- Indices de la tabla `password_resets`
--
ALTER TABLE `password_resets`
  ADD KEY `password_resets_email_index` (`email`);

--
-- Indices de la tabla `perfiles`
--
ALTER TABLE `perfiles`
  ADD PRIMARY KEY (`id`);

--
-- Indices de la tabla `posts`
--
ALTER TABLE `posts`
  ADD PRIMARY KEY (`id`);

--
-- Indices de la tabla `users`
--
ALTER TABLE `users`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `users_email_unique` (`email`);

--
-- AUTO_INCREMENT de las tablas volcadas
--

--
-- AUTO_INCREMENT de la tabla `failed_jobs`
--
ALTER TABLE `failed_jobs`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT;

--
-- AUTO_INCREMENT de la tabla `follows`
--
ALTER TABLE `follows`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=278;

--
-- AUTO_INCREMENT de la tabla `migrations`
--
ALTER TABLE `migrations`
  MODIFY `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=12;

--
-- AUTO_INCREMENT de la tabla `perfiles`
--
ALTER TABLE `perfiles`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=61;

--
-- AUTO_INCREMENT de la tabla `posts`
--
ALTER TABLE `posts`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=61;

--
-- AUTO_INCREMENT de la tabla `users`
--
ALTER TABLE `users`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=61;
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
